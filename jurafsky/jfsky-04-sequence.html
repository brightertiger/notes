<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Sequence Architectures: RNNs, LSTMs, and Attention | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="jfsky-04-sequence.html" class="active">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../probml/probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="../probml/probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="../probml/probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="../probml/probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="../probml/probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="../probml/probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="../probml/probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="../probml/probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="../probml/probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="../probml/probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">Jurafsky</a>
          </div>
          <h1 class="page-title">Sequence Architectures: RNNs, LSTMs, and Attention</h1>
          <div class="page-meta"><span class="tag">Jurafsky</span></div>
        </header>
        <article class="content">
          <h1 id="sequence-architectures-rnns-lstms-and-attention">Sequence Architectures: RNNs, LSTMs, and Attention</h1>
<p>Language is inherently sequential. This chapter covers neural architectures designed to process sequences: from basic RNNs to LSTMs to the attention mechanism that revolutionized NLP.</p>
<h2 id="the-big-picture">The Big Picture</h2>
<p><strong>The Problem</strong>: Language has dependencies across arbitrary distances.</p>
<ul>
<li>&quot;The <strong>cat</strong> that I saw yesterday <strong>was</strong> cute&quot; (subject-verb agreement)</li>
<li>Standard feedforward networks have fixed input size</li>
</ul>
<p><strong>The Solution</strong>: Architectures with <strong>memory</strong> that process sequences step by step.</p>
<p><strong>The Evolution</strong>:</p>
<pre><code>FFNNs ‚Üí RNNs ‚Üí LSTMs/GRUs ‚Üí Attention ‚Üí Transformers
(fixed context)  (memory)  (better memory)  (direct connections)
</code></pre>
<hr>
<h2 id="why-not-feedforward-networks">Why Not Feedforward Networks?</h2>
<p><strong>Limitation</strong>: Fixed context window.</p>
<p>A bigram FFNN can only see the previous word. But language has long-range dependencies:</p>
<ul>
<li>&quot;The students who did well on the exam <strong>were</strong> happy&quot;</li>
<li>Verb agrees with &quot;students&quot;, not &quot;exam&quot;</li>
</ul>
<p><strong>We need</strong>: Variable-length context.</p>
<hr>
<h2 id="recurrent-neural-networks-rnns">Recurrent Neural Networks (RNNs)</h2>
<h3 id="the-core-idea">The Core Idea</h3>
<p>Add a <strong>recurrent connection</strong> ‚Äî the hidden state from the previous step feeds into the current step.</p>
<p>$$h_t = g(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$
$$y_t = W_{hy} h_t + b_y$$</p>
<p><strong>The hidden state $h_t$ acts as memory!</strong></p>
<h3 id="intuition">Intuition</h3>
<p>Think of reading a sentence word by word:</p>
<ul>
<li>You update your understanding as each word arrives</li>
<li>Your current understanding depends on what you&#39;ve read so far</li>
<li>That&#39;s exactly what $h_t$ does</li>
</ul>
<h3 id="training-backpropagation-through-time-bptt">Training: Backpropagation Through Time (BPTT)</h3>
<ol>
<li><strong>Unroll</strong> the network across time steps</li>
<li><strong>Forward pass</strong>: Compute all hidden states and outputs</li>
<li><strong>Backward pass</strong>: Compute gradients through the unrolled graph</li>
<li><strong>Update</strong>: Sum gradients for shared weights</li>
</ol>
<p><strong>For long sequences</strong>: Use truncated BPTT (limit how far back gradients flow).</p>
<h3 id="rnn-language-model">RNN Language Model</h3>
<p>$$e_t = E x_t \quad \text{(word embedding)}$$
$$h_t = g(W_{hh} h_{t-1} + W_{he} e_t)$$
$$P(w_{t+1}) = \text{softmax}(W_{hy} h_t)$$</p>
<p><strong>Training</strong>: Teacher forcing ‚Äî use true previous word, not predicted word.</p>
<p><strong>Weight tying</strong>: Share parameters between input embedding E and output layer.</p>
<hr>
<h2 id="rnn-task-variants">RNN Task Variants</h2>
<h3 id="sequence-labeling-many-to-many-aligned">Sequence Labeling (Many-to-Many, aligned)</h3>
<p><strong>Task</strong>: Label each token (NER, POS tagging).</p>
<pre><code>Input:  John  loves  New   York
Output: B-PER O      B-LOC I-LOC
</code></pre>
<p>Predict at each step based on hidden state.</p>
<h3 id="sequence-classification-many-to-one">Sequence Classification (Many-to-One)</h3>
<p><strong>Task</strong>: Classify entire sequence (sentiment analysis).</p>
<pre><code>Input:  This movie was great!
Output: POSITIVE
</code></pre>
<p>Use final hidden state (or pooled states) for classification.</p>
<h3 id="sequence-generation-one-to-many-or-many-to-many">Sequence Generation (One-to-Many or Many-to-Many)</h3>
<p><strong>Task</strong>: Generate text.</p>
<pre><code>Input:  &lt;BOS&gt;
Output: The cat sat on the mat &lt;EOS&gt;
</code></pre>
<p>Autoregressive: Each output becomes next input.</p>
<hr>
<h2 id="rnn-architectures">RNN Architectures</h2>
<h3 id="stacked-rnns">Stacked RNNs</h3>
<p>Multiple RNN layers:</p>
<pre><code>Layer 3: h3_t = f(h3_{t-1}, h2_t)
Layer 2: h2_t = f(h2_{t-1}, h1_t)  
Layer 1: h1_t = f(h1_{t-1}, x_t)
</code></pre>
<p><strong>Benefit</strong>: Different abstraction levels at each layer.</p>
<h3 id="bidirectional-rnns">Bidirectional RNNs</h3>
<p>Process sequence both ways:</p>
<ul>
<li>Forward: $\vec{h}_t$ (left to right)</li>
<li>Backward: $\overleftarrow{h}_t$ (right to left)</li>
<li>Combined: $h_t = [\vec{h}_t; \overleftarrow{h}_t]$</li>
</ul>
<p><strong>Benefit</strong>: Each position has access to full context.</p>
<p><strong>Limitation</strong>: Can&#39;t use for autoregressive generation (need future that doesn&#39;t exist yet).</p>
<hr>
<h2 id="the-vanishing-gradient-problem">The Vanishing Gradient Problem</h2>
<h3 id="the-problem">The Problem</h3>
<p>Gradients shrink exponentially as they flow backward through time:</p>
<p>$$\frac{\partial L}{\partial h_1} = \frac{\partial L}{\partial h_T} \cdot \prod_{t=2}^{T} \frac{\partial h_t}{\partial h_{t-1}}$$</p>
<p>If $\frac{\partial h_t}{\partial h_{t-1}} &lt; 1$ consistently ‚Üí <strong>vanishing gradients</strong>.</p>
<p><strong>Consequence</strong>: RNNs struggle to learn long-range dependencies.</p>
<h3 id="why-it-happens">Why It Happens</h3>
<ul>
<li>Sigmoid derivative: max 0.25</li>
<li>Tanh derivative: max 1.0</li>
<li>Repeated multiplication through many steps ‚Üí exponential decay</li>
</ul>
<h3 id="solutions">Solutions</h3>
<ol>
<li><strong>Gradient clipping</strong> (for exploding gradients)</li>
<li><strong>Better architectures</strong>: LSTM, GRU</li>
<li><strong>Skip connections</strong>: Allow gradients to flow directly</li>
</ol>
<hr>
<h2 id="lstm-long-short-term-memory">LSTM (Long Short-Term Memory)</h2>
<h3 id="the-innovation">The Innovation</h3>
<p>Add <strong>explicit memory management</strong> through <strong>gates</strong>.</p>
<p>Two types of state:</p>
<ul>
<li><strong>Cell state $c_t$</strong>: Long-term memory (conveyor belt)</li>
<li><strong>Hidden state $h_t$</strong>: Working memory / output</li>
</ul>
<h3 id="the-three-gates">The Three Gates</h3>
<table>
<thead>
<tr>
<th>Gate</th>
<th>Purpose</th>
<th>Controls</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Forget</strong></td>
<td>What to erase from memory</td>
<td>$f_t$</td>
</tr>
<tr>
<td><strong>Input</strong></td>
<td>What new info to add</td>
<td>$i_t$</td>
</tr>
<tr>
<td><strong>Output</strong></td>
<td>What to expose as output</td>
<td>$o_t$</td>
</tr>
</tbody></table>
<h3 id="lstm-equations">LSTM Equations</h3>
<p><strong>Forget gate</strong> (what to keep from old memory):
$$f_t = \sigma(W_f x_t + U_f h_{t-1} + b_f)$$</p>
<p><strong>Input gate</strong> (how much of new info to add):
$$i_t = \sigma(W_i x_t + U_i h_{t-1} + b_i)$$</p>
<p><strong>Candidate values</strong> (new info to potentially add):
$$\tilde{c}<em>t = \tanh(W_c x_t + U_c h</em>{t-1} + b_c)$$</p>
<p><strong>Cell state update</strong> (the key equation!):
$$c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t$$</p>
<p><strong>Output gate</strong> (what to output):
$$o_t = \sigma(W_o x_t + U_o h_{t-1} + b_o)$$</p>
<p><strong>Hidden state</strong>:
$$h_t = o_t \odot \tanh(c_t)$$</p>
<h3 id="why-lstm-works">Why LSTM Works</h3>
<p>The cell state update is <strong>additive</strong>:
$$c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t$$</p>
<p>Gradients can flow through unchanged (when forget gate ‚âà 1).</p>
<hr>
<h2 id="gru-gated-recurrent-unit">GRU (Gated Recurrent Unit)</h2>
<p>Simplified LSTM with fewer parameters:</p>
<ul>
<li>Combines forget and input gates into <strong>update gate</strong></li>
<li>No separate cell state</li>
</ul>
<p>Often performs comparably to LSTM with less computation.</p>
<hr>
<h2 id="attention-mechanism">Attention Mechanism</h2>
<h3 id="the-bottleneck-problem">The Bottleneck Problem</h3>
<p>In encoder-decoder models, all information must pass through a fixed-size vector.</p>
<p><strong>Problem</strong>: Information gets lost for long sequences.</p>
<h3 id="the-attention-solution">The Attention Solution</h3>
<p>Let the decoder <strong>look at all encoder states</strong> when making each prediction.</p>
<h3 id="how-attention-works">How Attention Works</h3>
<p>For each decoder step:</p>
<ol>
<li><p><strong>Score</strong>: Compute similarity between decoder state and each encoder state
$$e_{ij} = \text{score}(s_{i-1}, h_j)$$</p>
</li>
<li><p><strong>Normalize</strong>: Convert scores to weights (softmax)
$$\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_k \exp(e_{ik})}$$</p>
</li>
<li><p><strong>Combine</strong>: Weighted sum of encoder states
$$c_i = \sum_j \alpha_{ij} h_j$$</p>
</li>
<li><p><strong>Use</strong>: Context vector informs prediction
$$s_i = f(s_{i-1}, y_{i-1}, c_i)$$</p>
</li>
</ol>
<h3 id="scoring-functions">Scoring Functions</h3>
<table>
<thead>
<tr>
<th>Type</th>
<th>Formula</th>
</tr>
</thead>
<tbody><tr>
<td>Dot product</td>
<td>$s^T h$</td>
</tr>
<tr>
<td>Scaled dot product</td>
<td>$\frac{s^T h}{\sqrt{d}}$</td>
</tr>
<tr>
<td>MLP</td>
<td>$v^T \tanh(W_1 s + W_2 h)$</td>
</tr>
</tbody></table>
<h3 id="benefits-of-attention">Benefits of Attention</h3>
<ol>
<li><strong>Long-range dependencies</strong>: Direct connections regardless of distance</li>
<li><strong>Interpretability</strong>: Attention weights show what the model focuses on</li>
<li><strong>Alignment</strong>: Helpful for translation (which source words map to which target words)</li>
</ol>
<hr>
<h2 id="self-attention-and-transformers">Self-Attention and Transformers</h2>
<h3 id="from-attention-to-self-attention">From Attention to Self-Attention</h3>
<p><strong>Regular attention</strong>: Query from decoder, keys/values from encoder.</p>
<p><strong>Self-attention</strong>: Query, keys, values all from same sequence.</p>
<p>$$\text{output}_i = \text{Attention}(x_i, (x_1, x_1), (x_2, x_2), ..., (x_n, x_n))$$</p>
<p>Each position attends to all positions in the same sequence.</p>
<h3 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h3>
<p>$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$</p>
<p>Where:</p>
<ul>
<li>Q = queries (what I&#39;m looking for)</li>
<li>K = keys (what I offer for matching)</li>
<li>V = values (what I actually provide)</li>
<li>$\sqrt{d_k}$ scaling prevents softmax saturation</li>
</ul>
<h3 id="multi-head-attention">Multi-Head Attention</h3>
<p>Run multiple attention operations in parallel:
$$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$
$$\text{MultiHead} = \text{Concat}(\text{head}_1, ..., \text{head}_h) W^O$$</p>
<p><strong>Benefit</strong>: Each head can capture different types of relationships.</p>
<h3 id="positional-encoding">Positional Encoding</h3>
<p>Attention is <strong>permutation invariant</strong> ‚Äî doesn&#39;t know word order!</p>
<p><strong>Solution</strong>: Add position information to embeddings.</p>
<p><strong>Sinusoidal encoding</strong>:
$$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d})$$
$$PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d})$$</p>
<h3 id="bert-architecture">BERT Architecture</h3>
<p><strong>Base model</strong>:</p>
<ul>
<li>12 attention heads</li>
<li>12 layers</li>
<li>768 hidden size (12 √ó 64)</li>
<li>110M parameters</li>
</ul>
<hr>
<h2 id="summary">Summary</h2>
<table>
<thead>
<tr>
<th>Architecture</th>
<th>Memory</th>
<th>Long-range</th>
<th>Parallelizable</th>
</tr>
</thead>
<tbody><tr>
<td><strong>RNN</strong></td>
<td>Hidden state</td>
<td>Limited</td>
<td>No</td>
</tr>
<tr>
<td><strong>LSTM</strong></td>
<td>Cell + hidden</td>
<td>Better</td>
<td>No</td>
</tr>
<tr>
<td><strong>Attention RNN</strong></td>
<td>+ context</td>
<td>Good</td>
<td>Partially</td>
</tr>
<tr>
<td><strong>Transformer</strong></td>
<td>Attention only</td>
<td>Excellent</td>
<td>Yes</td>
</tr>
</tbody></table>
<h3 id="key-takeaways">Key Takeaways</h3>
<ol>
<li><strong>RNNs</strong> process sequences with memory but struggle with long dependencies</li>
<li><strong>LSTMs</strong> use gates to control information flow, solving vanishing gradients</li>
<li><strong>Attention</strong> provides direct connections between any positions</li>
<li><strong>Transformers</strong> replace recurrence with pure attention, enabling parallelism</li>
<li>Modern NLP is dominated by <strong>Transformer-based models</strong> (BERT, GPT, etc.)</li>
</ol>

        </article>
        <nav class="page-navigation">
        <a href="jfsky-03-vectors.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Vector Semantics and Word Embeddings</span>
        </a>
        <a href="jfsky-05-encoder.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Encoder-Decoder Models</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>