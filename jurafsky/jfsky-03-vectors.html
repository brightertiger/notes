<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Vector Semantics and Word Embeddings | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="jfsky-03-vectors.html" class="active">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../probml/probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="../probml/probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="../probml/probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="../probml/probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="../probml/probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="../probml/probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="../probml/probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="../probml/probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="../probml/probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="../probml/probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">Jurafsky</a>
          </div>
          <h1 class="page-title">Vector Semantics and Word Embeddings</h1>
          <div class="page-meta"><span class="tag">Jurafsky</span></div>
        </header>
        <article class="content">
          <h1 id="vector-semantics-and-word-embeddings">Vector Semantics and Word Embeddings</h1>
<p>How do we represent word meaning computationally? This chapter covers the evolution from sparse count-based vectors to dense neural embeddings ‚Äî one of the most important advances in NLP.</p>
<h2 id="the-big-picture">The Big Picture</h2>
<p><strong>The Problem</strong>: Computers need numerical representations of words.</p>
<p><strong>Key Insight</strong> (Distributional Hypothesis):</p>
<blockquote>
<p>&quot;You shall know a word by the company it keeps&quot; ‚Äî J.R. Firth</p>
</blockquote>
<p>Words that appear in similar contexts have similar meanings.</p>
<p><strong>The Evolution</strong>:</p>
<pre><code>One-hot vectors ‚Üí Count-based vectors ‚Üí Neural embeddings
(sparse, no similarity)   (sparse, some similarity)   (dense, learned similarity)
</code></pre>
<hr>
<h2 id="challenges-of-lexical-semantics">Challenges of Lexical Semantics</h2>
<p>Why is meaning hard?</p>
<table>
<thead>
<tr>
<th>Challenge</th>
<th>Example</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Word forms</strong></td>
<td>sing, sang, sung (same lemma &quot;sing&quot;)</td>
</tr>
<tr>
<td><strong>Polysemy</strong></td>
<td>&quot;bank&quot; = river bank or financial bank</td>
</tr>
<tr>
<td><strong>Synonymy</strong></td>
<td>couch ‚âà sofa (same meaning)</td>
</tr>
<tr>
<td><strong>Relatedness</strong></td>
<td>coffee ~ cup (not synonyms, but related)</td>
</tr>
<tr>
<td><strong>Semantic frames</strong></td>
<td>&quot;A bought from B&quot; ‚âà &quot;B sold to A&quot;</td>
</tr>
<tr>
<td><strong>Connotation</strong></td>
<td>&quot;slender&quot; vs. &quot;skinny&quot; (same denotation, different feeling)</td>
</tr>
</tbody></table>
<hr>
<h2 id="vector-space-models">Vector Space Models</h2>
<h3 id="the-core-idea">The Core Idea</h3>
<p>Represent words as vectors in a high-dimensional space where:</p>
<ul>
<li><strong>Similar words</strong> are <strong>close together</strong></li>
<li><strong>Dissimilar words</strong> are <strong>far apart</strong></li>
</ul>
<h3 id="document-vectors-term-document-matrix">Document Vectors (Term-Document Matrix)</h3>
<table>
<thead>
<tr>
<th></th>
<th>Doc1</th>
<th>Doc2</th>
<th>Doc3</th>
</tr>
</thead>
<tbody><tr>
<td>cat</td>
<td>3</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>dog</td>
<td>2</td>
<td>4</td>
<td>0</td>
</tr>
<tr>
<td>pet</td>
<td>1</td>
<td>2</td>
<td>1</td>
</tr>
</tbody></table>
<ul>
<li><strong>Rows</strong>: Words (vocabulary of size V)</li>
<li><strong>Columns</strong>: Documents (D documents)</li>
<li><strong>Cell</strong>: Count of word in document</li>
</ul>
<p><strong>Use case</strong>: Information retrieval (find similar documents).</p>
<h3 id="word-vectors-term-term-matrix">Word Vectors (Term-Term Matrix)</h3>
<table>
<thead>
<tr>
<th></th>
<th>cat</th>
<th>dog</th>
<th>pet</th>
<th>food</th>
</tr>
</thead>
<tbody><tr>
<td>cat</td>
<td>-</td>
<td>15</td>
<td>20</td>
<td>8</td>
</tr>
<tr>
<td>dog</td>
<td>15</td>
<td>-</td>
<td>25</td>
<td>12</td>
</tr>
<tr>
<td>pet</td>
<td>20</td>
<td>25</td>
<td>-</td>
<td>10</td>
</tr>
</tbody></table>
<ul>
<li><strong>Rows and Columns</strong>: Words</li>
<li><strong>Cell</strong>: Co-occurrence count (how often words appear together)</li>
</ul>
<p><strong>Result</strong>: Each word is a V-dimensional vector.</p>
<hr>
<h2 id="measuring-similarity">Measuring Similarity</h2>
<h3 id="cosine-similarity">Cosine Similarity</h3>
<p>Normalized dot product ‚Äî measures angle between vectors:</p>
<p>$$\cos(\theta) = \frac{\vec{a} \cdot \vec{b}}{|\vec{a}| \cdot |\vec{b}|} = \frac{\sum_i a_i b_i}{\sqrt{\sum_i a_i^2} \cdot \sqrt{\sum_i b_i^2}}$$</p>
<p><strong>Interpretation</strong>:</p>
<ul>
<li>cos = 1: Identical direction (most similar)</li>
<li>cos = 0: Perpendicular (unrelated)</li>
<li>cos = -1: Opposite direction (antonyms, in some cases)</li>
</ul>
<p><strong>Why cosine over Euclidean?</strong></p>
<ul>
<li>Handles different vector magnitudes</li>
<li>A long document and short document can still be similar</li>
</ul>
<h3 id="for-unit-vectors">For Unit Vectors</h3>
<p>When vectors are normalized (length 1):
$$||\vec{a} - \vec{b}||^2 = 2(1 - \cos\theta)$$</p>
<p>Euclidean distance and cosine become equivalent!</p>
<hr>
<h2 id="tf-idf-weighting">TF-IDF Weighting</h2>
<p>Raw counts have problems:</p>
<ul>
<li>Common words (&quot;the&quot;, &quot;is&quot;) dominate</li>
<li>Rare but meaningful words get drowned out</li>
</ul>
<h3 id="term-frequency-tf">Term Frequency (TF)</h3>
<p>How often does word appear in document?</p>
<p><strong>Raw TF</strong>: $\text{tf}_{t,d} = \text{count}(t, d)$</p>
<p><strong>Log TF</strong> (dampens large counts):
$$\text{tf}_{t,d} = \log(1 + \text{count}(t, d))$$</p>
<h3 id="inverse-document-frequency-idf">Inverse Document Frequency (IDF)</h3>
<p>How rare is the word across documents?</p>
<p>$$\text{idf}_t = \log\left(\frac{N}{\text{df}_t}\right)$$</p>
<p>Where:</p>
<ul>
<li>N = total number of documents</li>
<li>df_t = number of documents containing term t</li>
</ul>
<p><strong>Effect</strong>: Common words (low IDF) get downweighted.</p>
<h3 id="tf-idf">TF-IDF</h3>
<p>Combine both:
$$w_{t,d} = \text{tf}_{t,d} \times \text{idf}_t$$</p>
<p><strong>High TF-IDF</strong>: Word appears often in this document but rarely overall ‚Üí distinctive!</p>
<hr>
<h2 id="pointwise-mutual-information-pmi">Pointwise Mutual Information (PMI)</h2>
<h3 id="the-intuition">The Intuition</h3>
<p>Are two words appearing together more than we&#39;d expect by chance?</p>
<p>$$\text{PMI}(x, y) = \log_2 \frac{P(x, y)}{P(x) \cdot P(y)}$$</p>
<p><strong>Interpretation</strong>:</p>
<ul>
<li>PMI &gt; 0: Words co-occur more than expected (associated)</li>
<li>PMI = 0: Words co-occur as expected (independent)</li>
<li>PMI &lt; 0: Words co-occur less than expected (avoid each other)</li>
</ul>
<h3 id="from-counts">From Counts</h3>
<p>$$\text{PMI}(x, y) = \log_2 \frac{\text{count}(x, y) \cdot N}{\text{count}(x) \cdot \text{count}(y)}$$</p>
<h3 id="positive-pmi-ppmi">Positive PMI (PPMI)</h3>
<p>Negative PMI values are unreliable (rare events).</p>
<p>$$\text{PPMI}(x, y) = \max(0, \text{PMI}(x, y))$$</p>
<hr>
<h2 id="from-sparse-to-dense-word2vec">From Sparse to Dense: Word2Vec</h2>
<h3 id="the-problem-with-count-vectors">The Problem with Count Vectors</h3>
<ul>
<li><strong>Very high dimensional</strong> (vocabulary size)</li>
<li><strong>Very sparse</strong> (mostly zeros)</li>
<li><strong>No generalization</strong> between similar words</li>
</ul>
<h3 id="the-neural-solution">The Neural Solution</h3>
<p>Learn <strong>dense, low-dimensional</strong> vectors (typically 100-300 dimensions).</p>
<p><strong>Key properties</strong>:</p>
<ul>
<li>Similar words have similar vectors</li>
<li>Relationships are captured geometrically</li>
</ul>
<h3 id="static-vs-contextual-embeddings">Static vs. Contextual Embeddings</h3>
<table>
<thead>
<tr>
<th>Type</th>
<th>Same word = same vector?</th>
<th>Examples</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Static</strong></td>
<td>Yes</td>
<td>Word2Vec, GloVe, FastText</td>
</tr>
<tr>
<td><strong>Contextual</strong></td>
<td>No (depends on context)</td>
<td>ELMo, BERT, GPT</td>
</tr>
</tbody></table>
<hr>
<h2 id="skip-gram-with-negative-sampling-sgns">Skip-Gram with Negative Sampling (SGNS)</h2>
<p>The most popular Word2Vec algorithm.</p>
<h3 id="the-task">The Task</h3>
<p>Given a target word, predict surrounding context words.</p>
<p><strong>Example</strong>: &quot;The quick <strong>brown</strong> fox jumps&quot;</p>
<ul>
<li>Target: &quot;brown&quot;</li>
<li>Context (window=2): &quot;The&quot;, &quot;quick&quot;, &quot;fox&quot;, &quot;jumps&quot;</li>
</ul>
<h3 id="training-setup">Training Setup</h3>
<ol>
<li><strong>Positive examples</strong>: (target, context) pairs from real text</li>
<li><strong>Negative examples</strong>: (target, random_word) pairs ‚Äî fake associations</li>
</ol>
<h3 id="the-objective">The Objective</h3>
<p>Maximize probability of real pairs, minimize probability of fake pairs:</p>
<p>$$L = \log \sigma(v_w \cdot v_c) + \sum_{i=1}^{k} \mathbb{E}<em>{c_i \sim P_n}[\log \sigma(-v_w \cdot v</em>{c_i})]$$</p>
<p>Where:</p>
<ul>
<li>$\sigma$ is sigmoid function</li>
<li>$v_w$ is target word vector</li>
<li>$v_c$ is context word vector</li>
<li>k is number of negative samples (typically 5-20)</li>
</ul>
<h3 id="negative-sampling-distribution">Negative Sampling Distribution</h3>
<p>Don&#39;t sample uniformly ‚Äî would get too many rare words.</p>
<p>$$P(w) \propto \text{freq}(w)^{0.75}$$</p>
<p>The 0.75 power smooths the distribution (gives rare words a better chance than pure frequency).</p>
<h3 id="two-embeddings-per-word">Two Embeddings Per Word</h3>
<p>Each word has:</p>
<ul>
<li><strong>Target embedding</strong>: When it&#39;s the center word</li>
<li><strong>Context embedding</strong>: When it appears in context</li>
</ul>
<p>Final embedding is often their sum or average.</p>
<hr>
<h2 id="enhancements-and-variations">Enhancements and Variations</h2>
<h3 id="fasttext-subword-embeddings">FastText (Subword Embeddings)</h3>
<p><strong>Problem</strong>: What about unknown words like &quot;ungooglable&quot;?</p>
<p><strong>Solution</strong>: Represent words as bag of character n-grams.</p>
<p>&quot;where&quot; ‚Üí {&lt;wh, whe, her, ere, re&gt;}</p>
<p>Word vector = sum of n-gram vectors.</p>
<p><strong>Benefit</strong>: Can handle any word, even unseen ones!</p>
<h3 id="glove-global-vectors">GloVe (Global Vectors)</h3>
<p>Combines advantages of count-based and neural methods.</p>
<p>Uses global co-occurrence statistics + optimization:
$$J = \sum_{i,j} f(X_{ij})(w_i^T \tilde{w}_j + b_i + \tilde{b}<em>j - \log X</em>{ij})^2$$</p>
<p>Often comparable to Word2Vec in practice.</p>
<hr>
<h2 id="word-analogies">Word Analogies</h2>
<p>Famous Word2Vec property:</p>
<p><strong>&quot;king&quot; - &quot;man&quot; + &quot;woman&quot; ‚âà &quot;queen&quot;</strong></p>
<p>Find word that completes analogy a:b :: a&#39;:?</p>
<p>$$b&#39; = \arg\min_{x} \text{distance}(x, b - a + a&#39;)$$</p>
<p><strong>Works for</strong>:</p>
<ul>
<li>Gender: king:queen :: man:woman</li>
<li>Capitals: Paris:France :: Tokyo:Japan</li>
<li>Tense: walking:walked :: swimming:swam</li>
</ul>
<hr>
<h2 id="bias-in-word-embeddings">Bias in Word Embeddings</h2>
<h3 id="the-problem">The Problem</h3>
<p>Word embeddings learn biases present in training data.</p>
<p><strong>Examples</strong>:</p>
<ul>
<li>&quot;doctor&quot; closer to &quot;man&quot; than &quot;woman&quot;</li>
<li>&quot;homemaker&quot; closer to &quot;woman&quot; than &quot;man&quot;</li>
<li>Names associated with certain ethnic groups linked to negative words</li>
</ul>
<h3 id="types-of-harm">Types of Harm</h3>
<p><strong>Allocation harm</strong>: System makes unfair decisions</p>
<ul>
<li>Resume screening favoring male-associated names</li>
</ul>
<p><strong>Representation harm</strong>: Reinforces stereotypes</p>
<ul>
<li>Search results, autocomplete suggestions</li>
</ul>
<h3 id="mitigation-strategies">Mitigation Strategies</h3>
<ul>
<li>Debias during training or post-hoc</li>
<li>Careful data curation</li>
<li>Evaluation for fairness</li>
</ul>
<hr>
<h2 id="summary">Summary</h2>
<table>
<thead>
<tr>
<th>Representation</th>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Count-based (TF-IDF)</strong></td>
<td>Interpretable, simple</td>
<td>Sparse, high-dimensional</td>
</tr>
<tr>
<td><strong>PMI</strong></td>
<td>Captures associations</td>
<td>Sparse, noisy for rare words</td>
</tr>
<tr>
<td><strong>Word2Vec</strong></td>
<td>Dense, captures analogy</td>
<td>Static, no context</td>
</tr>
<tr>
<td><strong>FastText</strong></td>
<td>Handles OOV words</td>
<td>Still static</td>
</tr>
<tr>
<td><strong>Contextual</strong></td>
<td>Word sense disambiguation</td>
<td>Computationally expensive</td>
</tr>
</tbody></table>
<h3 id="key-takeaways">Key Takeaways</h3>
<ol>
<li><strong>Words can be represented as vectors</strong> in semantic space</li>
<li><strong>Distributional similarity</strong> = semantic similarity</li>
<li><strong>Dense embeddings</strong> outperform sparse for most tasks</li>
<li><strong>Context matters</strong> ‚Äî motivates contextual embeddings (BERT, etc.)</li>
<li><strong>Beware of biases</strong> inherited from training data</li>
</ol>

        </article>
        <nav class="page-navigation">
        <a href="jfsky-02-tokenization.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">N-Grams and Language Models</span>
        </a>
        <a href="jfsky-04-sequence.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Sequence Architectures: RNNs, LSTMs, and Attention</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>