<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Encoder-Decoder Models | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="jfsky-05-encoder.html" class="active">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../probml/probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="../probml/probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="../probml/probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="../probml/probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="../probml/probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="../probml/probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="../probml/probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="../probml/probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="../probml/probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="../probml/probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">Jurafsky</a>
          </div>
          <h1 class="page-title">Encoder-Decoder Models</h1>
          <div class="page-meta"><span class="tag">Jurafsky</span></div>
        </header>
        <article class="content">
          <h1 id="encoder-decoder-models">Encoder-Decoder Models</h1>
<p>Encoder-decoder (seq2seq) models transform one sequence into another. They power machine translation, summarization, question answering, and many other NLP tasks.</p>
<h2 id="the-big-picture">The Big Picture</h2>
<p><strong>The Problem</strong>: Input and output sequences have different lengths and structures.</p>
<p><strong>Example (Translation)</strong>:</p>
<ul>
<li>English: &quot;The cat sat on the mat&quot; (6 words)</li>
<li>German: &quot;Die Katze sa√ü auf der Matte&quot; (6 words, different order)</li>
<li>Japanese: Áå´„Åå„Éû„ÉÉ„Éà„ÅÆ‰∏ä„Å´Â∫ß„Å£„Åü (different structure entirely)</li>
</ul>
<p><strong>The Solution</strong>: </p>
<ol>
<li><strong>Encoder</strong>: Compress input into a representation</li>
<li><strong>Decoder</strong>: Generate output from that representation</li>
</ol>
<hr>
<h2 id="encoder-decoder-architecture">Encoder-Decoder Architecture</h2>
<h3 id="the-two-components">The Two Components</h3>
<pre><code>Input Sequence ‚Üí [ENCODER] ‚Üí Context Vector ‚Üí [DECODER] ‚Üí Output Sequence
</code></pre>
<p><strong>Encoder</strong>:</p>
<ul>
<li>Processes input sequence</li>
<li>Produces contextualized hidden states</li>
<li>Creates a &quot;summary&quot; of the input</li>
</ul>
<p><strong>Decoder</strong>:</p>
<ul>
<li>Uses encoder output as initial context</li>
<li>Generates output tokens one at a time</li>
<li>Autoregressive: each output depends on previous outputs</li>
</ul>
<hr>
<h2 id="sequence-to-sequence-with-rnns">Sequence-to-Sequence with RNNs</h2>
<h3 id="encoder">Encoder</h3>
<p>Process input token by token:
$$h_t^{enc} = f(h_{t-1}^{enc}, x_t)$$</p>
<p>The final hidden state $h_T^{enc}$ summarizes the entire input.</p>
<h3 id="context-vector">Context Vector</h3>
<p>Simple approach: Use final encoder hidden state.
$$c = h_T^{enc}$$</p>
<p><strong>Problem</strong>: All information must squeeze through this bottleneck!</p>
<h3 id="decoder">Decoder</h3>
<p>Initialize with context, generate autoregressively:
$$h_t^{dec} = f(h_{t-1}^{dec}, y_{t-1}, c)$$
$$P(y_t) = \text{softmax}(W h_t^{dec})$$</p>
<h3 id="training-teacher-forcing">Training: Teacher Forcing</h3>
<p>During training, use <strong>ground truth</strong> previous tokens, not predicted ones.</p>
<p><strong>Without teacher forcing</strong>: Errors compound (predicted mistake ‚Üí more mistakes).
<strong>With teacher forcing</strong>: More stable training, faster convergence.</p>
<p><strong>The drawback</strong>: Exposure bias ‚Äî model never sees its own mistakes during training.</p>
<hr>
<h2 id="the-attention-solution">The Attention Solution</h2>
<h3 id="the-bottleneck-problem">The Bottleneck Problem</h3>
<p>As sequences get longer, the fixed-size context vector struggles to capture everything.</p>
<h3 id="dynamic-context">Dynamic Context</h3>
<p>Instead of one context vector, compute a <strong>different context for each decoder step</strong>:</p>
<p>$$c_i = \sum_{j=1}^{T_x} \alpha_{ij} h_j^{enc}$$</p>
<p>Where $\alpha_{ij}$ are attention weights ‚Äî how much should decoder step i focus on encoder position j?</p>
<h3 id="computing-attention-weights">Computing Attention Weights</h3>
<ol>
<li><p><strong>Score</strong> each encoder state against decoder state:
$$e_{ij} = \text{score}(s_{i-1}^{dec}, h_j^{enc})$$</p>
</li>
<li><p><strong>Normalize</strong> to get weights:
$$\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})}$$</p>
</li>
<li><p><strong>Combine</strong> to get context:
$$c_i = \sum_j \alpha_{ij} h_j^{enc}$$</p>
</li>
<li><p><strong>Decode</strong> using context:
$$s_i^{dec} = f(s_{i-1}^{dec}, y_{i-1}, c_i)$$</p>
</li>
</ol>
<h3 id="benefits-of-attention">Benefits of Attention</h3>
<table>
<thead>
<tr>
<th>Benefit</th>
<th>Explanation</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Long sequences</strong></td>
<td>No information bottleneck</td>
</tr>
<tr>
<td><strong>Alignment</strong></td>
<td>Learns which source words map to which target words</td>
</tr>
<tr>
<td><strong>Interpretability</strong></td>
<td>Can visualize what the model focuses on</td>
</tr>
<tr>
<td><strong>Gradient flow</strong></td>
<td>Direct paths for gradients</td>
</tr>
</tbody></table>
<hr>
<h2 id="transformer-encoder-decoder">Transformer Encoder-Decoder</h2>
<h3 id="key-difference-cross-attention">Key Difference: Cross-Attention</h3>
<p>The decoder has three types of attention:</p>
<ol>
<li><strong>Self-attention</strong> on encoder (bidirectional)</li>
<li><strong>Masked self-attention</strong> on decoder (causal ‚Äî can&#39;t see future)</li>
<li><strong>Cross-attention</strong>: Queries from decoder, Keys/Values from encoder</li>
</ol>
<h3 id="cross-attention-mechanism">Cross-Attention Mechanism</h3>
<p>$$\text{CrossAttn}(Q^{dec}, K^{enc}, V^{enc}) = \text{softmax}\left(\frac{Q^{dec} (K^{enc})^T}{\sqrt{d}}\right) V^{enc}$$</p>
<p>Decoder queries look up relevant information from encoder.</p>
<hr>
<h2 id="tokenization-for-seq2seq">Tokenization for Seq2Seq</h2>
<h3 id="the-challenge">The Challenge</h3>
<p>Different languages have different:</p>
<ul>
<li>Writing systems</li>
<li>Word boundaries</li>
<li>Vocabulary sizes</li>
</ul>
<h3 id="subword-tokenization">Subword Tokenization</h3>
<p>Use <strong>BPE</strong> or <strong>WordPiece</strong> for both languages:</p>
<ul>
<li>Handles rare words gracefully</li>
<li>Shares subwords across similar languages</li>
<li>Reduces vocabulary size</li>
</ul>
<hr>
<h2 id="evaluation-metrics">Evaluation Metrics</h2>
<h3 id="human-evaluation-gold-standard">Human Evaluation (Gold Standard)</h3>
<p><strong>Adequacy</strong>: Is the meaning preserved?</p>
<ul>
<li>1 = None, 5 = All meaning captured</li>
</ul>
<p><strong>Fluency</strong>: Is it grammatically correct and natural?</p>
<ul>
<li>1 = Incomprehensible, 5 = Native quality</li>
</ul>
<p><strong>Problem</strong>: Expensive, slow, not reproducible.</p>
<h3 id="automatic-metrics">Automatic Metrics</h3>
<h4 id="bleu-bilingual-evaluation-understudy">BLEU (Bilingual Evaluation Understudy)</h4>
<p>The classic MT metric.</p>
<p><strong>Core idea</strong>: Count n-gram matches between output and reference.</p>
<p>$$\text{BLEU} = BP \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)$$</p>
<p>Where:</p>
<ul>
<li>$p_n$ = precision for n-grams (typically n=1,2,3,4)</li>
<li>$w_n$ = weights (usually uniform: 0.25 each)</li>
<li>BP = brevity penalty (penalizes short translations)</li>
</ul>
<p><strong>Brevity Penalty</strong>:
$$BP = \min\left(1, \exp\left(1 - \frac{r}{c}\right)\right)$$</p>
<p>Where r = reference length, c = candidate length.</p>
<p><strong>BLEU ranges</strong>: 0 to 100 (100 = perfect match).</p>
<p><strong>Limitations</strong>:</p>
<ul>
<li>Doesn&#39;t capture meaning (just surface n-grams)</li>
<li>One reference may miss valid alternatives</li>
<li>Insensitive to word order issues</li>
</ul>
<h4 id="chrf-character-f-score">chrF (Character F-Score)</h4>
<p>Uses character n-grams instead of word n-grams.</p>
<p><strong>Benefits</strong>:</p>
<ul>
<li>Works for languages without clear word boundaries</li>
<li>More robust to morphological variation</li>
<li>Often correlates better with human judgment</li>
</ul>
<h4 id="bertscore">BERTScore</h4>
<p>Uses neural embeddings for semantic matching.</p>
<ol>
<li>Embed each token in reference and hypothesis using BERT</li>
<li>Greedily match tokens by cosine similarity</li>
<li>Compute precision, recall, F1 from matches</li>
</ol>
<p><strong>Benefits</strong>:</p>
<ul>
<li>Captures semantic similarity, not just surface form</li>
<li>&quot;automobile&quot; matches &quot;car&quot; even though different words</li>
</ul>
<hr>
<h2 id="decoding-strategies">Decoding Strategies</h2>
<h3 id="the-challenge-1">The Challenge</h3>
<p>At each step, we have a probability distribution over the entire vocabulary.</p>
<p><strong>Goal</strong>: Find the most likely complete sequence.</p>
<p><strong>Problem</strong>: Exhaustive search is intractable (V^T possibilities).</p>
<h3 id="greedy-decoding">Greedy Decoding</h3>
<p>Pick highest probability token at each step:
$$y_t = \arg\max_y P(y | y_{&lt;t}, x)$$</p>
<p><strong>Pros</strong>: Fast, simple.
<strong>Cons</strong>: Locally optimal ‚â† globally optimal. High-probability first word might lead to low-probability continuation.</p>
<h3 id="beam-search">Beam Search</h3>
<p>Keep top-K hypotheses at each step:</p>
<ol>
<li>Start with K copies of &lt;BOS&gt;</li>
<li>Expand each hypothesis with all possible next tokens</li>
<li>Keep top K by total probability</li>
<li>Repeat until all hypotheses end with &lt;EOS&gt;</li>
<li>Return highest-scoring complete hypothesis</li>
</ol>
<p><strong>Beam width K</strong>:</p>
<ul>
<li>K=1 is greedy decoding</li>
<li>K=5-10 typical for translation</li>
<li>Larger K = better but slower</li>
</ul>
<p><strong>Length normalization</strong> (prevent bias toward short sequences):
$$\text{score} = \frac{\log P(Y|X)}{|Y|^\alpha}$$</p>
<p>Where Œ± ‚âà 0.6-0.7.</p>
<h3 id="sampling-strategies-for-generation">Sampling Strategies (for Generation)</h3>
<p>For creative text generation, we want <strong>diversity</strong>, not just the most likely output.</p>
<p><strong>Top-K Sampling</strong>:</p>
<ol>
<li>Keep only top K most probable tokens</li>
<li>Redistribute probability among them</li>
<li>Sample from this truncated distribution</li>
</ol>
<p><strong>Top-P (Nucleus) Sampling</strong>:</p>
<ol>
<li>Sort tokens by probability</li>
<li>Keep smallest set with cumulative probability &gt; p</li>
<li>Sample from this set</li>
</ol>
<p><strong>Temperature</strong> (before softmax):
$$P(y) = \frac{\exp(z_y / T)}{\sum_j \exp(z_j / T)}$$</p>
<ul>
<li>T &lt; 1: More peaked (confident, less diverse)</li>
<li>T = 1: Original distribution</li>
<li>T &gt; 1: Flatter (more random, more diverse)</li>
</ul>
<hr>
<h2 id="summary">Summary</h2>
<table>
<thead>
<tr>
<th>Component</th>
<th>Purpose</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Encoder</strong></td>
<td>Compress input to representation</td>
</tr>
<tr>
<td><strong>Decoder</strong></td>
<td>Generate output autoregressively</td>
</tr>
<tr>
<td><strong>Attention</strong></td>
<td>Dynamic context at each step</td>
</tr>
<tr>
<td><strong>Cross-attention</strong></td>
<td>Transformer way of connecting encoder to decoder</td>
</tr>
<tr>
<td><strong>Beam search</strong></td>
<td>Better than greedy, tractable search</td>
</tr>
<tr>
<td><strong>BLEU/BERTScore</strong></td>
<td>Automatic evaluation</td>
</tr>
</tbody></table>
<h3 id="key-takeaways">Key Takeaways</h3>
<ol>
<li><strong>Encoder-decoder</strong> is the standard architecture for sequence transduction</li>
<li><strong>Attention</strong> solves the bottleneck problem and provides interpretability</li>
<li><strong>Evaluation is hard</strong> ‚Äî automatic metrics are imperfect proxies for quality</li>
<li><strong>Decoding strategy matters</strong> ‚Äî beam search for accuracy, sampling for diversity</li>
</ol>

        </article>
        <nav class="page-navigation">
        <a href="jfsky-04-sequence.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Sequence Architectures: RNNs, LSTMs, and Attention</span>
        </a>
        <a href="jfsky-06-transfer.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Transfer Learning and Pre-trained Models</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>