<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>N-Grams and Language Models | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="jfsky-02-tokenization.html" class="active">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../probml/probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="../probml/probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="../probml/probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="../probml/probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="../probml/probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="../probml/probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="../probml/probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="../probml/probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="../probml/probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="../probml/probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">Jurafsky</a>
          </div>
          <h1 class="page-title">N-Grams and Language Models</h1>
          <div class="page-meta"><span class="tag">Jurafsky</span></div>
        </header>
        <article class="content">
          <h1 id="n-grams-and-language-models">N-Grams and Language Models</h1>
<p>Language models assign probabilities to sequences of words. They answer: &quot;How likely is this sentence?&quot; This fundamental capability underlies spell checking, machine translation, speech recognition, and text generation.</p>
<h2 id="the-big-picture">The Big Picture</h2>
<p><strong>Key Question</strong>: What&#39;s the probability of a word sequence?</p>
<p>$$P(\text{&quot;the cat sat on the mat&quot;})$$</p>
<p><strong>Why This Matters</strong>:</p>
<ul>
<li>Spell checking: P(&quot;the cat&quot;) &gt; P(&quot;the kat&quot;)</li>
<li>Machine translation: Choose more fluent translation</li>
<li>Speech recognition: Distinguish &quot;recognize speech&quot; from &quot;wreck a nice beach&quot;</li>
<li>Text generation: Sample likely continuations</li>
</ul>
<hr>
<h2 id="language-model-fundamentals">Language Model Fundamentals</h2>
<h3 id="joint-probability-of-words">Joint Probability of Words</h3>
<p>We want to compute:
$$P(w_1, w_2, ..., w_n)$$</p>
<p>Using the <strong>chain rule</strong> of probability:
$$P(w_1, w_2, ..., w_n) = P(w_1) \times P(w_2|w_1) \times P(w_3|w_1,w_2) \times ... \times P(w_n|w_1,...,w_{n-1})$$</p>
<p>More compactly:
$$P(w_1, w_2, ..., w_n) = \prod_{i=1}^{n} P(w_i | w_{1:i-1})$$</p>
<p><strong>Problem</strong>: As sequences get longer, we need infinitely many parameters!</p>
<h3 id="the-markov-assumption">The Markov Assumption</h3>
<p><strong>Key insight</strong>: Approximate by using only recent history.</p>
<p>For a <strong>bigram model</strong> (2-gram):
$$P(w_n | w_1, w_2, ..., w_{n-1}) \approx P(w_n | w_{n-1})$$</p>
<p><strong>Assumption</strong>: The next word depends only on the previous word.</p>
<h3 id="n-gram-models">N-Gram Models</h3>
<table>
<thead>
<tr>
<th>Model</th>
<th>Conditioning Context</th>
<th>Example</th>
</tr>
</thead>
<tbody><tr>
<td>Unigram</td>
<td>None</td>
<td>P(the)</td>
</tr>
<tr>
<td>Bigram</td>
<td>Previous 1 word</td>
<td>P(cat | the)</td>
</tr>
<tr>
<td>Trigram</td>
<td>Previous 2 words</td>
<td>P(sat | the cat)</td>
</tr>
<tr>
<td>4-gram</td>
<td>Previous 3 words</td>
<td>P(on | the cat sat)</td>
</tr>
</tbody></table>
<p><strong>Trade-off</strong>: Larger n = better context but more parameters and sparser data.</p>
<hr>
<h2 id="estimating-n-gram-probabilities">Estimating N-Gram Probabilities</h2>
<h3 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h3>
<p>Use <strong>relative frequency</strong> (counting):</p>
<p><strong>Bigram probability</strong>:
$$P(w_n | w_{n-1}) = \frac{\text{count}(w_{n-1}, w_n)}{\text{count}(w_{n-1})}$$</p>
<p><strong>Example</strong>: Computing P(sat | the) from a corpus:</p>
<ul>
<li>Count &quot;the sat&quot; occurrences: 100</li>
<li>Count &quot;the ___&quot; occurrences: 10,000</li>
<li>P(sat | the) = 100/10,000 = 0.01</li>
</ul>
<h3 id="handling-sentence-boundaries">Handling Sentence Boundaries</h3>
<p>Add special tokens:</p>
<ul>
<li><strong>&lt;s&gt;</strong>: Beginning of sentence (BOS)</li>
<li><strong>&lt;/s&gt;</strong>: End of sentence (EOS)</li>
</ul>
<p><strong>Example</strong>: &quot;&lt;s&gt; the cat sat &lt;/s&gt;&quot;</p>
<p>This allows us to model:</p>
<ul>
<li>P(the | &lt;s&gt;) ‚Äî how likely is &quot;the&quot; to start a sentence?</li>
<li>P(&lt;/s&gt; | sat) ‚Äî how likely is &quot;sat&quot; to end a sentence?</li>
</ul>
<h3 id="practical-note-log-probabilities">Practical Note: Log Probabilities</h3>
<p>Multiplying many small probabilities ‚Üí numerical underflow!</p>
<p><strong>Solution</strong>: Work in log space:
$$\log(p_1 \times p_2) = \log(p_1) + \log(p_2)$$</p>
<p>Add log probabilities instead of multiplying probabilities.</p>
<hr>
<h2 id="perplexity">Perplexity</h2>
<h3 id="what-is-perplexity">What Is Perplexity?</h3>
<p>The standard metric for evaluating language models.</p>
<p><strong>Definition</strong>:
$$\text{PP}(W) = P(w_1, w_2, ..., w_n)^{-1/n}$$</p>
<p>Equivalently:
$$\text{PP}(W) = \sqrt[n]{\prod_{i=1}^{n} \frac{1}{P(w_i | w_{1:i-1})}}$$</p>
<h3 id="intuition-weighted-branching-factor">Intuition: Weighted Branching Factor</h3>
<p>Perplexity ‚âà average number of equally likely choices at each step.</p>
<p><strong>Example</strong>:</p>
<ul>
<li>Perplexity of 100 ‚âà model is choosing between 100 equally likely words</li>
<li>Perplexity of 10 ‚âà model is choosing between 10 equally likely words</li>
</ul>
<p><strong>Lower perplexity = better model</strong> (more confident predictions).</p>
<h3 id="connection-to-information-theory">Connection to Information Theory</h3>
<p>Perplexity relates to <strong>entropy</strong>:
$$\text{PP}(W) = 2^{H(W)}$$</p>
<p>Where entropy H measures uncertainty:
$$H(W) = -\frac{1}{n} \log_2 P(w_1, ..., w_n)$$</p>
<h3 id="important-caveats">Important Caveats</h3>
<p><strong>Perplexities are only comparable when</strong>:</p>
<ul>
<li>Using the same vocabulary</li>
<li>Using the same test set</li>
</ul>
<p>Adding rare words to vocabulary increases perplexity (more choices).</p>
<hr>
<h2 id="the-unknown-word-problem">The Unknown Word Problem</h2>
<h3 id="the-problem">The Problem</h3>
<p>What if we see a word we&#39;ve never seen before?
$$P(\text{unigoogleable} | w_{n-1}) = 0$$</p>
<p>Zero probability breaks everything:</p>
<ul>
<li>Product becomes zero</li>
<li>Perplexity becomes infinite</li>
</ul>
<h3 id="solution-unk-token">Solution: &lt;UNK&gt; Token</h3>
<ol>
<li>Define a vocabulary (e.g., most frequent 50,000 words)</li>
<li>Replace all other words with &lt;UNK&gt;</li>
<li>Treat &lt;UNK&gt; as just another word</li>
</ol>
<p><strong>Training</strong>: &quot;I saw a brachiosaurus&quot; ‚Üí &quot;I saw a &lt;UNK&gt;&quot;
<strong>Testing</strong>: Same replacement</p>
<p><strong>Caveat</strong>: Smaller vocabulary = lower perplexity (fewer choices). Not always fair to compare!</p>
<hr>
<h2 id="smoothing">Smoothing</h2>
<h3 id="the-zero-probability-problem">The Zero Probability Problem</h3>
<p>Even with &lt;UNK&gt;, we&#39;ll see <strong>new n-grams</strong>:
$$P(\text{cat} | \text{the green}) = 0 \text{ (if never seen together)}$$</p>
<p><strong>Smoothing</strong> redistributes probability mass to unseen events.</p>
<h3 id="laplace-add-one-smoothing">Laplace (Add-One) Smoothing</h3>
<p><strong>Idea</strong>: Pretend we saw everything at least once.</p>
<p><strong>Unigram</strong>:
$$P(w_i) = \frac{\text{count}(w_i) + 1}{N + V}$$</p>
<p><strong>Bigram</strong>:
$$P(w_i | w_j) = \frac{\text{count}(w_j, w_i) + 1}{\text{count}(w_j) + V}$$</p>
<p>Where V is vocabulary size.</p>
<p><strong>Problem</strong>: Add-1 is too aggressive ‚Äî steals too much from seen events.</p>
<p><strong>Solution</strong>: Add-k smoothing (k &lt; 1, e.g., k = 0.01).</p>
<h3 id="backoff">Backoff</h3>
<p><strong>Idea</strong>: If no evidence for trigram, use bigram. If no bigram, use unigram.</p>
<p>$$P_{BO}(w_i|w_{i-2},w_{i-1}) = \begin{cases} 
P(w_i|w_{i-2},w_{i-1}) &amp; \text{if count &gt; 0} \
\alpha \cdot P_{BO}(w_i|w_{i-1}) &amp; \text{otherwise}
\end{cases}$$</p>
<p>Where Œ± is a normalization factor.</p>
<h3 id="interpolation">Interpolation</h3>
<p><strong>Idea</strong>: Always mix all n-gram levels.</p>
<p>$$P(w_i | w_{i-2}, w_{i-1}) = \lambda_1 P(w_i) + \lambda_2 P(w_i | w_{i-1}) + \lambda_3 P(w_i | w_{i-2}, w_{i-1})$$</p>
<p>Where $\lambda_1 + \lambda_2 + \lambda_3 = 1$.</p>
<p>Learn Œª values from held-out data.</p>
<h3 id="kneser-ney-smoothing">Kneser-Ney Smoothing</h3>
<p><strong>State-of-the-art for n-gram models</strong>.</p>
<p><strong>Key insight</strong>: Use <em>continuation probability</em> ‚Äî how likely is a word to appear in new contexts?</p>
<p>$$P_{KN}(w_i | w_{i-1}) = \frac{\max(\text{count}(w_{i-1}, w_i) - d, 0)}{\sum_v \text{count}(w_{i-1}, v)} + \lambda(w_{i-1}) P_{continuation}(w_i)$$</p>
<p><strong>Intuition</strong>: </p>
<ul>
<li>Words like &quot;Francisco&quot; have high count but appear after &quot;San&quot; ‚Äî low continuation probability</li>
<li>Words like &quot;the&quot; appear in many contexts ‚Äî high continuation probability</li>
</ul>
<hr>
<h2 id="efficiency-considerations">Efficiency Considerations</h2>
<p>N-gram models can be huge! Practical tricks:</p>
<table>
<thead>
<tr>
<th>Technique</th>
<th>Purpose</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Quantization</strong></td>
<td>Store probabilities with fewer bits</td>
</tr>
<tr>
<td><strong>Tries</strong></td>
<td>Efficient storage and lookup</td>
</tr>
<tr>
<td><strong>String hashing</strong></td>
<td>Reduce memory for n-grams</td>
</tr>
<tr>
<td><strong>Bloom filters</strong></td>
<td>Fast membership testing</td>
</tr>
<tr>
<td><strong>Stupid Backoff</strong></td>
<td>Simple, fast approximation</td>
</tr>
</tbody></table>
<hr>
<h2 id="summary">Summary</h2>
<table>
<thead>
<tr>
<th>Concept</th>
<th>Key Idea</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Language Model</strong></td>
<td>Assign probabilities to word sequences</td>
</tr>
<tr>
<td><strong>N-gram</strong></td>
<td>Approximate using last n-1 words</td>
</tr>
<tr>
<td><strong>MLE</strong></td>
<td>Estimate from counts (relative frequency)</td>
</tr>
<tr>
<td><strong>Perplexity</strong></td>
<td>Model evaluation metric (lower = better)</td>
</tr>
<tr>
<td><strong>Smoothing</strong></td>
<td>Handle unseen n-grams</td>
</tr>
<tr>
<td><strong>Kneser-Ney</strong></td>
<td>Best n-gram smoothing technique</td>
</tr>
</tbody></table>
<h3 id="the-bigger-picture">The Bigger Picture</h3>
<p>N-gram models are:</p>
<ul>
<li><strong>Simple and interpretable</strong></li>
<li><strong>Capture local syntax</strong> (word order)</li>
<li><strong>Fast</strong> to train and use</li>
</ul>
<p>But they have <strong>limitations</strong>:</p>
<ul>
<li><strong>Fixed context</strong> (can&#39;t capture long-range dependencies)</li>
<li><strong>Sparse data</strong> (even trigrams need lots of data)</li>
<li><strong>No semantic understanding</strong> (just counting patterns)</li>
</ul>
<p>This motivates <strong>neural language models</strong> (covered in later chapters).</p>

        </article>
        <nav class="page-navigation">
        <a href="jfsky-01-regex.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Regular Expressions and Text Processing</span>
        </a>
        <a href="jfsky-03-vectors.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Vector Semantics and Word Embeddings</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>