<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Transfer Learning and Pre-trained Models | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="jfsky-06-transfer.html" class="active">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../probml/probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="../probml/probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="../probml/probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="../probml/probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="../probml/probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="../probml/probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="../probml/probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="../probml/probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="../probml/probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="../probml/probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">Jurafsky</a>
          </div>
          <h1 class="page-title">Transfer Learning and Pre-trained Models</h1>
          <div class="page-meta"><span class="tag">Jurafsky</span></div>
        </header>
        <article class="content">
          <h1 id="transfer-learning-and-pre-trained-models">Transfer Learning and Pre-trained Models</h1>
<p>Transfer learning has revolutionized NLP. Pre-train a large model on massive text data, then fine-tune for specific tasks. This chapter covers BERT and the pre-train/fine-tune paradigm.</p>
<h2 id="the-big-picture">The Big Picture</h2>
<p><strong>The Old Way</strong>: Train a model from scratch for each task.</p>
<ul>
<li>Requires lots of labeled data</li>
<li>Each task starts from zero</li>
</ul>
<p><strong>The New Way</strong>: Pre-train ‚Üí Fine-tune.</p>
<ul>
<li>Pre-train once on huge unlabeled corpus</li>
<li>Fine-tune with small labeled dataset per task</li>
<li>Transfer linguistic knowledge across tasks</li>
</ul>
<hr>
<h2 id="key-concepts">Key Concepts</h2>
<h3 id="contextual-embeddings">Contextual Embeddings</h3>
<p><strong>Static embeddings</strong> (Word2Vec): Same vector for &quot;bank&quot; regardless of context.</p>
<p><strong>Contextual embeddings</strong> (BERT): Different vector for &quot;river bank&quot; vs. &quot;bank account&quot;.</p>
<p>The same word gets different representations based on surrounding words.</p>
<h3 id="the-pre-train-‚Üí-fine-tune-paradigm">The Pre-train ‚Üí Fine-tune Paradigm</h3>
<pre><code>[MASSIVE UNLABELED TEXT] ‚Üí Pre-training ‚Üí [GENERAL LANGUAGE MODEL]
                                                    ‚Üì
[SMALL LABELED DATA] ‚Üí Fine-tuning ‚Üí [TASK-SPECIFIC MODEL]
</code></pre>
<p><strong>Pre-training</strong>: Self-supervised learning on vast text (books, Wikipedia, web).</p>
<p><strong>Fine-tuning</strong>: Supervised learning on task-specific data.</p>
<h3 id="language-model-types">Language Model Types</h3>
<table>
<thead>
<tr>
<th>Type</th>
<th>Direction</th>
<th>Example</th>
<th>Best For</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Causal</strong></td>
<td>Left-to-right</td>
<td>GPT</td>
<td>Generation</td>
</tr>
<tr>
<td><strong>Bidirectional</strong></td>
<td>Both directions</td>
<td>BERT</td>
<td>Understanding</td>
</tr>
<tr>
<td><strong>Encoder-Decoder</strong></td>
<td>Both + generation</td>
<td>T5</td>
<td>Both</td>
</tr>
</tbody></table>
<hr>
<h2 id="bidirectional-transformers-bert">Bidirectional Transformers (BERT)</h2>
<h3 id="why-bidirectional">Why Bidirectional?</h3>
<p>For many tasks, we can see the entire input at once.</p>
<p><strong>Causal models</strong> (GPT): Can only see left context.</p>
<pre><code>&quot;The cat sat on the [???]&quot; - what comes next?
</code></pre>
<p><strong>Bidirectional models</strong> (BERT): See full context.</p>
<pre><code>&quot;The cat sat on the [MASK]&quot; - what&#39;s missing?
</code></pre>
<h3 id="bert-architecture">BERT Architecture</h3>
<p><strong>Self-attention</strong> across entire sequence:
$$\text{Attention} = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$</p>
<p><strong>BERT Base</strong>:</p>
<ul>
<li>Vocabulary: 30K subwords (WordPiece)</li>
<li>Hidden size: 768 (12 heads √ó 64 dims)</li>
<li>Layers: 12</li>
<li>Parameters: 110M</li>
<li>Max sequence: 512 tokens</li>
</ul>
<p><strong>Compute note</strong>: Attention is O(n¬≤) in sequence length ‚Äî limits max length.</p>
<hr>
<h2 id="pre-training-objectives">Pre-training Objectives</h2>
<h3 id="masked-language-modeling-mlm">Masked Language Modeling (MLM)</h3>
<p><strong>The task</strong>: Predict randomly masked tokens.</p>
<pre><code>Input:  The cat [MASK] on the mat
Target: sat
</code></pre>
<p><strong>Masking strategy</strong> (for 15% of tokens):</p>
<ul>
<li>80%: Replace with [MASK]</li>
<li>10%: Replace with random word</li>
<li>10%: Keep original</li>
</ul>
<p><strong>Why this mix?</strong></p>
<ul>
<li>[MASK] never appears at fine-tuning ‚Üí train on real words too</li>
<li>Random replacement adds noise, prevents overfitting</li>
<li>Keeping some originals helps learn bidirectional context</li>
</ul>
<h3 id="span-masking-spanbert">Span Masking (SpanBERT)</h3>
<p>Mask contiguous spans instead of random tokens.</p>
<pre><code>Input:  The [MASK] [MASK] [MASK] the mat
Target: cat sat on
</code></pre>
<p><strong>Benefits</strong>:</p>
<ul>
<li>Better for tasks requiring span understanding (QA, NER)</li>
<li>Span Boundary Objective: Predict span from boundary tokens</li>
</ul>
<h3 id="next-sentence-prediction-nsp">Next Sentence Prediction (NSP)</h3>
<p><strong>The task</strong>: Are these sentences adjacent in the original text?</p>
<pre><code>[CLS] The cat sat [SEP] It was happy [SEP] ‚Üí IsNext
[CLS] The cat sat [SEP] I like pizza [SEP] ‚Üí NotNext
</code></pre>
<p><strong>Training data</strong>: 50% real pairs, 50% random pairs.</p>
<p><strong>Note</strong>: Later models (RoBERTa, ALBERT) found NSP less helpful than expected.</p>
<hr>
<h2 id="pre-training-data">Pre-training Data</h2>
<p>BERT was trained on:</p>
<ul>
<li><strong>BooksCorpus</strong>: 800M words</li>
<li><strong>English Wikipedia</strong>: 2.5B words</li>
</ul>
<p>Later models use more:</p>
<ul>
<li>CommonCrawl (filtered web text)</li>
<li>News articles</li>
<li>Code repositories</li>
</ul>
<p><strong>Compute requirement</strong>: Days to weeks on TPU/GPU clusters.</p>
<hr>
<h2 id="fine-tuning">Fine-tuning</h2>
<h3 id="the-basic-recipe">The Basic Recipe</h3>
<ol>
<li>Load pre-trained model</li>
<li>Add task-specific <strong>classification head</strong> (usually 1-2 layers)</li>
<li>Train on labeled data with small learning rate</li>
</ol>
<h3 id="fine-tuning-strategies">Fine-tuning Strategies</h3>
<table>
<thead>
<tr>
<th>Strategy</th>
<th>What&#39;s Updated</th>
<th>Best For</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Full fine-tuning</strong></td>
<td>All parameters</td>
<td>Lots of data, maximum performance</td>
</tr>
<tr>
<td><strong>Feature extraction</strong></td>
<td>Only head</td>
<td>Very small data</td>
</tr>
<tr>
<td><strong>Adapter tuning</strong></td>
<td>Small inserted modules</td>
<td>Efficient multi-task</td>
</tr>
<tr>
<td><strong>Prompt tuning</strong></td>
<td>Soft prompts only</td>
<td>Very large models</td>
</tr>
</tbody></table>
<h3 id="hyperparameters">Hyperparameters</h3>
<p><strong>Learning rate</strong>: 2e-5 to 5e-5 (much smaller than training from scratch!)</p>
<p><strong>Epochs</strong>: 2-4 (often sufficient)</p>
<p><strong>Batch size</strong>: 16-32</p>
<hr>
<h2 id="task-specific-architectures">Task-Specific Architectures</h2>
<h3 id="sequence-classification">Sequence Classification</h3>
<p><strong>Task</strong>: Classify entire input (sentiment, topic).</p>
<pre><code>Input:  [CLS] I loved this movie [SEP]
Output: Use [CLS] representation ‚Üí linear ‚Üí softmax ‚Üí class
</code></pre>
<h3 id="sentence-pair-classification">Sentence Pair Classification</h3>
<p><strong>Task</strong>: Classify relationship between two texts (NLI, similarity).</p>
<pre><code>Input:  [CLS] Premise text [SEP] Hypothesis text [SEP]
Output: [CLS] representation ‚Üí linear ‚Üí entailment/contradiction/neutral
</code></pre>
<h3 id="token-classification-ner-pos">Token Classification (NER, POS)</h3>
<p><strong>Task</strong>: Label each token.</p>
<pre><code>Input:  [CLS] John lives in New York [SEP]
Labels:       B-PER O     O  B-LOC I-LOC
</code></pre>
<p>Each token gets its own classification head output.</p>
<p><strong>WordPiece handling</strong>:</p>
<ul>
<li>Training: Expand labels to all subword tokens</li>
<li>Evaluation: Use label of first subword</li>
</ul>
<h3 id="span-prediction-qa">Span Prediction (QA)</h3>
<p><strong>Task</strong>: Find answer span in context.</p>
<pre><code>Input:  [CLS] Where is Paris? [SEP] Paris is in France [SEP]
Output: Predict start position (index 5: &quot;Paris&quot;)
        Predict end position (index 8: &quot;France&quot;)
</code></pre>
<p>Two classifiers: one for start, one for end position.</p>
<hr>
<h2 id="modern-variants">Modern Variants</h2>
<h3 id="roberta-robustly-optimized-bert">RoBERTa (Robustly Optimized BERT)</h3>
<p>Key changes:</p>
<ul>
<li>Remove NSP objective</li>
<li>Larger batches, more data</li>
<li>Dynamic masking (different masks each epoch)</li>
</ul>
<h3 id="albert-a-lite-bert">ALBERT (A Lite BERT)</h3>
<p>Parameter reduction:</p>
<ul>
<li>Factorized embedding parameters</li>
<li>Cross-layer parameter sharing</li>
<li>Sentence order prediction instead of NSP</li>
</ul>
<h3 id="distilbert">DistilBERT</h3>
<p>Knowledge distillation:</p>
<ul>
<li>40% smaller, 60% faster</li>
<li>97% of BERT performance</li>
</ul>
<hr>
<h2 id="summary">Summary</h2>
<table>
<thead>
<tr>
<th>Concept</th>
<th>Key Point</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Contextual embeddings</strong></td>
<td>Same word ‚Üí different vectors in different contexts</td>
</tr>
<tr>
<td><strong>Pre-training</strong></td>
<td>Self-supervised on massive text</td>
</tr>
<tr>
<td><strong>Fine-tuning</strong></td>
<td>Task-specific with small labeled data</td>
</tr>
<tr>
<td><strong>MLM</strong></td>
<td>Predict masked tokens (BERT&#39;s main objective)</td>
</tr>
<tr>
<td><strong>[CLS] token</strong></td>
<td>Aggregate representation for classification</td>
</tr>
<tr>
<td><strong>[SEP] token</strong></td>
<td>Separate segments in input</td>
</tr>
</tbody></table>
<h3 id="the-revolution">The Revolution</h3>
<p>Transfer learning changed NLP:</p>
<table>
<thead>
<tr>
<th>Before</th>
<th>After</th>
</tr>
</thead>
<tbody><tr>
<td>Train from scratch per task</td>
<td>Pre-train once, fine-tune many times</td>
</tr>
<tr>
<td>Need lots of labeled data</td>
<td>Works with small labeled data</td>
</tr>
<tr>
<td>Shallow features</td>
<td>Deep contextual understanding</td>
</tr>
<tr>
<td>Task-specific architectures</td>
<td>One architecture, many tasks</td>
</tr>
</tbody></table>
<h3 id="practical-tips">Practical Tips</h3>
<ol>
<li><strong>Start with pre-trained models</strong> ‚Äî rarely worth training from scratch</li>
<li><strong>Try multiple learning rates</strong> ‚Äî this is the most important hyperparameter</li>
<li><strong>Don&#39;t over-fine-tune</strong> ‚Äî 2-4 epochs is often enough</li>
<li><strong>Consider model size</strong> ‚Äî DistilBERT for production, BERT-large for best accuracy</li>
<li><strong>Domain matters</strong> ‚Äî SciBERT for science, BioBERT for biomedical, etc.</li>
</ol>

        </article>
        <nav class="page-navigation">
        <a href="jfsky-05-encoder.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Encoder-Decoder Models</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>