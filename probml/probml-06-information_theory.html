<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Information Theory | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="probml-06-information_theory.html" class="active">Information Theory</a></li>
            <li class="nav-item"><a href="probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">ProbML</a>
          </div>
          <h1 class="page-title">Information Theory</h1>
          <div class="page-meta"><span class="tag">ProbML</span></div>
        </header>
        <article class="content">
          <h1 id="information-theory">Information Theory</h1>
<p>Information theory provides mathematical tools for quantifying information, uncertainty, and the relationships between random variables. Originally developed for communication systems, it&#39;s now fundamental to machine learning.</p>
<h2 id="the-big-picture">The Big Picture</h2>
<p>Information theory answers questions like:</p>
<ul>
<li>How much uncertainty is in a distribution?</li>
<li>How different are two distributions?</li>
<li>How much does knowing X tell us about Y?</li>
</ul>
<p>These concepts are central to understanding loss functions, model evaluation, and feature selection.</p>
<hr>
<h2 id="entropy">Entropy</h2>
<h3 id="definition">Definition</h3>
<p><strong>Entropy</strong> measures the uncertainty or &quot;unpredictability&quot; of a random variable:</p>
<p>$$H(X) = -\sum_x p(x) \log p(x) = -\mathbb{E}[\log p(X)]$$</p>
<p><strong>Intuition</strong>: How many bits do we need, on average, to encode samples from this distribution?</p>
<h3 id="key-properties">Key Properties</h3>
<ul>
<li><strong>Non-negative</strong>: $H(X) \geq 0$</li>
<li><strong>Maximum for uniform distribution</strong>: If all outcomes equally likely, uncertainty is maximized</li>
<li><strong>Minimum for deterministic</strong>: $H(X) = 0$ when outcome is certain (Dirac delta)</li>
</ul>
<h3 id="examples">Examples</h3>
<p><strong>Fair coin</strong>: $H = -\frac{1}{2}\log\frac{1}{2} - \frac{1}{2}\log\frac{1}{2} = 1$ bit</p>
<p><strong>Biased coin (p=0.9)</strong>: $H = -0.9\log 0.9 - 0.1\log 0.1 \approx 0.47$ bits</p>
<p><strong>More predictable ‚Üí lower entropy!</strong></p>
<hr>
<h2 id="cross-entropy">Cross-Entropy</h2>
<h3 id="definition-1">Definition</h3>
<p>Cross-entropy measures the average number of bits needed to encode data from distribution p using a code optimized for distribution q:</p>
<p>$$H(p, q) = -\sum_x p(x) \log q(x)$$</p>
<p><strong>Intuition</strong>: How well does q approximate p?</p>
<h3 id="key-properties-1">Key Properties</h3>
<ul>
<li>$H(p, q) \geq H(p)$ (equality when p = q)</li>
<li>Cross-entropy is what we minimize in classification!</li>
</ul>
<h3 id="connection-to-machine-learning">Connection to Machine Learning</h3>
<p>When training a classifier:</p>
<ul>
<li><strong>p</strong> = true distribution (one-hot labels)</li>
<li><strong>q</strong> = predicted distribution (softmax outputs)</li>
</ul>
<p><strong>Cross-entropy loss</strong>:
$$\mathcal{L} = -\sum_c y_c \log \hat{y}_c$$</p>
<p>For one-hot labels, this simplifies to: $-\log \hat{y}_{true}$</p>
<hr>
<h2 id="joint-and-conditional-entropy">Joint and Conditional Entropy</h2>
<h3 id="joint-entropy">Joint Entropy</h3>
<p>Uncertainty about both X and Y together:
$$H(X, Y) = -\sum_{x,y} p(x, y) \log p(x, y)$$</p>
<h3 id="conditional-entropy">Conditional Entropy</h3>
<p>Remaining uncertainty about Y after observing X:
$$H(Y | X) = \sum_x p(x) H(Y | X = x) = -\sum_{x,y} p(x,y) \log p(y|x)$$</p>
<h3 id="chain-rule">Chain Rule</h3>
<p>$$H(X, Y) = H(X) + H(Y | X)$$</p>
<p><strong>Intuition</strong>: Total uncertainty = uncertainty in X + remaining uncertainty in Y given X.</p>
<hr>
<h2 id="perplexity">Perplexity</h2>
<h3 id="definition-2">Definition</h3>
<p>Perplexity is the exponentiated cross-entropy:
$$\text{Perplexity}(p, q) = 2^{H(p, q)}$$</p>
<p>Or for a sequence of N tokens:
$$\text{Perplexity} = \sqrt[N]{\prod_{i=1}^N \frac{1}{p(x_i)}}$$</p>
<p><strong>Interpretation</strong>: The weighted average number of choices (branching factor) the model is uncertain between.</p>
<h3 id="use-in-language-models">Use in Language Models</h3>
<ul>
<li><strong>Lower perplexity</strong> = better model</li>
<li>Perplexity of 1 = perfect prediction</li>
<li>Perplexity of V (vocabulary size) = random guessing</li>
</ul>
<p><strong>Example</strong>: Perplexity of 50 means the model is, on average, choosing between 50 equally likely next words.</p>
<hr>
<h2 id="kl-divergence">KL Divergence</h2>
<h3 id="definition-3">Definition</h3>
<p><strong>Kullback-Leibler divergence</strong> measures how different distribution q is from distribution p:</p>
<p>$$D_{KL}(p | q) = \sum_x p(x) \log \frac{p(x)}{q(x)} = H(p, q) - H(p)$$</p>
<p><strong>Intuition</strong>: Extra bits needed when using code for q to encode data from p.</p>
<h3 id="key-properties-2">Key Properties</h3>
<ul>
<li><strong>Non-negative</strong>: $D_{KL}(p | q) \geq 0$ (Gibbs&#39; inequality)</li>
<li><strong>Zero iff p = q</strong>: Perfect match</li>
<li><strong>Asymmetric</strong>: $D_{KL}(p | q) \neq D_{KL}(q | p)$ in general</li>
<li><strong>Not a true distance</strong> (asymmetric, no triangle inequality)</li>
</ul>
<h3 id="connection-to-mle">Connection to MLE</h3>
<p>When we minimize NLL, we&#39;re minimizing:
$$\text{NLL} = -\frac{1}{N}\sum_i \log q(x_i)$$</p>
<p>If samples come from empirical distribution $\hat{p}$:
$$\text{NLL} = H(\hat{p}, q) = H(\hat{p}) + D_{KL}(\hat{p} | q)$$</p>
<p>Since $H(\hat{p})$ is constant, <strong>minimizing NLL = minimizing KL divergence</strong>!</p>
<h3 id="forward-vs-reverse-kl">Forward vs. Reverse KL</h3>
<p><strong>Forward KL</strong> $D_{KL}(p | q)$: p is the reference</p>
<ul>
<li>Mode-covering: q tries to cover all of p&#39;s mass</li>
<li>Penalizes q for missing modes of p</li>
</ul>
<p><strong>Reverse KL</strong> $D_{KL}(q | p)$: q is the reference</p>
<ul>
<li>Mode-seeking: q concentrates on modes of p</li>
<li>Okay to miss some modes, but penalizes placing mass where p has none</li>
</ul>
<hr>
<h2 id="mutual-information">Mutual Information</h2>
<h3 id="definition-4">Definition</h3>
<p>How much does knowing X tell us about Y?</p>
<p>$$I(X; Y) = D_{KL}(p(x, y) | p(x)p(y))$$</p>
<p>Or equivalently:
$$I(X; Y) = H(X) - H(X | Y) = H(Y) - H(Y | X)$$</p>
<p><strong>Intuition</strong>: Reduction in uncertainty about X from knowing Y.</p>
<h3 id="key-properties-3">Key Properties</h3>
<ul>
<li><strong>Symmetric</strong>: $I(X; Y) = I(Y; X)$</li>
<li><strong>Non-negative</strong>: $I(X; Y) \geq 0$</li>
<li><strong>Zero iff independent</strong>: $I(X; Y) = 0 \Leftrightarrow X \perp Y$</li>
</ul>
<h3 id="as-generalized-correlation">As Generalized Correlation</h3>
<p>Mutual information captures <strong>any</strong> dependence (not just linear), making it more general than Pearson correlation.</p>
<h3 id="data-processing-inequality">Data Processing Inequality</h3>
<p>If X ‚Üí Y ‚Üí Z forms a Markov chain:
$$I(X; Z) \leq I(X; Y)$$</p>
<p><strong>Implication</strong>: Processing cannot increase information. You can only lose information through transformations!</p>
<hr>
<h2 id="fanos-inequality">Fano&#39;s Inequality</h2>
<h3 id="statement">Statement</h3>
<p>For any estimator $\hat{X}$ of X based on Y:
$$H(X | Y) \leq H(P_e) + P_e \log(|X| - 1)$$</p>
<p>Where $P_e = P(\hat{X} \neq X)$ is the error probability.</p>
<h3 id="implications">Implications</h3>
<ul>
<li><strong>Lower bound on error</strong>: If $H(X|Y)$ is high, error must be high</li>
<li><strong>Feature selection</strong>: Features with high mutual information with the target reduce classification error</li>
</ul>
<hr>
<h2 id="applications-in-ml">Applications in ML</h2>
<h3 id="loss-functions">Loss Functions</h3>
<p><strong>Cross-entropy loss</strong> minimizes KL divergence between true and predicted distributions.</p>
<h3 id="variational-inference">Variational Inference</h3>
<p>Approximate intractable posterior by minimizing KL divergence.</p>
<h3 id="information-bottleneck">Information Bottleneck</h3>
<p>Find representations that maximally compress input while retaining relevant information about the output.</p>
<h3 id="data-augmentation">Data Augmentation</h3>
<p>Spreads probability mass over larger input space, reducing overfitting.</p>
<hr>
<h2 id="summary">Summary</h2>
<table>
<thead>
<tr>
<th>Concept</th>
<th>Formula</th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Entropy</strong></td>
<td>$H(X) = -\mathbb{E}[\log p(X)]$</td>
<td>Uncertainty in X</td>
</tr>
<tr>
<td><strong>Cross-Entropy</strong></td>
<td>$H(p, q) = -\mathbb{E}_p[\log q]$</td>
<td>Bits to encode p using q</td>
</tr>
<tr>
<td><strong>KL Divergence</strong></td>
<td>$D_{KL}(p|q) = H(p,q) - H(p)$</td>
<td>Extra bits; difference between distributions</td>
</tr>
<tr>
<td><strong>Mutual Information</strong></td>
<td>$I(X;Y) = H(X) - H(X|Y)$</td>
<td>Information shared between X and Y</td>
</tr>
<tr>
<td><strong>Perplexity</strong></td>
<td>$2^{H(p,q)}$</td>
<td>Effective vocabulary size</td>
</tr>
</tbody></table>

        </article>
        <nav class="page-navigation">
        <a href="probml-05-decision_theory.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Decision Theory</span>
        </a>
        <a href="probml-08-optimization.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Optimization</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>