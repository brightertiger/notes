<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Convolutional Neural Networks | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="probml-14-cnn.html" class="active">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">ProbML</a>
          </div>
          <h1 class="page-title">Convolutional Neural Networks</h1>
          <div class="page-meta"><span class="tag">ProbML</span></div>
        </header>
        <article class="content">
          <h1 id="convolutional-neural-networks">Convolutional Neural Networks</h1>
<p>CNNs are specialized neural networks designed for processing grid-structured data, especially images. They&#39;re the foundation of modern computer vision.</p>
<h2 id="the-big-picture">The Big Picture</h2>
<p><strong>Problem with MLPs for images</strong>:</p>
<ul>
<li>Different image sizes ‚Üí different input dimensions</li>
<li>Translation invariance is hard to learn</li>
<li>Too many parameters (e.g., 1000√ó1000 image = 3 million inputs!)</li>
</ul>
<p><strong>CNN solution</strong>:</p>
<ul>
<li>Local connectivity (each neuron sees small region)</li>
<li>Weight sharing (same filter applied everywhere)</li>
<li>Translation equivariance built in</li>
</ul>
<hr>
<h2 id="the-convolution-operation">The Convolution Operation</h2>
<h3 id="1d-convolution">1D Convolution</h3>
<p>$$[w \star x]<em>i = \sum</em>{u=0}^{L-1} w_u \cdot x_{i+u}$$</p>
<p>Slide a <strong>filter</strong> (kernel) across the input and compute dot products.</p>
<h3 id="2d-convolution">2D Convolution</h3>
<p>$$[W \star X]<em>{i,j} = \sum</em>{u=0}^{H-1} \sum_{v=0}^{W-1} w_{u,v} \cdot x_{i+u, j+v}$$</p>
<p><strong>Interpretation</strong>: Template matching. High response where input matches the filter pattern.</p>
<h3 id="key-insight-weight-sharing">Key Insight: Weight Sharing</h3>
<p>Same filter weights used at every location ‚Üí huge parameter reduction!</p>
<p><strong>Example</strong>: 3√ó3 filter has 9 parameters, regardless of image size.</p>
<hr>
<h2 id="convolution-as-matrix-multiplication">Convolution as Matrix Multiplication</h2>
<p>Convolution can be expressed as multiplication by a <strong>Toeplitz matrix</strong>:
$$y = Cx$$</p>
<p>Where C has a special sparse structure with repeated weights.</p>
<p>This equivalence is useful for:</p>
<ul>
<li>Understanding computational cost</li>
<li>Implementing on hardware</li>
</ul>
<hr>
<h2 id="convolution-variants">Convolution Variants</h2>
<h3 id="valid-convolution">Valid Convolution</h3>
<p>No padding; output shrinks:</p>
<ul>
<li>Input: $(H, W)$</li>
<li>Filter: $(f_H, f_W)$</li>
<li>Output: $(H - f_H + 1, W - f_W + 1)$</li>
</ul>
<h3 id="same-zero-padding">Same (Zero) Padding</h3>
<p>Pad input with zeros to maintain size:</p>
<ul>
<li>Padding: $p = (f - 1) / 2$</li>
<li>Output same size as input</li>
</ul>
<h3 id="strided-convolution">Strided Convolution</h3>
<p>Skip positions to downsample:</p>
<ul>
<li>Stride $s$: move filter by s pixels</li>
<li>Output size: $\lfloor(H + 2p - f)/s + 1\rfloor$</li>
</ul>
<hr>
<h2 id="multi-channel-convolutions">Multi-Channel Convolutions</h2>
<h3 id="input-with-multiple-channels">Input with Multiple Channels</h3>
<p>For RGB images (3 channels), the filter is 3D:
$$z_{i,j} = \sum_c \sum_u \sum_v x_{i+u, j+v, c} \cdot w_{u,v,c}$$</p>
<p>Each filter produces one output channel.</p>
<h3 id="multiple-filters">Multiple Filters</h3>
<p>To detect multiple features, use multiple filters:</p>
<ul>
<li>Weight tensor: $(f_H, f_W, C_{in}, C_{out})$</li>
<li>Each filter produces one channel of output</li>
</ul>
<p><strong>Output</strong>: Stack of feature maps (one per filter).</p>
<h3 id="1√ó1-convolution">1√ó1 Convolution</h3>
<p>Special case: filter size = 1√ó1</p>
<ul>
<li>Acts only across channels, not spatial</li>
<li>Like a per-pixel fully-connected layer</li>
<li>Used to change number of channels cheaply</li>
</ul>
<hr>
<h2 id="pooling-layers">Pooling Layers</h2>
<h3 id="purpose">Purpose</h3>
<ul>
<li>Reduce spatial dimensions</li>
<li>Achieve translation <strong>invariance</strong> (small shifts don&#39;t matter)</li>
<li>Reduce parameters and computation</li>
</ul>
<h3 id="max-pooling">Max Pooling</h3>
<p>Take maximum value in each window:
$$y_{i,j} = \max_{(u,v) \in \text{window}} x_{i+u, j+v}$$</p>
<p>Most common: 2√ó2 window with stride 2 (halves dimensions).</p>
<h3 id="average-pooling">Average Pooling</h3>
<p>Take mean instead of max.</p>
<h3 id="global-average-pooling">Global Average Pooling</h3>
<p>Average over entire spatial dimensions:</p>
<ul>
<li>Input: $(H, W, C)$ ‚Üí Output: $(1, 1, C)$</li>
<li>Often used before final classifier</li>
</ul>
<hr>
<h2 id="dilated-atrous-convolution">Dilated (Atrous) Convolution</h2>
<p>Insert &quot;holes&quot; in the filter:</p>
<ul>
<li>Dilation rate r: sample every r-th pixel</li>
<li>Increases <strong>receptive field</strong> without increasing parameters</li>
<li>Useful for dense prediction (segmentation)</li>
</ul>
<hr>
<h2 id="transposed-convolution">Transposed Convolution</h2>
<p>&quot;Upsampling&quot; convolution for:</p>
<ul>
<li>Autoencoders</li>
<li>Generative models</li>
<li>Semantic segmentation</li>
</ul>
<p>Increases spatial dimensions (opposite of regular conv).</p>
<hr>
<h2 id="normalization">Normalization</h2>
<h3 id="batch-normalization">Batch Normalization</h3>
<p>Normalize across the batch dimension:
$$\hat{z}_n = \frac{z_n - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$
$$\tilde{z}_n = \gamma \hat{z}_n + \beta$$</p>
<p><strong>Per channel</strong>: Compute Œº, œÉ over (N, H, W) for each channel.</p>
<p><strong>Benefits</strong>:</p>
<ul>
<li>Stabilizes training</li>
<li>Allows higher learning rates</li>
<li>Some regularization effect</li>
</ul>
<p><strong>Issues</strong>:</p>
<ul>
<li>Depends on batch statistics ‚Üí problems with small batches</li>
<li>Different behavior at train vs. test time</li>
</ul>
<h3 id="layer-normalization">Layer Normalization</h3>
<p>Normalize across channels (and spatial dims):</p>
<ul>
<li>Independent of batch size</li>
<li>Better for RNNs and Transformers</li>
</ul>
<h3 id="instance-normalization">Instance Normalization</h3>
<p>Normalize per sample, per channel:</p>
<ul>
<li>Used in style transfer</li>
</ul>
<hr>
<h2 id="common-architectures">Common Architectures</h2>
<h3 id="resnet-residual-networks">ResNet (Residual Networks)</h3>
<p><strong>Key innovation</strong>: Skip connections
$$y = F(x) + x$$</p>
<p><strong>Residual block</strong>:</p>
<pre><code>x ‚Üí Conv ‚Üí BN ‚Üí ReLU ‚Üí Conv ‚Üí BN ‚Üí (+x) ‚Üí ReLU
</code></pre>
<p>Enables training 100+ layer networks.</p>
<h3 id="densenet">DenseNet</h3>
<p><strong>Key idea</strong>: Connect each layer to all subsequent layers
$$x_l = [x_0, f_1(x_0), f_2(x_0, x_1), ...]$$</p>
<p><strong>Benefits</strong>:</p>
<ul>
<li>Feature reuse</li>
<li>Strong gradient flow</li>
</ul>
<p><strong>Drawback</strong>: Memory intensive</p>
<h3 id="efficientnet">EfficientNet</h3>
<p><strong>Key insight</strong>: Scale depth, width, and resolution together</p>
<ul>
<li>Neural Architecture Search (NAS) to find optimal scaling</li>
</ul>
<hr>
<h2 id="adversarial-examples">Adversarial Examples</h2>
<h3 id="white-box-attacks">White-Box Attacks</h3>
<p>Attacker has full access to model.</p>
<p><strong>FGSM</strong> (Fast Gradient Sign Method):
$$x_{adv} = x + \epsilon \cdot \text{sign}(\nabla_x L)$$</p>
<p>Add small perturbation in gradient direction.</p>
<p><strong>PGD</strong> (Projected Gradient Descent):
Iterative version of FGSM; stronger attack.</p>
<h3 id="black-box-attacks">Black-Box Attacks</h3>
<p>No access to model internals:</p>
<ul>
<li>Query-based attacks</li>
<li>Transfer attacks (adversarial examples transfer across models)</li>
</ul>
<h3 id="defenses">Defenses</h3>
<ul>
<li>Adversarial training</li>
<li>Input preprocessing</li>
<li>Certified defenses (provable robustness)</li>
</ul>
<hr>
<h2 id="summary">Summary</h2>
<table>
<thead>
<tr>
<th>Component</th>
<th>Purpose</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Convolution</strong></td>
<td>Local feature detection with weight sharing</td>
</tr>
<tr>
<td><strong>Pooling</strong></td>
<td>Downsample, add invariance</td>
</tr>
<tr>
<td><strong>Stride</strong></td>
<td>Alternative to pooling for downsampling</td>
</tr>
<tr>
<td><strong>Padding</strong></td>
<td>Control output size</td>
</tr>
<tr>
<td><strong>1√ó1 Conv</strong></td>
<td>Channel mixing</td>
</tr>
<tr>
<td><strong>Skip connections</strong></td>
<td>Enable deep networks</td>
</tr>
<tr>
<td><strong>Normalization</strong></td>
<td>Stabilize training</td>
</tr>
</tbody></table>
<h3 id="why-cnns-work-for-images">Why CNNs Work for Images</h3>
<ol>
<li><strong>Local structure</strong>: Nearby pixels are related</li>
<li><strong>Translation equivariance</strong>: Features can appear anywhere</li>
<li><strong>Hierarchical composition</strong>: Simple features ‚Üí complex objects</li>
<li><strong>Parameter efficiency</strong>: Weight sharing dramatically reduces parameters</li>
</ol>
<h3 id="practical-tips">Practical Tips</h3>
<ol>
<li>Use pre-trained models when possible (transfer learning)</li>
<li>Start with proven architectures (ResNet, EfficientNet)</li>
<li>Data augmentation is crucial</li>
<li>Batch normalization helps training</li>
<li>Global average pooling instead of flattening before classifier</li>
</ol>

        </article>
        <nav class="page-navigation">
        <a href="probml-13-ffnn.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Feed-Forward Neural Networks</span>
        </a>
        <a href="probml-15-rnn.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Recurrent Neural Networks and Transformers</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>