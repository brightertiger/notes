<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Probability: Advanced Topics | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="probml-03-probability.html" class="active">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">ProbML</a>
          </div>
          <h1 class="page-title">Probability: Advanced Topics</h1>
          <div class="page-meta"><span class="tag">ProbML</span></div>
        </header>
        <article class="content">
          <h1 id="probability-advanced-topics">Probability: Advanced Topics</h1>
<p>This chapter covers more advanced probability concepts including covariance, correlation, mixture models, and Markov chains ‚Äî essential tools for understanding many machine learning algorithms.</p>
<h2 id="covariance-and-correlation">Covariance and Correlation</h2>
<h3 id="covariance">Covariance</h3>
<p>Covariance measures how two random variables <strong>vary together</strong>:</p>
<p>$$\text{Cov}(X, Y) = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])]$$</p>
<p><strong>Interpretation</strong>:</p>
<ul>
<li><strong>Positive covariance</strong>: When X is above its mean, Y tends to be above its mean</li>
<li><strong>Negative covariance</strong>: When X is above its mean, Y tends to be below its mean</li>
<li><strong>Zero covariance</strong>: No linear relationship</li>
</ul>
<p><strong>Alternative formula</strong>:
$$\text{Cov}(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]$$</p>
<p><strong>Properties</strong>:</p>
<ul>
<li>$\text{Cov}(X, X) = \text{Var}(X)$</li>
<li>$\text{Cov}(X, Y) = \text{Cov}(Y, X)$</li>
<li>$\text{Cov}(aX + b, Y) = a \cdot \text{Cov}(X, Y)$</li>
</ul>
<h3 id="correlation">Correlation</h3>
<p>Correlation is a <strong>scaled</strong> version of covariance, always between -1 and 1:</p>
<p>$$\rho_{XY} = \frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X)} \cdot \sqrt{\text{Var}(Y)}}$$</p>
<p><strong>Interpretation</strong>:</p>
<ul>
<li>$\rho = 1$: Perfect positive linear relationship</li>
<li>$\rho = -1$: Perfect negative linear relationship</li>
<li>$\rho = 0$: No linear relationship (but could have non-linear relationship!)</li>
</ul>
<h3 id="independence-vs-uncorrelation">Independence vs. Uncorrelation</h3>
<p><strong>Key distinction</strong>:</p>
<ul>
<li><strong>Independent</strong> ‚üπ <strong>Uncorrelated</strong> (always true)</li>
<li><strong>Uncorrelated</strong> ‚ü∏ <strong>NOT</strong> ‚üπ <strong>Independent</strong> (converse is false!)</li>
</ul>
<p><strong>Example</strong>: Let X ~ Uniform(-1, 1) and Y = X¬≤. Then:</p>
<ul>
<li>$\text{Cov}(X, Y) = 0$ (by symmetry)</li>
<li>But X and Y are clearly dependent (Y is completely determined by X!)</li>
</ul>
<h3 id="correlation-‚â†-causation">Correlation ‚â† Causation</h3>
<p>A correlation between X and Y could be due to:</p>
<ol>
<li>X causes Y</li>
<li>Y causes X</li>
<li>A third variable Z causes both (confounding)</li>
<li>Pure coincidence (spurious correlation)</li>
</ol>
<p><strong>Simpson&#39;s Paradox</strong>: A trend that appears in groups of data can <strong>disappear or reverse</strong> when groups are combined. Always be cautious about aggregated data!</p>
<hr>
<h2 id="mixture-models">Mixture Models</h2>
<p>Mixture models represent complex distributions as <strong>weighted combinations</strong> of simpler distributions.</p>
<h3 id="definition">Definition</h3>
<p>$$p(y | \theta) = \sum_{k=1}^K \pi_k \cdot p_k(y)$$</p>
<p>Where:</p>
<ul>
<li>$\pi_k$ are <strong>mixing proportions</strong> (weights): $\sum_k \pi_k = 1$, all $\pi_k \geq 0$</li>
<li>$p_k(y)$ are <strong>component distributions</strong></li>
</ul>
<h3 id="generative-process">Generative Process</h3>
<p>To sample from a mixture:</p>
<ol>
<li>Sample component index: $k \sim \text{Categorical}(\pi_1, ..., \pi_K)$</li>
<li>Sample from chosen component: $y \sim p_k(y)$</li>
</ol>
<h3 id="gaussian-mixture-models-gmms">Gaussian Mixture Models (GMMs)</h3>
<p>The most common mixture model uses Gaussian components:</p>
<p>$$p(y) = \sum_{k=1}^K \pi_k \cdot \mathcal{N}(y | \mu_k, \Sigma_k)$$</p>
<p><strong>Applications</strong>:</p>
<ul>
<li><strong>Clustering</strong>: Soft assignment of points to clusters</li>
<li><strong>Density estimation</strong>: Model complex, multi-modal distributions</li>
<li><strong>Outlier detection</strong>: Points with low probability under all components</li>
</ul>
<h3 id="k-means-as-a-special-case">K-Means as a Special Case</h3>
<p>K-Means clustering is a limiting case of GMM with:</p>
<ul>
<li>Uniform mixing proportions: $\pi_k = 1/K$</li>
<li>Spherical Gaussians: $\Sigma_k = \sigma^2 I$ (same for all components)</li>
<li>Hard assignments (as $\sigma \to 0$)</li>
</ul>
<hr>
<h2 id="markov-chains">Markov Chains</h2>
<p>Markov chains model <strong>sequences</strong> where each state depends only on the previous state.</p>
<h3 id="chain-rule-for-sequences">Chain Rule for Sequences</h3>
<p>For a sequence $(x_1, x_2, x_3, ...)$:
$$p(x_1, x_2, x_3) = p(x_1) \cdot p(x_2 | x_1) \cdot p(x_3 | x_1, x_2)$$</p>
<p>This is exact but requires modeling complex conditional dependencies.</p>
<h3 id="the-markov-assumption">The Markov Assumption</h3>
<p><strong>First-order Markov property</strong>: The future depends only on the present, not the past.
$$p(x_t | x_1, x_2, ..., x_{t-1}) = p(x_t | x_{t-1})$$</p>
<p>This simplifies the chain rule to:
$$p(x_1, x_2, x_3) = p(x_1) \cdot p(x_2 | x_1) \cdot p(x_3 | x_2)$$</p>
<h3 id="state-transition-matrix">State Transition Matrix</h3>
<p>The conditional distribution $p(x_t | x_{t-1})$ defines a <strong>transition matrix</strong> T where:
$$T_{ij} = p(x_t = j | x_{t-1} = i)$$</p>
<p><strong>Properties</strong>:</p>
<ul>
<li>Rows sum to 1 (each row is a valid probability distribution)</li>
<li>$T^n$ gives n-step transition probabilities</li>
</ul>
<h3 id="higher-order-markov-models">Higher-Order Markov Models</h3>
<p><strong>Second-order Markov</strong>: $p(x_t | x_{t-1}, x_{t-2})$</p>
<ul>
<li>Used in bigram language models</li>
</ul>
<p><strong>n-th order Markov</strong>: $p(x_t | x_{t-1}, ..., x_{t-n})$</p>
<ul>
<li>Used in n-gram language models</li>
</ul>
<p><strong>Trade-off</strong>: Higher order captures more context but requires more parameters.</p>
<h3 id="applications-in-ml">Applications in ML</h3>
<ul>
<li><strong>Language modeling</strong>: Predicting the next word</li>
<li><strong>Hidden Markov Models</strong>: Markov chains with hidden states</li>
<li><strong>Reinforcement learning</strong>: MDP (Markov Decision Process)</li>
<li><strong>MCMC</strong>: Markov Chain Monte Carlo for sampling</li>
</ul>
<hr>
<h2 id="the-multivariate-gaussian">The Multivariate Gaussian</h2>
<p>The multivariate Gaussian (MVN) is crucial for modeling correlated continuous variables.</p>
<h3 id="definition-1">Definition</h3>
<p>For a d-dimensional random vector $\mathbf{x}$:</p>
<p>$$\mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \frac{1}{(2\pi)^{d/2}|\boldsymbol{\Sigma}|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1}(\mathbf{x} - \boldsymbol{\mu})\right)$$</p>
<p>Where:</p>
<ul>
<li>$\boldsymbol{\mu}$: Mean vector (d √ó 1)</li>
<li>$\boldsymbol{\Sigma}$: Covariance matrix (d √ó d, symmetric positive definite)</li>
</ul>
<h3 id="key-properties">Key Properties</h3>
<p><strong>Marginals</strong>: If $(X, Y) \sim \mathcal{N}$, then $X \sim \mathcal{N}$ and $Y \sim \mathcal{N}$</p>
<p><strong>Conditionals</strong>: $X | Y \sim \mathcal{N}$ (also Gaussian!)</p>
<p><strong>Linear transformations</strong>: If $\mathbf{x} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$, then:
$$A\mathbf{x} + \mathbf{b} \sim \mathcal{N}(A\boldsymbol{\mu} + \mathbf{b}, A\boldsymbol{\Sigma}A^T)$$</p>
<h3 id="geometry-of-the-covariance-matrix">Geometry of the Covariance Matrix</h3>
<p>The covariance matrix determines the shape of the Gaussian:</p>
<ul>
<li><strong>Diagonal Œ£</strong>: Ellipse aligned with axes</li>
<li><strong>Spherical Œ£ = œÉ¬≤I</strong>: Circle/sphere</li>
<li><strong>General Œ£</strong>: Rotated ellipse</li>
</ul>
<p>The eigenvectors of Œ£ give the principal axes; eigenvalues give the variance along each axis.</p>
<hr>
<h2 id="summary">Summary</h2>
<table>
<thead>
<tr>
<th>Concept</th>
<th>Key Insight</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Covariance</strong></td>
<td>Measures linear co-variation; unscaled</td>
</tr>
<tr>
<td><strong>Correlation</strong></td>
<td>Scaled covariance between -1 and 1</td>
</tr>
<tr>
<td><strong>Independence</strong></td>
<td>Implies zero correlation, but not vice versa</td>
</tr>
<tr>
<td><strong>Mixture Models</strong></td>
<td>Complex distributions as weighted sums of simple ones</td>
</tr>
<tr>
<td><strong>GMM</strong></td>
<td>Gaussian components; soft clustering</td>
</tr>
<tr>
<td><strong>Markov Chains</strong></td>
<td>Future depends only on present, not past</td>
</tr>
<tr>
<td><strong>Transition Matrix</strong></td>
<td>Encodes all transition probabilities</td>
</tr>
<tr>
<td><strong>Multivariate Gaussian</strong></td>
<td>Generalizes Gaussian to multiple correlated variables</td>
</tr>
</tbody></table>

        </article>
        <nav class="page-navigation">
        <a href="probml-02-probability.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Probability Foundations</span>
        </a>
        <a href="probml-04-statistics.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Statistics</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>