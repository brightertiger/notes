<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Linear Regression | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="probml-11-linear_regression.html" class="active">Linear Regression</a></li>
            <li class="nav-item"><a href="probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">ProbML</a>
          </div>
          <h1 class="page-title">Linear Regression</h1>
          <div class="page-meta"><span class="tag">ProbML</span></div>
        </header>
        <article class="content">
          <h1 id="linear-regression">Linear Regression</h1>
<p>Linear regression is the foundation of supervised learning for continuous outputs. Understanding it deeply gives insight into more complex models.</p>
<h2 id="the-big-picture">The Big Picture</h2>
<p><strong>The model</strong>:
$$p(y | x, \theta) = \mathcal{N}(y | w^T x + b, \sigma^2)$$</p>
<p><strong>Translation</strong>: Given features x, the output y is normally distributed around a linear prediction, with some noise œÉ¬≤.</p>
<hr>
<h2 id="types-of-linear-regression">Types of Linear Regression</h2>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Simple</strong></td>
<td>One input feature</td>
</tr>
<tr>
<td><strong>Multiple</strong></td>
<td>Many input features</td>
</tr>
<tr>
<td><strong>Multivariate</strong></td>
<td>Multiple output variables</td>
</tr>
<tr>
<td><strong>Polynomial</strong></td>
<td>Non-linear by adding $x^2, x^3$, etc. as features</td>
</tr>
</tbody></table>
<p><strong>Key insight</strong>: &quot;Linear&quot; refers to linearity in <strong>parameters</strong>, not features. Polynomial regression is still &quot;linear regression&quot;!</p>
<hr>
<h2 id="least-squares-estimation">Least Squares Estimation</h2>
<h3 id="the-objective">The Objective</h3>
<p>Minimize the Negative Log-Likelihood:
$$\text{NLL}(w, \sigma^2) = \frac{1}{2\sigma^2}\sum_{i=1}^N (y_i - \hat{y}_i)^2 + \frac{N}{2}\log(2\pi\sigma^2)$$</p>
<p>The first term is the <strong>Residual Sum of Squares (RSS)</strong>.</p>
<h3 id="the-normal-equations">The Normal Equations</h3>
<p>Setting $\nabla_w \text{RSS} = 0$:
$$X^T X w = X^T y$$</p>
<p><strong>Solution</strong>:
$$\hat{w} = (X^T X)^{-1} X^T y$$</p>
<p><strong>Why &quot;normal&quot;?</strong> The residual vector $(y - Xw)$ is orthogonal (normal) to the column space of X.</p>
<h3 id="geometric-interpretation">Geometric Interpretation</h3>
<p>$\hat{y} = X\hat{w}$ is the <strong>projection</strong> of y onto the column space of X. We find the closest point in the subspace spanned by the features.</p>
<h3 id="practical-computation">Practical Computation</h3>
<p>Direct matrix inversion can be numerically unstable. Better approaches:</p>
<ol>
<li><strong>SVD</strong>: $X = U \Sigma V^T$, then $\hat{w} = V \Sigma^{-1} U^T y$</li>
<li><strong>QR decomposition</strong>: More stable for ill-conditioned problems</li>
</ol>
<h3 id="simple-linear-regression">Simple Linear Regression</h3>
<p>For one feature:
$$\hat{w} = \frac{\text{Cov}(X, Y)}{\text{Var}(X)} = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}$$
$$\hat{b} = \bar{y} - \hat{w}\bar{x}$$</p>
<p><strong>Intuition</strong>: Slope is ratio of covariance to variance. Intercept ensures line passes through $(\bar{x}, \bar{y})$.</p>
<h3 id="estimating-the-noise-variance">Estimating the Noise Variance</h3>
<p>$$\hat{\sigma}^2 = \frac{1}{N}\sum_{i=1}^N (y_i - \hat{y}_i)^2 = \frac{\text{RSS}}{N}$$</p>
<p><strong>Note</strong>: This is biased! Unbiased version divides by (N - p - 1).</p>
<hr>
<h2 id="goodness-of-fit">Goodness of Fit</h2>
<h3 id="residual-analysis">Residual Analysis</h3>
<p>Check assumptions by plotting residuals:</p>
<ul>
<li>Should be normally distributed</li>
<li>Should have zero mean</li>
<li>Should be homoscedastic (constant variance)</li>
<li>Should be independent</li>
</ul>
<h3 id="coefficient-of-determination-r¬≤">Coefficient of Determination (R¬≤)</h3>
<p>$$R^2 = 1 - \frac{\text{RSS}}{\text{TSS}} = 1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}$$</p>
<p>Where:</p>
<ul>
<li><strong>TSS</strong> (Total Sum of Squares): Variance of y</li>
<li><strong>RSS</strong> (Residual Sum of Squares): Unexplained variance</li>
</ul>
<p><strong>Interpretation</strong>:</p>
<ul>
<li>R¬≤ = 1: Perfect fit</li>
<li>R¬≤ = 0: Model no better than predicting the mean</li>
<li>R¬≤ &lt; 0: Model is worse than predicting the mean (possible with regularization)</li>
</ul>
<h3 id="rmse-root-mean-squared-error">RMSE (Root Mean Squared Error)</h3>
<p>$$\text{RMSE} = \sqrt{\frac{1}{N}\sum(y_i - \hat{y}_i)^2}$$</p>
<p>In same units as y ‚Äî more interpretable than MSE.</p>
<hr>
<h2 id="ridge-regression-l2-regularization">Ridge Regression (L2 Regularization)</h2>
<h3 id="the-problem-with-ols">The Problem with OLS</h3>
<p>MLE can overfit when:</p>
<ul>
<li>Features are correlated (multicollinearity)</li>
<li>Number of features exceeds samples (p &gt; N)</li>
<li>$(X^T X)$ is ill-conditioned</li>
</ul>
<h3 id="the-ridge-solution">The Ridge Solution</h3>
<p>Add L2 penalty on weights:
$$L(w) = \text{RSS} + \lambda |w|^2$$</p>
<p><strong>Closed-form solution</strong>:
$$\hat{w}^{\text{ridge}} = (X^T X + \lambda I)^{-1} X^T y$$</p>
<p><strong>Effect</strong>: Adding ŒªI to the diagonal makes the matrix invertible!</p>
<h3 id="bayesian-interpretation">Bayesian Interpretation</h3>
<p>Ridge = MAP estimation with Gaussian prior:
$$p(w) = \mathcal{N}(0, \lambda^{-1} \sigma^2 I)$$</p>
<h3 id="connection-to-pca">Connection to PCA</h3>
<p>Ridge regression shrinks coefficients more in directions of low variance (small eigenvalues of $X^TX$).</p>
<p><strong>Intuition</strong>: Directions with little data support get regularized more heavily.</p>
<hr>
<h2 id="robust-regression">Robust Regression</h2>
<h3 id="the-outlier-problem">The Outlier Problem</h3>
<p>OLS is sensitive to outliers (squared error heavily penalizes large residuals).</p>
<h3 id="solutions">Solutions</h3>
<ol>
<li><p><strong>Student-t distribution</strong>: Heavy tails don&#39;t penalize outliers as much</p>
<ul>
<li>Fit via EM algorithm</li>
</ul>
</li>
<li><p><strong>Laplace distribution</strong>: Corresponds to L1 loss (MAE)</p>
<ul>
<li>More robust than Gaussian</li>
</ul>
</li>
<li><p><strong>Huber Loss</strong>: Best of both worlds</p>
<ul>
<li>L2 for small errors (smooth optimization)</li>
<li>L1 for large errors (robustness)</li>
</ul>
</li>
<li><p><strong>RANSAC</strong>: Iteratively identify and exclude outliers</p>
</li>
</ol>
<hr>
<h2 id="lasso-regression-l1-regularization">Lasso Regression (L1 Regularization)</h2>
<h3 id="the-l1-penalty">The L1 Penalty</h3>
<p>$$L(w) = \text{RSS} + \lambda |w|_1 = \text{RSS} + \lambda \sum_j |w_j|$$</p>
<h3 id="sparsity">Sparsity!</h3>
<p>Unlike Ridge, Lasso can set coefficients exactly to zero.</p>
<p><strong>Why?</strong> Consider the Lagrangian view:</p>
<ul>
<li>L2 constraint: $|w|^2 \leq B$ (sphere)</li>
<li>L1 constraint: $|w|_1 \leq B$ (diamond)</li>
</ul>
<p>The diamond has corners on the axes. The optimal solution often hits a corner, making some weights zero.</p>
<h3 id="regularization-path">Regularization Path</h3>
<p>As Œª decreases from ‚àû to 0:</p>
<ul>
<li>Weights &quot;enter&quot; the model one by one</li>
<li>Order of entry indicates relative importance</li>
<li>Use cross-validation to select optimal Œª</li>
</ul>
<h3 id="bayesian-interpretation-1">Bayesian Interpretation</h3>
<p>Lasso = MAP with Laplace prior:
$$p(w) \propto \exp(-\lambda |w|)$$</p>
<hr>
<h2 id="elastic-net">Elastic Net</h2>
<h3 id="combining-l1-and-l2">Combining L1 and L2</h3>
<p>$$L(w) = \text{RSS} + \lambda_1 |w|_1 + \lambda_2 |w|^2$$</p>
<h3 id="advantages">Advantages</h3>
<ul>
<li><strong>Sparsity</strong> from L1</li>
<li><strong>Grouping effect</strong> from L2: Correlated features tend to get similar coefficients</li>
<li>More stable than pure Lasso</li>
</ul>
<hr>
<h2 id="optimization-coordinate-descent">Optimization: Coordinate Descent</h2>
<h3 id="the-algorithm">The Algorithm</h3>
<p>For Lasso and Elastic Net:</p>
<ol>
<li>Initialize all weights</li>
<li>For each coordinate j:<ul>
<li>Fix all other weights</li>
<li>Optimize w_j (has closed-form solution!)</li>
</ul>
</li>
<li>Repeat until convergence</li>
</ol>
<p><strong>Why it works</strong>: Each subproblem is easy, and cycling through converges to the global optimum for convex problems.</p>
<hr>
<h2 id="summary">Summary</h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Penalty</th>
<th>Sparsity</th>
<th>Computation</th>
<th>Best For</th>
</tr>
</thead>
<tbody><tr>
<td><strong>OLS</strong></td>
<td>None</td>
<td>No</td>
<td>Closed-form</td>
<td>Well-conditioned problems</td>
</tr>
<tr>
<td><strong>Ridge</strong></td>
<td>L2</td>
<td>No</td>
<td>Closed-form</td>
<td>Multicollinearity</td>
</tr>
<tr>
<td><strong>Lasso</strong></td>
<td>L1</td>
<td>Yes</td>
<td>Iterative</td>
<td>Feature selection</td>
</tr>
<tr>
<td><strong>Elastic Net</strong></td>
<td>L1 + L2</td>
<td>Yes</td>
<td>Iterative</td>
<td>Correlated features</td>
</tr>
</tbody></table>
<h3 id="practical-tips">Practical Tips</h3>
<ol>
<li><strong>Always visualize residuals</strong> to check assumptions</li>
<li><strong>Standardize features</strong> before regularization</li>
<li><strong>Use cross-validation</strong> to choose Œª</li>
<li><strong>Start simple</strong> (OLS), add complexity as needed</li>
<li><strong>Lasso for interpretability</strong> (sparse models)</li>
<li><strong>Ridge for prediction</strong> (usually slightly better than Lasso)</li>
</ol>

        </article>
        <nav class="page-navigation">
        <a href="probml-10-logistic_regression.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Logistic Regression</span>
        </a>
        <a href="probml-13-ffnn.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Feed-Forward Neural Networks</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>