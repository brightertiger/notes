<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Recurrent Neural Networks and Transformers | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="probml-15-rnn.html" class="active">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">ProbML</a>
          </div>
          <h1 class="page-title">Recurrent Neural Networks and Transformers</h1>
          <div class="page-meta"><span class="tag">ProbML</span></div>
        </header>
        <article class="content">
          <h1 id="recurrent-neural-networks-and-transformers">Recurrent Neural Networks and Transformers</h1>
<p>RNNs process sequential data by maintaining a hidden state that carries information across time steps. Transformers, which use attention mechanisms, have largely replaced RNNs for many tasks.</p>
<h2 id="the-big-picture">The Big Picture</h2>
<p><strong>Sequential data</strong> is everywhere: text, speech, time series, video.</p>
<p><strong>The challenge</strong>: Variable-length inputs with temporal dependencies.</p>
<p><strong>RNN solution</strong>: Maintain a memory (hidden state) that updates as new inputs arrive.</p>
<p><strong>Transformer solution</strong>: Use attention to relate all positions directly.</p>
<hr>
<h2 id="core-rnn-architecture">Core RNN Architecture</h2>
<h3 id="the-basic-update">The Basic Update</h3>
<p>$$h_t = \phi(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$$</p>
<p><strong>Components</strong>:</p>
<ul>
<li>$x_t$: Input at time t</li>
<li>$h_{t-1}$: Previous hidden state (the &quot;memory&quot;)</li>
<li>$W_{xh}$: Input-to-hidden weights</li>
<li>$W_{hh}$: Hidden-to-hidden weights (recurrent connection)</li>
<li>$\phi$: Non-linearity (usually tanh)</li>
</ul>
<p><strong>Key insight</strong>: Same weights used at every time step (weight sharing through time).</p>
<hr>
<h2 id="types-of-sequence-tasks">Types of Sequence Tasks</h2>
<h3 id="seq2vec-many-to-one">Seq2Vec (Many-to-One)</h3>
<p>Variable-length input ‚Üí Fixed output</p>
<p><strong>Examples</strong>: Sentiment analysis, document classification</p>
<p><strong>Approach</strong>: Use final hidden state (or aggregate all states) as representation.</p>
<h3 id="vec2seq-one-to-many">Vec2Seq (One-to-Many)</h3>
<p>Fixed input ‚Üí Variable-length output</p>
<p><strong>Examples</strong>: Image captioning, music generation</p>
<p><strong>Approach</strong>: Condition on input vector, generate sequence autoregressively.</p>
<h3 id="seq2seq-many-to-many">Seq2Seq (Many-to-Many)</h3>
<p>Variable input ‚Üí Variable output</p>
<p><strong>Examples</strong>: Machine translation, summarization</p>
<p><strong>Approach</strong>: Encoder-decoder architecture.</p>
<hr>
<h2 id="bidirectional-rnns">Bidirectional RNNs</h2>
<p>Process sequence in both directions:</p>
<p><strong>Forward</strong>: $\vec{h}<em>t = f(x_t, \vec{h}</em>{t-1})$
<strong>Backward</strong>: $\overleftarrow{h}<em>t = f(x_t, \overleftarrow{h}</em>{t+1})$</p>
<p><strong>Final state</strong>: Concatenate both: $h_t = [\vec{h}_t; \overleftarrow{h}_t]$</p>
<p><strong>Benefit</strong>: Each position has access to both past and future context.</p>
<p><strong>Limitation</strong>: Can&#39;t be used for autoregressive generation (need future that doesn&#39;t exist yet).</p>
<hr>
<h2 id="the-vanishingexploding-gradient-problem">The Vanishing/Exploding Gradient Problem</h2>
<h3 id="the-problem">The Problem</h3>
<p>Gradient through L time steps:
$$\frac{\partial L}{\partial h_0} = \prod_{t=1}^{L} \frac{\partial h_t}{\partial h_{t-1}} \cdot \frac{\partial L}{\partial h_L}$$</p>
<p>If $|W_{hh}| &lt; 1$: Gradients vanish exponentially
If $|W_{hh}| &gt; 1$: Gradients explode exponentially</p>
<h3 id="exploding-gradient-solution">Exploding Gradient Solution</h3>
<p><strong>Gradient clipping</strong>:
$$g \leftarrow \min\left(1, \frac{\tau}{|g|}\right) g$$</p>
<h3 id="vanishing-gradient-solutions">Vanishing Gradient Solutions</h3>
<p>Use architectures with <strong>additive updates</strong> instead of multiplicative:</p>
<ul>
<li>LSTM</li>
<li>GRU</li>
<li>Skip connections</li>
</ul>
<hr>
<h2 id="lstm-long-short-term-memory">LSTM (Long Short-Term Memory)</h2>
<h3 id="the-key-innovation">The Key Innovation</h3>
<p>Separate <strong>cell state</strong> $C_t$ that flows through time with minimal transformation.</p>
<h3 id="gates">Gates</h3>
<p>Three gates control information flow:</p>
<p><strong>Forget Gate</strong>: What to discard from cell state
$$f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)$$</p>
<p><strong>Input Gate</strong>: What new information to add
$$i_t = \sigma(W_i [h_{t-1}, x_t] + b_i)$$</p>
<p><strong>Output Gate</strong>: What to output from cell state
$$o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)$$</p>
<h3 id="update-equations">Update Equations</h3>
<p><strong>Candidate cell state</strong>:
$$\tilde{C}<em>t = \tanh(W_c [h</em>{t-1}, x_t] + b_c)$$</p>
<p><strong>Cell state update</strong> (additive!):
$$C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$$</p>
<p><strong>Hidden state output</strong>:
$$h_t = o_t \odot \tanh(C_t)$$</p>
<h3 id="why-lstm-works">Why LSTM Works</h3>
<p>The cell state acts like a &quot;conveyor belt&quot; ‚Äî gradients can flow through unchanged if the forget gate is open.</p>
<hr>
<h2 id="gru-gated-recurrent-unit">GRU (Gated Recurrent Unit)</h2>
<p>Simplified version of LSTM with fewer parameters.</p>
<p><strong>Two gates</strong>:</p>
<ul>
<li><strong>Update gate</strong> $z_t$: How much to update hidden state</li>
<li><strong>Reset gate</strong> $r_t$: How much past state to forget</li>
</ul>
<p>$$h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t$$</p>
<p><strong>Trade-off</strong>: Fewer parameters, competitive performance.</p>
<hr>
<h2 id="backpropagation-through-time-bptt">Backpropagation Through Time (BPTT)</h2>
<h3 id="the-algorithm">The Algorithm</h3>
<ol>
<li>Unroll network through time</li>
<li>Forward pass: Compute all hidden states</li>
<li>Backward pass: Compute gradients through the unrolled graph</li>
<li>Sum gradients for shared weights across time</li>
</ol>
<h3 id="truncated-bptt">Truncated BPTT</h3>
<p>For long sequences:</p>
<ul>
<li>Don&#39;t backprop through entire sequence</li>
<li>Truncate to manageable length (e.g., 100 steps)</li>
<li>Trade-off: Can&#39;t learn very long-range dependencies</li>
</ul>
<hr>
<h2 id="decoding-strategies">Decoding Strategies</h2>
<h3 id="greedy-decoding">Greedy Decoding</h3>
<p>At each step, pick the most likely token:
$$y_t = \arg\max_y P(y | y_{&lt;t}, x)$$</p>
<p><strong>Problem</strong>: Locally optimal choices may not be globally optimal.</p>
<h3 id="beam-search">Beam Search</h3>
<p>Keep top-K candidates at each step:</p>
<ol>
<li>Expand each candidate in all possible ways</li>
<li>Keep the K highest-probability sequences</li>
<li>Continue until all sequences end</li>
</ol>
<p><strong>Benefit</strong>: Better than greedy; balances quality and computation.</p>
<h3 id="sampling">Sampling</h3>
<p>For generation diversity:</p>
<p><strong>Top-K sampling</strong>: Sample from top K tokens only</p>
<p><strong>Top-P (nucleus) sampling</strong>: Sample from smallest set with cumulative probability &gt; p</p>
<p><strong>Temperature</strong>: Scale logits before softmax</p>
<ul>
<li>Low T: More deterministic</li>
<li>High T: More random</li>
</ul>
<hr>
<h2 id="attention-mechanism">Attention Mechanism</h2>
<h3 id="the-problem-with-basic-rnns">The Problem with Basic RNNs</h3>
<p>All information must flow through the bottleneck of the hidden state.</p>
<p>For long sequences, early information gets &quot;washed out.&quot;</p>
<h3 id="the-attention-solution">The Attention Solution</h3>
<p>Allow the decoder to &quot;look at&quot; all encoder states:</p>
<p>$$\text{Attention}(q, (k_1,v_1), ..., (k_m,v_m)) = \sum_{i=1}^m \alpha_i \cdot v_i$$</p>
<p>Where attention weights $\alpha_i$ depend on similarity between query $q$ and keys $k_i$.</p>
<h3 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h3>
<p>$$\alpha_i = \text{softmax}\left(\frac{q^T k_i}{\sqrt{d}}\right)$$</p>
<p><strong>Scaling by $\sqrt{d}$</strong>: Prevents softmax saturation when dimensions are large.</p>
<h3 id="seq2seq-with-attention">Seq2Seq with Attention</h3>
<p>Instead of using only the final encoder state:</p>
<ul>
<li>Query: Current decoder hidden state</li>
<li>Keys &amp; Values: All encoder hidden states</li>
</ul>
<p>Context at each decoding step:
$$c_t = \sum_i \alpha_i(h_t^{dec}, h_i^{enc}) \cdot h_i^{enc}$$</p>
<hr>
<h2 id="transformers">Transformers</h2>
<h3 id="the-revolution">The Revolution</h3>
<p><strong>Key insight</strong>: Attention is all you need ‚Äî no recurrence required!</p>
<p><strong>Benefits</strong>:</p>
<ul>
<li>Parallelizable (no sequential dependency)</li>
<li>Direct connections between all positions</li>
<li>Scales to very long sequences</li>
</ul>
<h3 id="self-attention">Self-Attention</h3>
<p>Each position attends to all positions (including itself):
$$y_i = \text{Attention}(x_i, (x_1,x_1), (x_2,x_2), ..., (x_n,x_n))$$</p>
<p><strong>Query, Key, Value</strong>: All derived from same input via learned projections.</p>
<h3 id="multi-head-attention">Multi-Head Attention</h3>
<p>Run multiple attention operations in parallel:
$$h_i = \text{Attention}(W_i^Q x, W_i^K x, W_i^V x)$$
$$\text{output} = \text{Concat}(h_1, ..., h_H) W^O$$</p>
<p><strong>Benefit</strong>: Capture different types of relationships.</p>
<h3 id="positional-encoding">Positional Encoding</h3>
<p>Attention is permutation-invariant ‚Äî it doesn&#39;t know position!</p>
<p><strong>Solution</strong>: Add position information to inputs:
$$x_{pos} = x + \text{PE}(pos)$$</p>
<p><strong>Sinusoidal encoding</strong> (original Transformer):</p>
<ul>
<li>Different frequencies for different dimensions</li>
<li>Can generalize to unseen lengths</li>
</ul>
<p><strong>Learned embeddings</strong> (common in practice).</p>
<h3 id="transformer-architecture">Transformer Architecture</h3>
<p><strong>Encoder block</strong>:</p>
<ol>
<li>Multi-head self-attention + residual + LayerNorm</li>
<li>Feed-forward network + residual + LayerNorm</li>
</ol>
<p><strong>Decoder block</strong>:</p>
<ol>
<li>Masked self-attention (can&#39;t see future)</li>
<li>Cross-attention to encoder</li>
<li>Feed-forward network</li>
</ol>
<hr>
<h2 id="pre-trained-language-models">Pre-trained Language Models</h2>
<h3 id="elmo">ELMo</h3>
<p>Concatenate forward and backward LSTM representations.</p>
<h3 id="bert-bidirectional-encoder">BERT (Bidirectional Encoder)</h3>
<p><strong>Pre-training tasks</strong>:</p>
<ul>
<li>Masked Language Modeling (MLM): Predict masked tokens</li>
<li>Next Sentence Prediction</li>
</ul>
<p><strong>Fine-tuning</strong>: Add task-specific head.</p>
<h3 id="gpt-generative-pre-training">GPT (Generative Pre-Training)</h3>
<p><strong>Architecture</strong>: Decoder-only transformer (causal masking)</p>
<p><strong>Pre-training</strong>: Autoregressive language modeling</p>
<p><strong>Key insight</strong>: Scale up model and data ‚Üí emergent capabilities.</p>
<h3 id="t5-text-to-text-transfer-transformer">T5 (Text-to-Text Transfer Transformer)</h3>
<p><strong>Unifying framework</strong>: Every task as text-to-text</p>
<ul>
<li>Classification: &quot;classify: text ‚Üí label&quot;</li>
<li>Translation: &quot;translate: source ‚Üí target&quot;</li>
</ul>
<hr>
<h2 id="summary">Summary</h2>
<table>
<thead>
<tr>
<th>Architecture</th>
<th>Key Feature</th>
<th>Best For</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Basic RNN</strong></td>
<td>Recurrent hidden state</td>
<td>Short sequences</td>
</tr>
<tr>
<td><strong>LSTM/GRU</strong></td>
<td>Gates + additive updates</td>
<td>Medium sequences</td>
</tr>
<tr>
<td><strong>Bidirectional</strong></td>
<td>Both directions</td>
<td>When future is available</td>
</tr>
<tr>
<td><strong>Attention</strong></td>
<td>Direct access to all positions</td>
<td>Long-range dependencies</td>
</tr>
<tr>
<td><strong>Transformer</strong></td>
<td>Self-attention + parallelism</td>
<td>Everything (modern default)</td>
</tr>
</tbody></table>
<h3 id="when-to-use-what">When to Use What</h3>
<ul>
<li><strong>RNN/LSTM</strong>: Small data, limited compute, streaming data</li>
<li><strong>Transformer</strong>: Large data, sufficient compute, best performance</li>
<li><strong>Pre-trained models</strong>: Almost always start here and fine-tune!</li>
</ul>

        </article>
        <nav class="page-navigation">
        <a href="probml-14-cnn.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Convolutional Neural Networks</span>
        </a>
        <a href="probml-16-exemplar.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Exemplar-Based Methods</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>