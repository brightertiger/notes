<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Recurrent Neural Networks | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">SLP Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regex</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">Tokenization</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vectors</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="probml-00.html">ProbML Notes</a></li>
            <li class="nav-item"><a href="probml-01-introduction.html">Introduction</a></li>
            <li class="nav-item"><a href="probml-02-probability.html">Probability</a></li>
            <li class="nav-item"><a href="probml-03-probability.html">Probability (Advanced Topics)</a></li>
            <li class="nav-item"><a href="probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="probml-13-ffnn.html">Feed Forward Neural Networks</a></li>
            <li class="nav-item"><a href="probml-14-cnn.html">Convolution NN</a></li>
            <li class="nav-item"><a href="probml-15-rnn.html" class="active">Recurrent Neural Networks</a></li>
            <li class="nav-item"><a href="probml-16-exemplar.html">Exemplar Methods</a></li>
            <li class="nav-item"><a href="probml-18-trees.html">Trees</a></li>
            <li class="nav-item"><a href="probml-19-ssl.html">SSL</a></li>
            <li class="nav-item"><a href="probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">ProbML</a>
          </div>
          <h1 class="page-title">Recurrent Neural Networks</h1>
          <div class="page-meta"><span class="tag">ProbML</span></div>
        </header>
        <article class="content">
          <h1 id="recurrent-neural-networks">Recurrent Neural Networks</h1>
<ul>
<li><p>RNNs are designed to process sequential data by maintaining internal state</p>
</li>
<li><p>Unlike feedforward networks, RNNs share parameters across different time steps</p>
</li>
<li><p>The hidden state carries information across the sequence, acting as memory</p>
</li>
<li><p>Core Recurrent Cell</p>
<ul>
<li>Basic RNN update: $h_t = \phi(W_{xh}x_t + W_{hh}h_{t-1} + b_h)$<ul>
<li>$W_{xh}$: Input-to-hidden weights</li>
<li>$W_{hh}$: Hidden-to-hidden weights (recurrent connections)</li>
<li>$h_{t-1}$: Previous hidden state</li>
<li>$\phi$: Activation function (typically tanh)</li>
</ul>
</li>
</ul>
</li>
<li><p>Types of Sequence Processing Tasks</p>
<ul>
<li><p>Seq2Seq (sequence generation)</p>
<ul>
<li>Maps fixed input to variable-length output sequence</li>
<li>Examples: Image captioning, machine translation</li>
<li>Autoregressive generation: Each output depends on previous outputs</li>
</ul>
</li>
<li><p>Seq2Vec (sequence classification)</p>
<ul>
<li>Maps variable-length input to fixed output vector</li>
<li>Examples: Sentiment analysis, document classification</li>
<li>Often uses final hidden state or aggregation of all states</li>
</ul>
</li>
<li><p>Vec2Seq (conditioned generation)</p>
<ul>
<li>Maps fixed input to variable-length output sequence</li>
<li>Example: Generate text conditioned on a topic vector</li>
</ul>
</li>
<li><p>Seq2Seq (sequence-to-sequence)</p>
<ul>
<li>Maps variable-length input to variable-length output</li>
<li>Examples: Machine translation, summarization</li>
<li>Typically employs encoder-decoder architecture</li>
</ul>
</li>
</ul>
</li>
<li><p>Bidirectional RNNs</p>
<ul>
<li>Process sequence in both forward and backward directions</li>
<li>Captures both past and future context for each position</li>
<li>Forward hidden states: $\vec{h}<em>t = \phi(W</em>{xh}^{\rightarrow}x_t + W_{hh}^{\rightarrow}\vec{h}_{t-1})$</li>
<li>Backward hidden states: $\overleftarrow{h}<em>t = \phi(W</em>{xh}^{\leftarrow}x_t + W_{hh}^{\leftarrow}\overleftarrow{h}_{t+1})$</li>
<li>Final representation combines both directions: $h_t = [\vec{h}_t; \overleftarrow{h}_t]$</li>
</ul>
</li>
<li><p>Challenges with Basic RNNs</p>
<ul>
<li>Vanishing Gradients: Signal from distant time steps diminishes exponentially</li>
<li>Exploding Gradients: Gradients grow uncontrollably (solved with gradient clipping)</li>
<li>Limited context window: Difficulty capturing long-range dependencies</li>
</ul>
</li>
<li><p>Advanced RNN Architectures</p>
<ul>
<li><p>LSTM (Long Short-Term Memory)</p>
<ul>
<li>Explicitly designed to capture long-term dependencies</li>
<li>Cell state ($C_t$) acts as conveyor belt of information through time</li>
<li>Three gates control information flow:<ul>
<li>Input gate ($I_t$): Controls what new information enters the cell</li>
<li>Forget gate ($F_t$): Controls what information is discarded</li>
<li>Output gate ($O_t$): Controls what information is exposed as output</li>
</ul>
</li>
<li>LSTM equations:<ul>
<li>$I_t = \sigma(W_{ix}X_t + W_{ih}H_{t-1})$</li>
<li>$F_t = \sigma(W_{fx}X_t + W_{fh}H_{t-1})$</li>
<li>$O_t = \sigma(W_{ox}X_t + W_{oh}H_{t-1})$</li>
<li>$\tilde{C}<em>t = \tanh(W</em>{cx}X_t + W_{ch}H_{t-1})$ (candidate cell state)</li>
<li>$C_t = F_t \odot C_{t-1} + I_t \odot \tilde{C}_t$ (cell state update)</li>
<li>$H_t = O_t \odot \tanh(C_t)$ (hidden state)</li>
</ul>
</li>
<li>Solves vanishing gradient through additive updates and gating</li>
</ul>
</li>
<li><p>GRU (Gated Recurrent Unit)</p>
<ul>
<li>Simplified version of LSTM with fewer parameters</li>
<li>Has two gates: update gate and reset gate</li>
<li>Update gate controls how much previous state is retained</li>
<li>Reset gate controls how much previous state influences candidate state</li>
<li>Competitive performance with LSTM but more efficient</li>
</ul>
</li>
</ul>
</li>
<li><p>Backpropagation through Time (BPTT)</p>
<ul>
<li>Unrolling the computation graph along time axis</li>
<li>$h_t = W_{hx}x_t + W_{hh}h_{t-1} = f(x_t, h_{t-1}, w_h)$</li>
<li>$o_t = W_{ho}h_t = g(h_t, w_{oh})$</li>
<li>$L = {1 \over T}\sum l(y_t, o_t)$</li>
<li>${\delta L \over \delta w_h} = {1 \over T} \sum {\delta l \over \delta w_h}$</li>
<li>${\delta L \over \delta w_h} = {1 \over T} \sum {\delta l \over \delta o_t} {\delta o_t \over \delta h_t} {\delta h_t \over \delta w_h}$</li>
<li>${\delta h_t \over \delta w_h} = {\delta h_t \over \delta w_h} + {\delta h_t \over \delta h_{t-1}} {\delta h_{t-1} \over \delta w_h}$</li>
<li>Common to truncate the update to length of the longest subsequence in the batch</li>
<li>As the sequence goes forward, the hidden state keeps getting multiplied by W(hh)</li>
<li>Gradients can decay or explode as we go backwards in time</li>
<li>Solution is to use additive rather than multiplicative updates</li>
</ul>
</li>
<li><p>Decoding</p>
<ul>
<li>Output is generated one token at a time</li>
<li>Simple Solution: Greedy Decoding<ul>
<li>Argmax over vocab at each step</li>
<li>Keep sampling unless <EOS> token output</li>
</ul>
</li>
<li>May not be globally optimal path</li>
<li>Alternative: Beam Search<ul>
<li>Compute top-K candidate outputs at each step</li>
<li>Expand each one in V possible ways</li>
<li>Total VK candidates generated</li>
</ul>
</li>
<li>GPT used top-k and top-p sampling<ul>
<li>Top-K sampling: Redistribute the probability mass</li>
<li>Top-P sampling: Sample till the cumulative probability exceeds p</li>
</ul>
</li>
</ul>
</li>
<li><p>Attention</p>
<ul>
<li>In RNNs, hidden state linearly combines the inputs and then sends them to an activation function</li>
<li>Attention mechanism allows for more flexibility.<ul>
<li>Suppose there are m feature vectors or values</li>
<li>Model decides which to use based on the input query vector q and its similarity to a set of m keys</li>
<li>If query is most similar to key i, then we use value i.</li>
</ul>
</li>
<li>Attention acts as a soft dictionary lookup<ul>
<li>Compare query q to each key k(i)</li>
<li>Retrieve the corresponding value v(i)</li>
<li>To make the operation differentiable:<ul>
<li>Compute a convex combination</li>
</ul>
</li>
<li>$Attn(q,(k_1,v_1),(k_2, v_2)...,(k_m,v_m)) = \sum_{i=1}^m \alpha_i (q, {k_i}) v_i$<ul>
<li>$\alpha_i (q, {k_i})$ are the attention weights</li>
</ul>
</li>
<li>Attention weights are computed from an attention score function $a(q,k_i)$<ul>
<li>Computes the similarity between query and key</li>
</ul>
</li>
<li>Once the scores are computed, use soft max to impose distribution</li>
<li>Masking helps in ignoring the index which are invalid while computing soft max</li>
<li>For computational efficiency, set the dim of query and key to be same (say d)<ul>
<li>The similarity is given by dot product</li>
<li>The weights are randomly initialized</li>
<li>The expected variance of dot product will be d.</li>
<li>Scale the dot product by $\sqrt d$</li>
<li>Scaled Dot-Product Attention<ul>
<li>Attention Weight: $a(q,k) = {q^Tk \over \sqrt d}$</li>
<li>Scaled Dot Product Attention: $Attn(Q,K,V) =  S({QK^T \over \sqrt d})V$</li>
</ul>
</li>
</ul>
</li>
<li>Example: Seq2Seq with Attention<ul>
<li>Consider encoder-decoder architecture</li>
<li>In the decoder:<ul>
<li>$h_t = f(h_{t-1}, c)$</li>
<li>c is the context vector from encoder</li>
<li>Usually the last hidden state of the encoder</li>
</ul>
</li>
<li>Attention allows the decoder to look at all the input words<ul>
<li>Better alignment between source and target</li>
</ul>
</li>
<li>Make the context dynamic<ul>
<li>Query: previous hidden state of the decoder</li>
<li>Key: all the hidden states from the encoder</li>
<li>Value: all the hidden states from the encoder</li>
<li>$c_t = \sum_{i=1}^T \alpha_i(h_{t-1}^d, {h_i^e})h_i^e$</li>
</ul>
</li>
<li>If RNN has multiple hidden layers, usually take the top most layer</li>
<li>Can be extended to Seq2Vec models</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Transformers</p>
<ul>
<li>Transformers are seq2seq models using attention in both encoder and decoder steps</li>
<li>Eliminate the need for RNNs</li>
<li>Self Attention:<ul>
<li>Modify the encoder such that it attends to itself</li>
<li>Given a sequence of input tokens $[x_1, x_2, x_3...,x_n]$</li>
<li>Sequence of output tokens: $y_i = Attn(x_i, (x_1,x_1), (x_2, x_2)...,(x_n, x_n))$<ul>
<li>Query is xi</li>
<li>Keys and Values are are x1,x2‚Ä¶xn (all valid inputs)</li>
</ul>
</li>
<li>In the decoder step:<ul>
<li>$y_i = Attn(y_{i-1}, (y_1,y_1), (y_2, y_2)...(y_{i-1}, y_{i-1}))$</li>
<li>Each new token generated has access to all the previous output</li>
</ul>
</li>
</ul>
</li>
<li>Multi-Head Attention<ul>
<li>Use multiple attention matrices to capture different nuances and similarities</li>
<li>$h_i = Attn(W_i^q q_i, (W_i^k k_i, W_i^v v_i))$</li>
<li>Stack all the heads together and use a projection matrix to get he output</li>
<li>Set $p_q h = p_k h = p_v h = p_o$ for parallel computation **How?</li>
</ul>
</li>
<li>Positional Encoding<ul>
<li>Attention is permutation invariant</li>
<li>Positional encodings help overcome this</li>
<li>Sinusoidal Basis</li>
<li>Positional Embeddings are combined with original input X ‚Üí X + P</li>
</ul>
</li>
<li>Combining All the Blocks<ul>
<li>Encoder<ul>
<li>Input: $ Z = LN(MHA(X,X,X) + X$</li>
<li>Encoder: $E = LN(FF(Z) + Z)$<ul>
<li>For the first layer:<ul>
<li>$ Z = \text{POS}(\text{Embed}(X))$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>In general, model has N copies of the encoder</li>
<li>Decoder <ul>
<li>Has access to both: encoder and previous tokens</li>
<li>Input: $ Z = LN(MHA(X,X,X) + X$</li>
<li>Input $ Z = LN(MHA(Z,E,E) + Z$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Representation Learning</p>
<ul>
<li>Contextual Word Embeddings<ul>
<li>Hidden state depends on all previous tokens</li>
<li>Use the latent representation for classification / other downstream tasks</li>
<li>Pre-train on a large corpus </li>
<li>Fine-tune on small task specific dataset</li>
<li>Transfer Learning</li>
</ul>
</li>
<li>ELMo<ul>
<li>Embeddings from Language Model</li>
<li>Fit two RNN models<ul>
<li>Left to Right</li>
<li>Right to Left</li>
</ul>
</li>
<li>Combine the hidden state representations to fetch embedding for each word</li>
</ul>
</li>
<li>BERT<ul>
<li>Bi-Directional Encoder Representations from Transformers</li>
<li>Pre-trained using Cloze task (MLM i.e. Masked Language Modeling)</li>
<li>Additional Objective: Next sentence Prediction</li>
</ul>
</li>
<li>GPT <ul>
<li>Generative Pre-training Transformer</li>
<li>Causal model using Masked Decoder</li>
<li>Train it as a language model on web text</li>
</ul>
</li>
<li>T5<ul>
<li>Text-to-Text Transfer Transformer</li>
<li>Single model to perform multiple tasks</li>
<li>Tell the task to perform as part of input sequence</li>
</ul>
</li>
</ul>
</li>
</ul>

        </article>
        <nav class="page-navigation">
        <a href="probml-14-cnn.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Convolution NN</span>
        </a>
        <a href="probml-16-exemplar.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Exemplar Methods</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>