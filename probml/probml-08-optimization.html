<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Optimization | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="probml-08-optimization.html" class="active">Optimization</a></li>
            <li class="nav-item"><a href="probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">ProbML</a>
          </div>
          <h1 class="page-title">Optimization</h1>
          <div class="page-meta"><span class="tag">ProbML</span></div>
        </header>
        <article class="content">
          <h1 id="optimization">Optimization</h1>
<p>Optimization is at the heart of machine learning ‚Äî finding the parameters that minimize loss functions. This chapter covers the algorithms and techniques used to train models effectively.</p>
<h2 id="the-big-picture">The Big Picture</h2>
<p><strong>The optimization problem</strong>:
$$\theta^* = \arg\min_\theta L(\theta)$$</p>
<p>We want to find parameters Œ∏ that minimize some loss function L.</p>
<p><strong>Challenges</strong>:</p>
<ul>
<li>Non-convex landscapes (many local minima)</li>
<li>High-dimensional parameter spaces (millions of parameters)</li>
<li>Noisy gradients (from mini-batch sampling)</li>
<li>Computational constraints</li>
</ul>
<hr>
<h2 id="basic-concepts">Basic Concepts</h2>
<h3 id="optima">Optima</h3>
<ul>
<li><strong>Global optimum</strong>: Best solution in the entire parameter space</li>
<li><strong>Local optimum</strong>: Best solution in a neighborhood (not necessarily globally best)</li>
</ul>
<h3 id="optimality-conditions">Optimality Conditions</h3>
<p>For smooth functions, at a minimum:</p>
<ol>
<li><p><strong>First-order condition</strong>: Gradient is zero
$$\nabla L(\theta^*) = 0$$</p>
</li>
<li><p><strong>Second-order condition</strong>: Hessian is positive semi-definite
$$\nabla^2 L(\theta^*) \succeq 0$$</p>
</li>
</ol>
<h3 id="convexity">Convexity</h3>
<p>A function is <strong>convex</strong> if any local minimum is also a global minimum.</p>
<p>For convex functions, optimization is &quot;easy&quot; ‚Äî gradient descent will find the global optimum.</p>
<p><strong>Unfortunately</strong>: Most deep learning losses are non-convex!</p>
<h3 id="lipschitz-continuity">Lipschitz Continuity</h3>
<p>A function is L-Lipschitz if:
$$|f(x_1) - f(x_2)| \leq L |x_1 - x_2|$$</p>
<p><strong>Interpretation</strong>: The function can&#39;t change too rapidly. This property is useful for proving convergence.</p>
<hr>
<h2 id="first-order-methods">First-Order Methods</h2>
<p>Use only gradient information (first derivatives).</p>
<h3 id="gradient-descent">Gradient Descent</h3>
<p>The simplest optimization algorithm:</p>
<p>$$\theta_{t+1} = \theta_t - \eta \nabla L(\theta_t)$$</p>
<p>Where Œ∑ is the <strong>learning rate</strong> or step size.</p>
<p><strong>Intuition</strong>: Move in the direction of steepest descent.</p>
<h3 id="step-size-selection">Step Size Selection</h3>
<p>Choosing Œ∑ is crucial:</p>
<ul>
<li><strong>Too small</strong>: Slow convergence</li>
<li><strong>Too large</strong>: Oscillations, divergence</li>
</ul>
<p><strong>Options</strong>:</p>
<ol>
<li><p><strong>Constant step size</strong>: Simple but suboptimal</p>
</li>
<li><p><strong>Line search</strong>: Find optimal Œ∑ at each step
$$\eta_t = \arg\min_\eta L(\theta_t - \eta \nabla L(\theta_t))$$</p>
</li>
<li><p><strong>Learning rate schedule</strong>: Decrease Œ∑ over time</p>
<ul>
<li>Must satisfy Robbins-Monro conditions for convergence</li>
</ul>
</li>
</ol>
<h3 id="momentum">Momentum</h3>
<p>Gradient descent is slow in flat regions and oscillates in narrow valleys.</p>
<p><strong>Heavy ball momentum</strong>:
$$m_t = \beta m_{t-1} + \nabla L(\theta_{t-1})$$
$$\theta_t = \theta_{t-1} - \eta m_t$$</p>
<p><strong>Intuition</strong>: Accumulate velocity like a ball rolling downhill. EWMA of gradients smooths out oscillations and accelerates in consistent directions.</p>
<p><strong>Typical value</strong>: Œ≤ = 0.9</p>
<h3 id="nesterov-momentum">Nesterov Momentum</h3>
<p>Momentum can overshoot. Nesterov adds &quot;lookahead&quot;:</p>
<p>$$m_{t+1} = \beta m_t - \eta \nabla L(\theta_t + \beta m_t)$$</p>
<p><strong>Idea</strong>: Compute gradient at the anticipated next position, not current position.</p>
<hr>
<h2 id="second-order-methods">Second-Order Methods</h2>
<p>Use curvature information (Hessian).</p>
<h3 id="newtons-method">Newton&#39;s Method</h3>
<p>Use quadratic approximation:
$$L(\theta) \approx L(\theta_t) + \nabla L^T(\theta - \theta_t) + \frac{1}{2}(\theta - \theta_t)^T H (\theta - \theta_t)$$</p>
<p>Optimal step:
$$\theta_{t+1} = \theta_t - H^{-1} \nabla L$$</p>
<p><strong>Advantages</strong>: Faster convergence (quadratic vs. linear)</p>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Hessian H is expensive to compute (O(d¬≤) storage, O(d¬≥) inversion)</li>
<li>Not scalable to deep learning</li>
</ul>
<h3 id="quasi-newton-methods-bfgs">Quasi-Newton Methods (BFGS)</h3>
<p>Approximate the Hessian using gradient information:</p>
<ul>
<li>Build up Hessian approximation over iterations</li>
<li><strong>L-BFGS</strong>: Limited memory version; uses only recent gradients</li>
</ul>
<p>Useful for smaller models where full-batch gradients are available.</p>
<hr>
<h2 id="stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD)</h2>
<h3 id="the-key-insight">The Key Insight</h3>
<p>For finite-sum problems:
$$L(\theta) = \frac{1}{N}\sum_{i=1}^N \ell(y_i, f(x_i; \theta))$$</p>
<p>Computing the full gradient requires summing over all N examples ‚Äî expensive!</p>
<p><strong>SGD approximation</strong>: Use a random mini-batch B ‚äÇ {1, ..., N}:
$$\nabla L(\theta) \approx \frac{1}{|B|}\sum_{i \in B} \nabla \ell(y_i, f(x_i; \theta))$$</p>
<h3 id="properties">Properties</h3>
<ul>
<li><strong>Unbiased</strong>: Expected gradient equals true gradient</li>
<li><strong>High variance</strong>: Individual mini-batch gradients are noisy</li>
<li><strong>Much faster</strong>: Each step is O(|B|) instead of O(N)</li>
</ul>
<h3 id="mini-batch-size-trade-offs">Mini-batch Size Trade-offs</h3>
<table>
<thead>
<tr>
<th>Batch Size</th>
<th>Gradient Quality</th>
<th>Computation</th>
<th>Generalization</th>
</tr>
</thead>
<tbody><tr>
<td>Small</td>
<td>Noisy</td>
<td>Fast per step</td>
<td>Often better (regularization effect)</td>
</tr>
<tr>
<td>Large</td>
<td>Accurate</td>
<td>Slow per step, but parallelizable</td>
<td>May overfit</td>
</tr>
</tbody></table>
<hr>
<h2 id="variance-reduction">Variance Reduction</h2>
<p>Reduce noise in SGD gradient estimates.</p>
<h3 id="svrg-stochastic-variance-reduced-gradient">SVRG (Stochastic Variance Reduced Gradient)</h3>
<p>Periodically compute full gradient; use it to correct mini-batch estimates:
$$g_t = \nabla \ell_i(\theta_t) - \nabla \ell_i(\tilde{\theta}) + \nabla L(\tilde{\theta})$$</p>
<h3 id="saga">SAGA</h3>
<p>Maintain running estimates of gradients for each example; update incrementally.</p>
<p><strong>Trade-off</strong>: Extra memory for reduced variance.</p>
<hr>
<h2 id="adaptive-learning-rates">Adaptive Learning Rates</h2>
<p>Different parameters may need different learning rates.</p>
<h3 id="adagrad">AdaGrad</h3>
<p>Adapt learning rate based on historical gradient magnitudes:</p>
<p>$$s_t = s_{t-1} + g_t^2$$
$$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{s_t + \epsilon}} g_t$$</p>
<p><strong>Effect</strong>: Parameters with large gradients get smaller learning rates.</p>
<p><strong>Problem</strong>: Learning rate decreases monotonically and may become too small.</p>
<h3 id="rmsprop">RMSProp</h3>
<p>Use exponential moving average instead of sum:</p>
<p>$$s_t = \beta s_{t-1} + (1 - \beta) g_t^2$$
$$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{s_t + \epsilon}} g_t$$</p>
<p><strong>Prevents</strong> learning rate from vanishing.</p>
<h3 id="adadelta">AdaDelta</h3>
<p>Like RMSProp, but also scales by historical update magnitudes:</p>
<p>$$\delta_t = \beta \delta_{t-1} + (1 - \beta) (\Delta\theta)^2$$
$$\theta_{t+1} = \theta_t - \frac{\sqrt{\delta_t + \epsilon}}{\sqrt{s_t + \epsilon}} g_t$$</p>
<h3 id="adam-adaptive-moment-estimation">Adam (Adaptive Moment Estimation)</h3>
<p>The most popular optimizer. Combines momentum with adaptive learning rates:</p>
<p><strong>First moment</strong> (mean of gradients):
$$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$$</p>
<p><strong>Second moment</strong> (mean of squared gradients):
$$s_t = \beta_2 s_{t-1} + (1 - \beta_2) g_t^2$$</p>
<p><strong>Bias correction</strong> (important for early iterations):
$$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{s}_t = \frac{s_t}{1 - \beta_2^t}$$</p>
<p><strong>Update</strong>:
$$\theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{s}_t} + \epsilon}$$</p>
<p><strong>Default values</strong>: $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\epsilon = 10^{-8}$</p>
<hr>
<h2 id="constrained-optimization">Constrained Optimization</h2>
<h3 id="lagrange-multipliers">Lagrange Multipliers</h3>
<p>Convert constrained to unconstrained optimization.</p>
<p>For equality constraint $h(\theta) = 0$:
$$\mathcal{L}(\theta, \lambda) = L(\theta) - \lambda h(\theta)$$</p>
<p>At optimum:
$$\nabla L = \lambda \nabla h$$</p>
<p><strong>Geometric interpretation</strong>: Gradient of objective is parallel to gradient of constraint.</p>
<h3 id="kkt-conditions">KKT Conditions</h3>
<p>For inequality constraints $g(\theta) \leq 0$:
$$\mathcal{L}(\theta, \mu) = L(\theta) + \mu g(\theta)$$</p>
<p><strong>Complementary slackness</strong>:</p>
<ul>
<li>If constraint is active ($g(\theta) = 0$): $\mu &gt; 0$</li>
<li>If constraint is inactive ($g(\theta) &lt; 0$): $\mu = 0$</li>
<li>Always: $\mu \cdot g(\theta) = 0$</li>
</ul>
<h3 id="proximal-gradient-descent">Proximal Gradient Descent</h3>
<p>For composite objectives with non-smooth terms (e.g., L1 regularization):
$$L(\theta) = f(\theta) + g(\theta)$$</p>
<p>Where f is smooth and g is non-smooth.</p>
<ol>
<li>Gradient step on smooth part</li>
<li><strong>Proximal operator</strong> to handle non-smooth part:
$$\text{prox}_g(x) = \arg\min_z \left[g(z) + \frac{1}{2}|z - x|^2\right]$$</li>
</ol>
<p><strong>Example</strong>: For L1 penalty, proximal operator is soft-thresholding.</p>
<hr>
<h2 id="em-algorithm">EM Algorithm</h2>
<p>For models with latent variables, direct MLE is difficult.</p>
<h3 id="the-problem">The Problem</h3>
<p>$$\log p(Y | \theta) = \log \sum_z p(Y, z | \theta)$$</p>
<p>The sum inside the log is intractable.</p>
<h3 id="the-solution">The Solution</h3>
<p>Iterate between:</p>
<p><strong>E-step</strong>: Compute posterior over latent variables given current parameters
$$q(z) = p(z | Y, \theta^{old})$$</p>
<p><strong>M-step</strong>: Maximize expected complete-data log-likelihood
$$\theta^{new} = \arg\max_\theta \mathbb{E}_{q(z)}[\log p(Y, z | \theta)]$$</p>
<h3 id="properties-1">Properties</h3>
<ul>
<li><strong>Monotonic</strong>: Likelihood never decreases</li>
<li><strong>Converges</strong>: To a local maximum</li>
<li><strong>May get stuck</strong>: Multiple restarts recommended</li>
</ul>
<h3 id="example-gmm">Example: GMM</h3>
<ul>
<li><strong>E-step</strong>: Compute responsibilities (soft cluster assignments)</li>
<li><strong>M-step</strong>: Update means, covariances, and mixing proportions</li>
</ul>
<hr>
<h2 id="simulated-annealing">Simulated Annealing</h2>
<p>For non-differentiable or discrete optimization:</p>
<ol>
<li>Start with high &quot;temperature&quot; T</li>
<li>Propose random moves</li>
<li>Accept improvements always; accept worse moves with probability $\exp(-\Delta L / T)$</li>
<li>Gradually decrease T</li>
</ol>
<p><strong>Idea</strong>: High T allows escaping local minima; low T focuses on refinement.</p>
<hr>
<h2 id="practical-tips">Practical Tips</h2>
<h3 id="learning-rate">Learning Rate</h3>
<ul>
<li>Start with a reasonable default (e.g., 0.001 for Adam)</li>
<li>Use learning rate warmup for large models</li>
<li>Decay learning rate during training</li>
</ul>
<h3 id="initialization">Initialization</h3>
<ul>
<li>Poor initialization can prevent learning</li>
<li>Xavier/Glorot: Scale by fan-in/fan-out</li>
<li>He: For ReLU networks</li>
</ul>
<h3 id="gradient-clipping">Gradient Clipping</h3>
<p>Prevent exploding gradients by clipping:
$$g \leftarrow \min\left(1, \frac{\tau}{|g|}\right) g$$</p>
<h3 id="early-stopping">Early Stopping</h3>
<p>Monitor validation loss; stop when it starts increasing.</p>
<hr>
<h2 id="summary">Summary</h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Key Idea</th>
<th>When to Use</th>
</tr>
</thead>
<tbody><tr>
<td><strong>SGD</strong></td>
<td>Mini-batch gradients</td>
<td>Large datasets</td>
</tr>
<tr>
<td><strong>Momentum</strong></td>
<td>Accumulate velocity</td>
<td>Faster than vanilla SGD</td>
</tr>
<tr>
<td><strong>Adam</strong></td>
<td>Adaptive + momentum</td>
<td>Default for deep learning</td>
</tr>
<tr>
<td><strong>L-BFGS</strong></td>
<td>Quasi-Newton</td>
<td>Small-medium models, full batch</td>
</tr>
<tr>
<td><strong>Proximal</strong></td>
<td>Handle non-smooth terms</td>
<td>L1 regularization</td>
</tr>
<tr>
<td><strong>EM</strong></td>
<td>Latent variables</td>
<td>Mixture models</td>
</tr>
</tbody></table>
<p><strong>General advice</strong>:</p>
<ol>
<li>Start with Adam</li>
<li>Try SGD + momentum if Adam overfits</li>
<li>Use learning rate schedules</li>
<li>Watch for vanishing/exploding gradients</li>
</ol>

        </article>
        <nav class="page-navigation">
        <a href="probml-06-information_theory.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Information Theory</span>
        </a>
        <a href="probml-09-discriminant_analysis.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Discriminant Analysis</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>