<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Introduction to Machine Learning | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="probml-01-introduction.html" class="active">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">ProbML</a>
          </div>
          <h1 class="page-title">Introduction to Machine Learning</h1>
          <div class="page-meta"><span class="tag">ProbML</span></div>
        </header>
        <article class="content">
          <h1 id="introduction-to-machine-learning">Introduction to Machine Learning</h1>
<p>Machine learning is the science of getting computers to learn from data without being explicitly programmed. This chapter introduces the fundamental concepts, problem types, and challenges that define the field.</p>
<h2 id="what-is-machine-learning">What is Machine Learning?</h2>
<p><strong>Tom Mitchell&#39;s Definition</strong>:</p>
<blockquote>
<p>A computer program is said to learn from experience E with respect to some class of tasks T, and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.</p>
</blockquote>
<p><strong>In plain English</strong>: A machine learning system gets better at a task as it sees more data.</p>
<p><strong>Example</strong>: A spam filter</p>
<ul>
<li><strong>Task (T)</strong>: Classify emails as spam or not spam</li>
<li><strong>Experience (E)</strong>: A dataset of labeled emails</li>
<li><strong>Performance (P)</strong>: Accuracy on new emails</li>
</ul>
<hr>
<h2 id="supervised-learning">Supervised Learning</h2>
<p>In supervised learning, we have input-output pairs and want to learn the mapping between them.</p>
<h3 id="the-setup">The Setup</h3>
<ul>
<li><strong>Inputs</strong> (X): Also called features, covariates, or predictors<ul>
<li>Example: Pixel values of an image, words in an email</li>
</ul>
</li>
<li><strong>Outputs</strong> (Y): Also called labels, targets, or responses<ul>
<li>Example: &quot;cat&quot; or &quot;dog&quot;, spam or not spam</li>
</ul>
</li>
</ul>
<p><strong>Goal</strong>: Learn a function $f: X \rightarrow Y$ that generalizes to new, unseen examples.</p>
<h3 id="classification">Classification</h3>
<p>When the output is a <strong>discrete category</strong>:</p>
<p>$$L(\theta) = \frac{1}{N}\sum_{i=1}^N I{y_i \neq f(x_i, \theta)}$$</p>
<p>This is the <strong>misclassification rate</strong> ‚Äî the fraction of examples we get wrong.</p>
<p><strong>Key concepts</strong>:</p>
<ul>
<li><strong>Empirical Risk</strong>: Average loss on training data</li>
<li><strong>Empirical Risk Minimization (ERM)</strong>: Find parameters that minimize training loss</li>
<li><strong>Generalization</strong>: The real goal is to perform well on <em>new</em> data, not just training data</li>
</ul>
<h3 id="dealing-with-uncertainty">Dealing with Uncertainty</h3>
<p>Models can&#39;t predict with 100% certainty. There are two types of uncertainty:</p>
<p><strong>Model Uncertainty (Epistemic)</strong></p>
<ul>
<li>Arises from lack of knowledge about the true mapping</li>
<li>Can be reduced with more data</li>
<li>Example: We don&#39;t know if a blurry image is a cat or dog</li>
</ul>
<p><strong>Data Uncertainty (Aleatoric)</strong></p>
<ul>
<li>Arises from inherent randomness in the data</li>
<li>Cannot be reduced even with infinite data</li>
<li>Example: A coin flip is inherently random</li>
</ul>
<h3 id="probabilistic-predictions">Probabilistic Predictions</h3>
<p>Instead of just predicting a class, predict a probability distribution over classes:</p>
<p>$$p(y | x, \theta)$$</p>
<p><strong>Why probabilities?</strong></p>
<ul>
<li>Quantify confidence</li>
<li>Enable better decision making</li>
<li>Allow principled handling of uncertainty</li>
</ul>
<p><strong>Negative Log-Likelihood (NLL)</strong>:</p>
<p>$$\text{NLL}(\theta) = -\frac{1}{N}\sum_{i=1}^N \log p(y_i | f(x_i, \theta))$$</p>
<p>Minimizing NLL is equivalent to <strong>Maximum Likelihood Estimation (MLE)</strong> ‚Äî finding parameters that make the observed data most probable.</p>
<h3 id="regression">Regression</h3>
<p>When the output is a <strong>continuous value</strong>:</p>
<p>$$L(\theta) = \frac{1}{N}\sum_{i=1}^N (y_i - f(x_i, \theta))^2$$</p>
<p>This is <strong>Mean Squared Error (MSE)</strong> ‚Äî the average squared difference between predictions and true values.</p>
<p><strong>Connection to Probability</strong>: If we assume Gaussian noise:</p>
<p>$$p(y | x, \theta) = \mathcal{N}(y | f(x, \theta), \sigma^2)$$</p>
<p>Then minimizing NLL is equivalent to minimizing MSE!</p>
<h3 id="types-of-regression-models">Types of Regression Models</h3>
<table>
<thead>
<tr>
<th>Model</th>
<th>Description</th>
<th>Flexibility</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Linear</strong></td>
<td>$f(x) = w^Tx + b$</td>
<td>Low</td>
</tr>
<tr>
<td><strong>Polynomial</strong></td>
<td>Includes $x^2, x^3$, etc.</td>
<td>Medium</td>
</tr>
<tr>
<td><strong>Neural Networks</strong></td>
<td>Nested nonlinear functions</td>
<td>High</td>
</tr>
</tbody></table>
<hr>
<h2 id="overfitting-and-generalization">Overfitting and Generalization</h2>
<h3 id="the-overfitting-problem">The Overfitting Problem</h3>
<p>A model that perfectly fits training data but fails on new data is <strong>overfitting</strong>. It has memorized the training set rather than learning the underlying pattern.</p>
<p><strong>Signs of overfitting</strong>:</p>
<ul>
<li>Training error much lower than test error</li>
<li>Model is very complex relative to data size</li>
<li>Model captures noise as if it were signal</li>
</ul>
<h3 id="understanding-the-errors">Understanding the Errors</h3>
<p><strong>Population Risk</strong>: Theoretical expected loss on the true data generating process
$$R(\theta) = \mathbb{E}_{(x,y) \sim p^*}[L(y, f(x, \theta))]$$</p>
<p><strong>Empirical Risk</strong>: Average loss on training data
$$\hat{R}(\theta) = \frac{1}{N}\sum_{i=1}^N L(y_i, f(x_i, \theta))$$</p>
<p><strong>Generalization Gap</strong>: Difference between population and empirical risk
$$\text{Gap} = R(\theta) - \hat{R}(\theta)$$</p>
<p>A large gap indicates overfitting.</p>
<h3 id="the-u-shaped-test-error-curve">The U-Shaped Test Error Curve</h3>
<pre><code>Error
  ‚îÇ    
  ‚îÇ  ‚ï≤                   ‚ï± Training Error
  ‚îÇ   ‚ï≤      ___________‚ï±  (keeps decreasing)
  ‚îÇ    ‚ï≤____‚ï±
  ‚îÇ     
  ‚îÇ         ‚ï±‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï≤
  ‚îÇ        ‚ï±    Test   ‚ï≤
  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï±    Error    ‚ï≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ (U-shaped)
  ‚îÇ       
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Model Complexity
     Simple          Complex
</code></pre>
<ul>
<li><strong>Underfitting</strong> (left): Model too simple, high bias</li>
<li><strong>Sweet spot</strong> (middle): Good balance</li>
<li><strong>Overfitting</strong> (right): Model too complex, high variance</li>
</ul>
<hr>
<h2 id="no-free-lunch-theorem">No Free Lunch Theorem</h2>
<p><strong>There is no single best model that works for all problems.</strong></p>
<p>Every model makes assumptions about the data. When those assumptions match reality, the model works well. When they don&#39;t, it fails.</p>
<p><strong>Implication</strong>: Understanding your problem domain is crucial for choosing the right model.</p>
<hr>
<h2 id="unsupervised-learning">Unsupervised Learning</h2>
<p>In unsupervised learning, we only have inputs X ‚Äî no labels.</p>
<p><strong>Goal</strong>: Discover hidden structure in data.</p>
<h3 id="common-tasks">Common Tasks</h3>
<p><strong>Clustering</strong>: Group similar data points together</p>
<ul>
<li>Example: Customer segmentation, document grouping</li>
</ul>
<p><strong>Dimensionality Reduction</strong>: Find lower-dimensional representations</p>
<ul>
<li>Example: Compress images while preserving important information</li>
</ul>
<p><strong>Density Estimation</strong>: Model the probability distribution $p(x)$</p>
<ul>
<li>Example: Anomaly detection (low probability = anomalous)</li>
</ul>
<p><strong>Self-Supervised Learning</strong>: Create proxy tasks from unlabeled data</p>
<ul>
<li>Example: Predict missing words in text (BERT), predict next frame in video</li>
</ul>
<h3 id="evaluation-challenge">Evaluation Challenge</h3>
<p>Without labels, how do we evaluate? Common approaches:</p>
<ul>
<li>Likelihood of held-out data</li>
<li>Performance on downstream tasks</li>
<li>Human evaluation of quality</li>
</ul>
<hr>
<h2 id="reinforcement-learning">Reinforcement Learning</h2>
<p>An <strong>agent</strong> learns to interact with an <strong>environment</strong> to maximize cumulative <strong>reward</strong>.</p>
<p><strong>Key differences from supervised learning</strong>:</p>
<ul>
<li>No explicit labels ‚Äî only reward signals</li>
<li>Rewards are often delayed (sparse feedback)</li>
<li>Agent&#39;s actions affect future states (sequential decision making)</li>
</ul>
<p><strong>Analogy</strong>: </p>
<ul>
<li>Supervised learning = learning with a teacher who gives correct answers</li>
<li>Reinforcement learning = learning with a critic who only says &quot;good&quot; or &quot;bad&quot;</li>
</ul>
<hr>
<h2 id="data-preprocessing">Data Preprocessing</h2>
<h3 id="text-data">Text Data</h3>
<p>Raw text needs transformation before ML models can process it.</p>
<p><strong>Bag of Words (BoW)</strong></p>
<ul>
<li>Represent document as vector of word counts</li>
<li>Loses word order but captures content</li>
</ul>
<p><strong>Problem</strong>: Common words (&quot;the&quot;, &quot;a&quot;) dominate counts.</p>
<p><strong>TF-IDF</strong> (Term Frequency - Inverse Document Frequency):
$$\text{TF-IDF} = \log(1 + \text{TF}) \times \text{IDF}$$</p>
<p>Where:</p>
<ul>
<li>TF = term frequency (how often word appears in document)</li>
<li>IDF = $\log\frac{N}{1 + \text{DF}}$ (inverse of how many documents contain the word)</li>
</ul>
<p><strong>Effect</strong>: Downweight common words, upweight distinctive words.</p>
<p><strong>Word Embeddings</strong>: Map words to dense vectors that capture semantic meaning</p>
<ul>
<li>Similar words have similar vectors</li>
<li>&quot;king&quot; - &quot;man&quot; + &quot;woman&quot; ‚âà &quot;queen&quot;</li>
</ul>
<p><strong>Handling Unknown Words</strong>:</p>
<ul>
<li>UNK token: Replace rare/unseen words with a special token</li>
<li>Subword units (BPE): Break words into common pieces</li>
</ul>
<h3 id="missing-data">Missing Data</h3>
<p>How data is missing matters!</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
<th>Example</th>
<th>Handling</th>
</tr>
</thead>
<tbody><tr>
<td><strong>MCAR</strong></td>
<td>Missing Completely At Random</td>
<td>Random sensor failures</td>
<td>Easier to handle</td>
</tr>
<tr>
<td><strong>MAR</strong></td>
<td>Missing At Random (depends on observed data)</td>
<td>Older people less likely to report income</td>
<td>Model the missingness</td>
</tr>
<tr>
<td><strong>NMAR</strong></td>
<td>Not Missing At Random</td>
<td>Sick people skip health surveys</td>
<td>Most challenging</td>
</tr>
</tbody></table>
<hr>
<h2 id="summary">Summary</h2>
<table>
<thead>
<tr>
<th>Concept</th>
<th>Key Insight</th>
</tr>
</thead>
<tbody><tr>
<td><strong>ML Definition</strong></td>
<td>Learning improves with experience</td>
</tr>
<tr>
<td><strong>Supervised Learning</strong></td>
<td>Learn input-output mapping from labeled data</td>
</tr>
<tr>
<td><strong>Classification</strong></td>
<td>Predict discrete categories</td>
</tr>
<tr>
<td><strong>Regression</strong></td>
<td>Predict continuous values</td>
</tr>
<tr>
<td><strong>Probabilistic View</strong></td>
<td>Quantify uncertainty with probability distributions</td>
</tr>
<tr>
<td><strong>Overfitting</strong></td>
<td>Memorizing training data instead of learning patterns</td>
</tr>
<tr>
<td><strong>Generalization</strong></td>
<td>The ultimate goal: perform well on new data</td>
</tr>
<tr>
<td><strong>Unsupervised Learning</strong></td>
<td>Find structure without labels</td>
</tr>
<tr>
<td><strong>RL</strong></td>
<td>Learn from reward signals through interaction</td>
</tr>
</tbody></table>
<p>The probabilistic perspective unifies these concepts ‚Äî learning is inference under uncertainty.</p>

        </article>
        <nav class="page-navigation">
        <a href="probml-00.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Probabilistic Machine Learning Notes</span>
        </a>
        <a href="probml-02-probability.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Probability Foundations</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>