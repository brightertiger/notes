<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Discriminant Analysis | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="probml-09-discriminant_analysis.html" class="active">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">ProbML</a>
          </div>
          <h1 class="page-title">Discriminant Analysis</h1>
          <div class="page-meta"><span class="tag">ProbML</span></div>
        </header>
        <article class="content">
          <h1 id="discriminant-analysis">Discriminant Analysis</h1>
<p>Discriminant analysis covers two fundamental approaches to classification: generative and discriminative models. Understanding their differences helps you choose the right approach for your problem.</p>
<h2 id="generative-vs-discriminative-models">Generative vs. Discriminative Models</h2>
<h3 id="discriminative-models">Discriminative Models</h3>
<p>Model the <strong>posterior probability</strong> directly:
$$p(y | x, \theta)$$</p>
<p><strong>Approach</strong>: Learn the decision boundary between classes.</p>
<p><strong>Examples</strong>: Logistic regression, SVMs, neural networks</p>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Often more accurate when we have enough data</li>
<li>Make fewer assumptions</li>
<li>More robust to model misspecification</li>
</ul>
<h3 id="generative-models">Generative Models</h3>
<p>Model the <strong>joint distribution</strong> via class-conditional densities and priors:
$$p(y = c | x, \theta) \propto p(x | y = c, \theta) \times p(y = c)$$</p>
<p><strong>Components</strong>:</p>
<ul>
<li>$p(x | y = c, \theta)$: Class-conditional density ‚Äî how does data look for class c?</li>
<li>$p(y = c)$: Prior probability ‚Äî how common is class c?</li>
</ul>
<p><strong>Examples</strong>: Naive Bayes, LDA, QDA, Gaussian mixture models</p>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Can generate new samples</li>
<li>Handle missing data naturally</li>
<li>Work well with small datasets</li>
<li>Can incorporate prior knowledge</li>
</ul>
<hr>
<h2 id="gaussian-discriminant-analysis">Gaussian Discriminant Analysis</h2>
<p>Assume class-conditional densities are multivariate Gaussian:
$$p(x | y = c, \theta) = \mathcal{N}(x | \mu_c, \Sigma_c)$$</p>
<h3 id="quadratic-discriminant-analysis-qda">Quadratic Discriminant Analysis (QDA)</h3>
<p>Each class has its own mean <strong>and</strong> covariance:
$$p(x | y = c) = \mathcal{N}(\mu_c, \Sigma_c)$$</p>
<p><strong>Log posterior</strong> (up to constant):
$$\log p(y = c | x) = \log \pi_c - \frac{1}{2}\log|\Sigma_c| - \frac{1}{2}(x - \mu_c)^T\Sigma_c^{-1}(x - \mu_c)$$</p>
<p>The decision boundary is <strong>quadratic</strong> in x (hence the name).</p>
<h3 id="linear-discriminant-analysis-lda">Linear Discriminant Analysis (LDA)</h3>
<p><strong>Key assumption</strong>: All classes share the same covariance matrix.
$$\Sigma_c = \Sigma \quad \text{for all } c$$</p>
<p>This simplifies the log posterior to:
$$\log p(y = c | x) = \log \pi_c + x^T\Sigma^{-1}\mu_c - \frac{1}{2}\mu_c^T\Sigma^{-1}\mu_c$$</p>
<p>The decision boundary is <strong>linear</strong> in x!</p>
<p><strong>Connection to logistic regression</strong>: LDA can be written in the same form, but makes stronger (Gaussian) assumptions.</p>
<h3 id="fitting-gda">Fitting GDA</h3>
<p><strong>Parameter estimation</strong> (usually via MLE):</p>
<ul>
<li>$\hat{\pi}_c = N_c / N$ (class proportions)</li>
<li>$\hat{\mu}<em>c = \frac{1}{N_c}\sum</em>{i: y_i = c} x_i$ (class means)</li>
<li>$\hat{\Sigma}<em>c = \frac{1}{N_c}\sum</em>{i: y_i = c} (x_i - \hat{\mu}_c)(x_i - \hat{\mu}_c)^T$ (class covariances)</li>
</ul>
<p>For LDA, pool covariances:
$$\hat{\Sigma} = \frac{1}{N}\sum_c \sum_{i: y_i = c} (x_i - \hat{\mu}_c)(x_i - \hat{\mu}_c)^T$$</p>
<h3 id="lda-vs-qda-trade-off">LDA vs QDA Trade-off</h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>LDA</th>
<th>QDA</th>
</tr>
</thead>
<tbody><tr>
<td>Parameters</td>
<td>$O(Cd + d^2)$</td>
<td>$O(Cd + Cd^2)$</td>
</tr>
<tr>
<td>Flexibility</td>
<td>Lower</td>
<td>Higher</td>
</tr>
<tr>
<td>Variance</td>
<td>Lower</td>
<td>Higher</td>
</tr>
<tr>
<td>Best when</td>
<td>Classes have similar shapes</td>
<td>Classes have different shapes</td>
</tr>
</tbody></table>
<p><strong>Regularization</strong>: When covariance estimates are unstable, shrink towards LDA:
$$\hat{\Sigma}_c(\alpha) = \alpha\hat{\Sigma}_c + (1-\alpha)\hat{\Sigma}$$</p>
<h3 id="nearest-centroid-classifier">Nearest Centroid Classifier</h3>
<p>Classification simplifies to: assign x to the class with nearest mean (using Mahalanobis distance with Œ£‚Åª¬π).</p>
<hr>
<h2 id="naive-bayes-classifiers">Naive Bayes Classifiers</h2>
<h3 id="the-naive-independence-assumption">The Naive Independence Assumption</h3>
<p>Assume features are <strong>conditionally independent</strong> given the class:
$$p(x | y = c) = \prod_{d=1}^D p(x_d | y = c)$$</p>
<p><strong>Why &quot;naive&quot;?</strong> This assumption is almost never true in practice!</p>
<h3 id="the-posterior">The Posterior</h3>
<p>$$p(y = c | x, \theta) \propto \pi_c \prod_{d=1}^D p(x_d | y = c, \theta_{dc})$$</p>
<p>Each feature contributes independently to the log-posterior.</p>
<h3 id="feature-distributions">Feature Distributions</h3>
<p>Choose distribution based on feature type:</p>
<ul>
<li><strong>Binary features</strong>: Bernoulli distribution</li>
<li><strong>Categorical features</strong>: Categorical distribution</li>
<li><strong>Continuous features</strong>: Gaussian distribution</li>
</ul>
<h3 id="why-naive-bayes-works">Why Naive Bayes Works</h3>
<p>Despite the wrong assumption:</p>
<ol>
<li><strong>Few parameters</strong>: Very sample-efficient</li>
<li><strong>Rankings often correct</strong>: We only need relative, not absolute probabilities</li>
<li><strong>Errors cancel</strong>: Overestimates and underestimates may balance out</li>
</ol>
<h3 id="when-to-use-naive-bayes">When to Use Naive Bayes</h3>
<ul>
<li><strong>Text classification</strong>: High-dimensional, sparse features</li>
<li><strong>Small datasets</strong>: Fewer parameters = less overfitting</li>
<li><strong>Fast prediction needed</strong>: Inference is very efficient</li>
<li><strong>As a baseline</strong>: Simple and hard to beat in some domains</li>
</ul>
<hr>
<h2 id="comparing-approaches">Comparing Approaches</h2>
<h3 id="generative-advantages">Generative Advantages</h3>
<ol>
<li><strong>Handle missing data</strong>: Can marginalize out missing features</li>
<li><strong>Semi-supervised learning</strong>: Can use unlabeled data (via EM)</li>
<li><strong>Prior knowledge</strong>: Natural way to incorporate domain knowledge</li>
<li><strong>Sample generation</strong>: Can create synthetic examples</li>
</ol>
<h3 id="discriminative-advantages">Discriminative Advantages</h3>
<ol>
<li><strong>Direct objective</strong>: Optimize what we care about</li>
<li><strong>Fewer assumptions</strong>: More robust to model misspecification</li>
<li><strong>Often more accurate</strong>: With enough data</li>
<li><strong>Flexible</strong>: Can model complex decision boundaries</li>
</ol>
<h3 id="when-to-use-which">When to Use Which</h3>
<table>
<thead>
<tr>
<th>Situation</th>
<th>Recommendation</th>
</tr>
</thead>
<tbody><tr>
<td>Small dataset</td>
<td>Generative (LDA, NB)</td>
</tr>
<tr>
<td>Large dataset</td>
<td>Discriminative (logistic, NN)</td>
</tr>
<tr>
<td>Missing features</td>
<td>Generative</td>
</tr>
<tr>
<td>Need probabilities</td>
<td>Either (both can be calibrated)</td>
</tr>
<tr>
<td>Need interpretability</td>
<td>LDA or logistic regression</td>
</tr>
<tr>
<td>High dimensions + sparse</td>
<td>Naive Bayes</td>
</tr>
</tbody></table>
<hr>
<h2 id="summary">Summary</h2>
<table>
<thead>
<tr>
<th>Model</th>
<th>Assumptions</th>
<th>Decision Boundary</th>
<th>Parameters</th>
</tr>
</thead>
<tbody><tr>
<td><strong>QDA</strong></td>
<td>Gaussian per class</td>
<td>Quadratic</td>
<td>$O(Cd^2)$</td>
</tr>
<tr>
<td><strong>LDA</strong></td>
<td>Gaussian, shared Œ£</td>
<td>Linear</td>
<td>$O(Cd + d^2)$</td>
</tr>
<tr>
<td><strong>Naive Bayes</strong></td>
<td>Conditional independence</td>
<td>Can be nonlinear</td>
<td>$O(Cd)$</td>
</tr>
<tr>
<td><strong>Logistic Regression</strong></td>
<td>Linear log-odds</td>
<td>Linear</td>
<td>$O(Cd)$</td>
</tr>
</tbody></table>
<p><strong>Key insight</strong>: The choice between generative and discriminative is about the bias-variance trade-off. Generative models make stronger assumptions (more bias) but need less data (less variance).</p>

        </article>
        <nav class="page-navigation">
        <a href="probml-08-optimization.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Optimization</span>
        </a>
        <a href="probml-10-logistic_regression.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Logistic Regression</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>