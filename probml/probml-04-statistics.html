<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Statistics | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="probml-04-statistics.html" class="active">Statistics</a></li>
            <li class="nav-item"><a href="probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">ProbML</a>
          </div>
          <h1 class="page-title">Statistics</h1>
          <div class="page-meta"><span class="tag">ProbML</span></div>
        </header>
        <article class="content">
          <h1 id="statistics">Statistics</h1>
<p>Statistics is the science of learning from data. This chapter covers the key concepts for estimating model parameters and quantifying uncertainty in those estimates.</p>
<h2 id="the-big-picture">The Big Picture</h2>
<p><strong>Inference</strong>: Quantifying uncertainty about unknown quantities using finite data samples.</p>
<p>Two major paradigms:</p>
<ul>
<li><strong>Frequentist</strong>: Parameters are fixed; uncertainty comes from random sampling</li>
<li><strong>Bayesian</strong>: Parameters are random variables with prior distributions</li>
</ul>
<hr>
<h2 id="maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</h2>
<p>The most common approach to parameter estimation: choose parameters that make the observed data most probable.</p>
<h3 id="the-setup">The Setup</h3>
<p>Given:</p>
<ul>
<li>Data: $D = {x_1, x_2, ..., x_N}$ (assumed i.i.d.)</li>
<li>Parametric model: $p(x | \theta)$</li>
</ul>
<h3 id="the-likelihood-function">The Likelihood Function</h3>
<p>$$L(\theta; D) = p(D | \theta) = \prod_{i=1}^N p(x_i | \theta)$$</p>
<p><strong>Key insight</strong>: We treat the data as fixed and vary Œ∏. For which Œ∏ was this data most likely?</p>
<h3 id="log-likelihood">Log-Likelihood</h3>
<p>Products are numerically unstable. Convert to sums using logs:</p>
<p>$$\ell(\theta; D) = \log L(\theta; D) = \sum_{i=1}^N \log p(x_i | \theta)$$</p>
<h3 id="the-mle-estimate">The MLE Estimate</h3>
<p>$$\hat{\theta}_{MLE} = \arg\max_\theta \ell(\theta; D) = \arg\min_\theta -\ell(\theta; D)$$</p>
<p>Equivalently: minimize the <strong>Negative Log-Likelihood (NLL)</strong>.</p>
<h3 id="why-mle-works">Why MLE Works</h3>
<p><strong>Theoretical justifications</strong>:</p>
<ol>
<li><strong>Bayesian view</strong>: MLE is MAP estimate with uniform (uninformative) prior</li>
<li><strong>Information-theoretic view</strong>: MLE minimizes KL divergence between model and empirical distribution</li>
</ol>
<h3 id="sufficient-statistics">Sufficient Statistics</h3>
<p>A <strong>sufficient statistic</strong> summarizes all information in the data relevant to estimating Œ∏.</p>
<p><strong>Example (Bernoulli)</strong>: For $N$ coin flips, the sufficient statistics are:</p>
<ul>
<li>$N_1$ = number of heads</li>
<li>$N_0$ = number of tails</li>
</ul>
<p>You don&#39;t need to know the order of the flips!</p>
<hr>
<h2 id="mle-examples">MLE Examples</h2>
<h3 id="bernoulli-distribution">Bernoulli Distribution</h3>
<p>Model: $p(y | \theta) = \theta^y (1-\theta)^{1-y}$</p>
<p>NLL:
$$\text{NLL}(\theta) = -[N_1 \log\theta + N_0 \log(1-\theta)]$$</p>
<p>Setting derivative to zero:
$$\hat{\theta}_{MLE} = \frac{N_1}{N_0 + N_1} = \frac{\text{# heads}}{\text{# flips}}$$</p>
<p><strong>Intuitive result</strong>: Estimate probability as observed frequency.</p>
<h3 id="gaussian-distribution">Gaussian Distribution</h3>
<p>Model: $p(y | \mu, \sigma^2) = \mathcal{N}(y | \mu, \sigma^2)$</p>
<p>MLE estimates:
$$\hat{\mu} = \frac{1}{N}\sum_{i=1}^N y_i \quad \text{(sample mean)}$$
$$\hat{\sigma}^2 = \frac{1}{N}\sum_{i=1}^N (y_i - \hat{\mu})^2 \quad \text{(sample variance)}$$</p>
<h3 id="linear-regression">Linear Regression</h3>
<p>Model: $p(y | x, w, \sigma^2) = \mathcal{N}(y | w^T x + b, \sigma^2)$</p>
<p>NLL is proportional to:
$$\text{NLL} \propto \sum_{i=1}^N (y_i - w^T x_i - b)^2$$</p>
<p><strong>Key insight</strong>: MLE for Gaussian regression = minimize squared error!</p>
<hr>
<h2 id="empirical-risk-minimization">Empirical Risk Minimization</h2>
<p>ERM generalizes MLE beyond log-loss to any loss function.</p>
<h3 id="definition">Definition</h3>
<p>$$\hat{\theta}<em>{ERM} = \arg\min_\theta \frac{1}{N}\sum</em>{i=1}^N \ell(y_i, f(x_i; \theta))$$</p>
<p><strong>Common loss functions</strong>:</p>
<ul>
<li>Log-loss: gives MLE</li>
<li>Squared loss: regression</li>
<li>0-1 loss: classification accuracy</li>
<li>Hinge loss: SVMs</li>
</ul>
<h3 id="surrogate-losses">Surrogate Losses</h3>
<p>The 0-1 loss is non-differentiable. We use smooth <strong>surrogate losses</strong> that are easier to optimize:</p>
<ul>
<li>Log-loss (cross-entropy)</li>
<li>Hinge loss</li>
<li>Exponential loss</li>
</ul>
<hr>
<h2 id="online-learning">Online Learning</h2>
<p>When data arrives sequentially, we can&#39;t afford to retrain from scratch each time.</p>
<h3 id="recursive-updates">Recursive Updates</h3>
<p>Many statistics can be updated incrementally:</p>
<p><strong>Running mean</strong>:
$$\mu_t = \mu_{t-1} + \frac{1}{t}(x_t - \mu_{t-1})$$</p>
<p><strong>Exponentially Weighted Moving Average (EWMA)</strong>:
$$\mu_t = \beta \mu_{t-1} + (1-\beta) x_t$$</p>
<hr>
<h2 id="regularization">Regularization</h2>
<p>MLE can overfit ‚Äî the estimated model fits training data perfectly but fails on new data.</p>
<h3 id="the-problem">The Problem</h3>
<ul>
<li>Empirical distribution ‚â† true distribution</li>
<li>MLE finds parameters optimal for the empirical distribution</li>
<li>May not generalize well</li>
</ul>
<h3 id="the-solution-add-a-penalty">The Solution: Add a Penalty</h3>
<p>$$\hat{\theta} = \arg\min_\theta \left[\text{NLL}(\theta) + \lambda R(\theta)\right]$$</p>
<p>Where $R(\theta)$ penalizes complex models.</p>
<h3 id="map-estimation">MAP Estimation</h3>
<p>From the Bayesian view, regularization corresponds to adding a prior:</p>
<p>$$\hat{\theta}_{MAP} = \arg\max_\theta [p(D | \theta) \cdot p(\theta)]$$</p>
<p>Taking logs:
$$\hat{\theta}_{MAP} = \arg\min_\theta [-\log p(D|\theta) - \log p(\theta)]$$</p>
<p><strong>Examples</strong>:</p>
<ul>
<li>Gaussian prior ‚Üí L2 regularization (Ridge)</li>
<li>Laplace prior ‚Üí L1 regularization (Lasso)</li>
</ul>
<h3 id="choosing-regularization-strength">Choosing Regularization Strength</h3>
<p>The regularization parameter Œª controls the bias-variance trade-off.</p>
<p><strong>Methods to choose Œª</strong>:</p>
<ul>
<li><strong>Validation set</strong>: Test on held-out data</li>
<li><strong>Cross-validation</strong>: For small datasets</li>
<li><strong>One Standard Error Rule</strong>: Choose simplest model within one SE of best</li>
</ul>
<h3 id="early-stopping">Early Stopping</h3>
<p>Another form of regularization: stop training before the model overfits.</p>
<ul>
<li>Monitor validation error</li>
<li>Stop when it starts increasing</li>
</ul>
<hr>
<h2 id="bayesian-statistics">Bayesian Statistics</h2>
<p>The Bayesian approach treats parameters as random variables.</p>
<h3 id="the-bayesian-recipe">The Bayesian Recipe</h3>
<ol>
<li><strong>Prior</strong>: $p(\theta)$ ‚Äî initial beliefs before seeing data</li>
<li><strong>Likelihood</strong>: $p(D | \theta)$ ‚Äî probability of data given parameters</li>
<li><strong>Posterior</strong>: $p(\theta | D) \propto p(D | \theta) \cdot p(\theta)$ ‚Äî updated beliefs</li>
</ol>
<h3 id="posterior-predictive-distribution">Posterior Predictive Distribution</h3>
<p>To predict new data, <strong>integrate over parameter uncertainty</strong>:</p>
<p>$$p(y_{new} | x_{new}, D) = \int p(y_{new} | x_{new}, \theta) \cdot p(\theta | D) d\theta$$</p>
<p><strong>Compare to plug-in prediction</strong>: $p(y_{new} | x_{new}, \hat{\theta})$</p>
<p>The Bayesian approach properly accounts for uncertainty in Œ∏!</p>
<h3 id="conjugate-priors">Conjugate Priors</h3>
<p>When the prior and posterior have the same functional form:</p>
<ul>
<li><strong>Bernoulli-Beta</strong>: Prior on coin bias</li>
<li><strong>Gaussian-Gaussian</strong>: Prior on Gaussian mean</li>
<li><strong>Poisson-Gamma</strong>: Prior on Poisson rate</li>
</ul>
<p>Makes computation tractable.</p>
<h3 id="map-vs-full-bayesian">MAP vs. Full Bayesian</h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>MAP</th>
<th>Full Bayesian</th>
</tr>
</thead>
<tbody><tr>
<td>Output</td>
<td>Point estimate</td>
<td>Full distribution</td>
</tr>
<tr>
<td>Computation</td>
<td>Optimization</td>
<td>Integration</td>
</tr>
<tr>
<td>Uncertainty</td>
<td>Not captured</td>
<td>Fully captured</td>
</tr>
<tr>
<td>Regularization</td>
<td>Equivalent to adding prior</td>
<td>Built-in</td>
</tr>
</tbody></table>
<hr>
<h2 id="frequentist-statistics">Frequentist Statistics</h2>
<p>In the frequentist view:</p>
<ul>
<li>Parameters Œ∏ are fixed (unknown) constants</li>
<li>Data D is random (sampled from true distribution)</li>
<li>Uncertainty comes from randomness in sampling</li>
</ul>
<h3 id="sampling-distribution">Sampling Distribution</h3>
<p>If we repeated the experiment many times, our estimate $\hat{\theta}$ would vary. The <strong>sampling distribution</strong> describes this variation.</p>
<h3 id="bootstrap">Bootstrap</h3>
<p>When the true sampling distribution is unknown, approximate it by resampling:</p>
<ol>
<li>Draw N samples <strong>with replacement</strong> from your data</li>
<li>Compute the statistic of interest</li>
<li>Repeat many times</li>
<li>The distribution of statistics approximates the sampling distribution</li>
</ol>
<p><strong>Key fact</strong>: Each bootstrap sample contains ~63.2% of unique original observations:
$$P(\text{included}) = 1 - \left(1 - \frac{1}{N}\right)^N \approx 1 - \frac{1}{e} \approx 0.632$$</p>
<h3 id="confidence-intervals">Confidence Intervals</h3>
<p>A 95% confidence interval means: if we repeated the experiment many times, 95% of the computed intervals would contain the true parameter.</p>
<p><strong>Note</strong>: This is NOT the same as &quot;95% probability that Œ∏ is in this interval&quot;!</p>
<hr>
<h2 id="bias-variance-trade-off">Bias-Variance Trade-off</h2>
<h3 id="bias">Bias</h3>
<p>How far off is our estimator on average?</p>
<p>$$\text{bias}(\hat{\theta}) = \mathbb{E}[\hat{\theta}] - \theta^*$$</p>
<p><strong>Unbiased</strong>: $\mathbb{E}[\hat{\theta}] = \theta^*$</p>
<p><strong>Example</strong>: Sample variance $\frac{1}{N}\sum(x_i - \bar{x})^2$ is biased! The unbiased version divides by N-1.</p>
<h3 id="variance">Variance</h3>
<p>How much does our estimate fluctuate across different datasets?</p>
<p>$$\text{Var}(\hat{\theta}) = \mathbb{E}[(\hat{\theta} - \mathbb{E}[\hat{\theta}])^2]$$</p>
<h3 id="mean-squared-error">Mean Squared Error</h3>
<p>Combines both:
$$\text{MSE}(\hat{\theta}) = \mathbb{E}[(\hat{\theta} - \theta^*)^2] = \text{bias}^2 + \text{variance}$$</p>
<p><strong>Key insight</strong>: Sometimes it&#39;s worth accepting bias if it substantially reduces variance!</p>
<p>This is exactly what regularization does.</p>
<hr>
<h2 id="summary">Summary</h2>
<table>
<thead>
<tr>
<th>Concept</th>
<th>Key Idea</th>
</tr>
</thead>
<tbody><tr>
<td><strong>MLE</strong></td>
<td>Choose Œ∏ that maximizes probability of observed data</td>
</tr>
<tr>
<td><strong>NLL</strong></td>
<td>Negative log-likelihood; what we minimize</td>
</tr>
<tr>
<td><strong>Sufficient Statistics</strong></td>
<td>Compress data without losing information about Œ∏</td>
</tr>
<tr>
<td><strong>ERM</strong></td>
<td>Generalization of MLE to any loss function</td>
</tr>
<tr>
<td><strong>Regularization</strong></td>
<td>Penalty on complexity to prevent overfitting</td>
</tr>
<tr>
<td><strong>MAP</strong></td>
<td>MLE + prior = regularized MLE</td>
</tr>
<tr>
<td><strong>Bayesian</strong></td>
<td>Full distribution over Œ∏, not just point estimate</td>
</tr>
<tr>
<td><strong>Posterior Predictive</strong></td>
<td>Integrate predictions over parameter uncertainty</td>
</tr>
<tr>
<td><strong>Bootstrap</strong></td>
<td>Approximate sampling distribution by resampling</td>
</tr>
<tr>
<td><strong>Bias-Variance</strong></td>
<td>MSE = bias¬≤ + variance; trade-off is fundamental</td>
</tr>
</tbody></table>

        </article>
        <nav class="page-navigation">
        <a href="probml-03-probability.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Probability: Advanced Topics</span>
        </a>
        <a href="probml-05-decision_theory.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Decision Theory</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>