<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Recommendation Systems | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="probml-21-recsys.html" class="active">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">ProbML</a>
          </div>
          <h1 class="page-title">Recommendation Systems</h1>
          <div class="page-meta"><span class="tag">ProbML</span></div>
        </header>
        <article class="content">
          <h1 id="recommendation-systems">Recommendation Systems</h1>
<p>Recommendation systems predict user preferences for items (movies, products, songs, etc.). They power personalization across the internet ‚Äî from Netflix to Amazon to Spotify.</p>
<h2 id="the-big-picture">The Big Picture</h2>
<p><strong>The problem</strong>: Users interact with a tiny fraction of available items. Can we predict what they&#39;d like?</p>
<p><strong>Key challenge</strong>: The user-item matrix is extremely sparse (&gt;99% missing).</p>
<p><strong>Goal</strong>: Fill in the missing entries (predict ratings) or rank items for each user.</p>
<hr>
<h2 id="types-of-feedback">Types of Feedback</h2>
<h3 id="explicit-feedback">Explicit Feedback</h3>
<p>Users directly express preferences:</p>
<ul>
<li>Star ratings (1-5)</li>
<li>Thumbs up/down</li>
<li>Reviews</li>
</ul>
<p><strong>Pros</strong>: Clear signal about preferences.
<strong>Cons</strong>: Sparse (users rarely rate), not missing at random (users rate things they care about).</p>
<h3 id="implicit-feedback">Implicit Feedback</h3>
<p>Inferred from user behavior:</p>
<ul>
<li>Clicks, views, purchases</li>
<li>Time spent</li>
<li>Add to cart</li>
</ul>
<p><strong>Pros</strong>: Abundant, always available.
<strong>Cons</strong>: Noisy, positive-only (absence doesn&#39;t mean dislike).</p>
<hr>
<h2 id="collaborative-filtering">Collaborative Filtering</h2>
<h3 id="the-core-idea">The Core Idea</h3>
<p>Users &quot;collaborate&quot; to help recommend items:</p>
<ul>
<li>Users with similar preferences in the past will have similar preferences in the future</li>
<li>Items liked by similar users are likely to be liked by the target user</li>
</ul>
<h3 id="user-based-cf">User-Based CF</h3>
<p>For target user u and item i:
$$\hat{y}<em>{ui} = \frac{\sum</em>{u&#39;} \text{sim}(u, u&#39;) \cdot y_{u&#39;i}}{\sum_{u&#39;} \text{sim}(u, u&#39;)}$$</p>
<p><strong>Steps</strong>:</p>
<ol>
<li>Find users similar to u (based on rating patterns)</li>
<li>Aggregate their ratings for item i</li>
</ol>
<h3 id="item-based-cf">Item-Based CF</h3>
<p>For target user u and item i:
$$\hat{y}<em>{ui} = \frac{\sum</em>{i&#39;} \text{sim}(i, i&#39;) \cdot y_{ui&#39;}}{\sum_{i&#39;} \text{sim}(i, i&#39;)}$$</p>
<p><strong>Steps</strong>:</p>
<ol>
<li>Find items similar to i (based on who rated them)</li>
<li>Aggregate user u&#39;s ratings for those items</li>
</ol>
<p><strong>In practice</strong>: Item-based often preferred (item similarities more stable than user similarities).</p>
<h3 id="challenges">Challenges</h3>
<ul>
<li><strong>Sparsity</strong>: Few common ratings for similarity calculation</li>
<li><strong>Scalability</strong>: Computing all pairwise similarities is expensive</li>
<li><strong>Cold start</strong>: No history for new users/items</li>
</ul>
<hr>
<h2 id="matrix-factorization">Matrix Factorization</h2>
<h3 id="the-idea">The Idea</h3>
<p>The rating matrix R can be approximated as a product of low-rank matrices:
$$R \approx U \cdot V^T$$</p>
<p>Where:</p>
<ul>
<li>U is user matrix (N √ó K): Each row is a user&#39;s &quot;embedding&quot;</li>
<li>V is item matrix (M √ó K): Each row is an item&#39;s &quot;embedding&quot;</li>
<li>K &lt;&lt; N, M (typically K = 10-200)</li>
</ul>
<h3 id="interpretation">Interpretation</h3>
<p>Each dimension captures a latent &quot;factor&quot;:</p>
<ul>
<li>Movie factors might capture: Action content, Romance level, Production year...</li>
<li>User factors capture: Preference for action, romance, etc.</li>
</ul>
<p><strong>Prediction</strong>: $\hat{y}_{ui} = u_u^T v_i$ (dot product of embeddings)</p>
<h3 id="training">Training</h3>
<p>Minimize squared error on observed ratings:
$$L = \sum_{(u,i) \in \text{observed}} (y_{ui} - u_u^T v_i)^2 + \lambda(|U|^2 + |V|^2)$$</p>
<p><strong>Note</strong>: Can&#39;t use SVD directly (missing values). Use:</p>
<ul>
<li><strong>Alternating Least Squares (ALS)</strong>: Fix U, solve for V; fix V, solve for U</li>
<li><strong>SGD</strong>: Stochastic gradient descent on observed entries</li>
</ul>
<h3 id="adding-biases">Adding Biases</h3>
<p>Users and items have inherent tendencies:
$$\hat{y}_{ui} = \mu + b_u + c_i + u_u^T v_i$$</p>
<p>Where:</p>
<ul>
<li>Œº: Global average rating</li>
<li>$b_u$: User bias (some users rate higher on average)</li>
<li>$c_i$: Item bias (some items are generally liked more)</li>
</ul>
<hr>
<h2 id="probabilistic-matrix-factorization">Probabilistic Matrix Factorization</h2>
<h3 id="bayesian-approach">Bayesian Approach</h3>
<p>Model ratings as:
$$p(y_{ui} | u_u, v_i) = \mathcal{N}(\mu + b_u + c_i + u_u^T v_i, \sigma^2)$$</p>
<p>Add priors on embeddings:
$$u_u \sim \mathcal{N}(0, \sigma_u^2 I)$$
$$v_i \sim \mathcal{N}(0, \sigma_v^2 I)$$</p>
<p><strong>Benefits</strong>:</p>
<ul>
<li>Principled handling of uncertainty</li>
<li>Regularization from priors</li>
<li>Can incorporate side information</li>
</ul>
<hr>
<h2 id="bayesian-personalized-ranking-bpr">Bayesian Personalized Ranking (BPR)</h2>
<h3 id="for-implicit-feedback">For Implicit Feedback</h3>
<p><strong>Problem</strong>: With implicit data, we only have positive examples.</p>
<p><strong>Approach</strong>: Learn to rank positives above negatives.</p>
<p><strong>Assumption</strong>: User prefers items they interacted with over items they didn&#39;t.</p>
<h3 id="the-loss">The Loss</h3>
<p>For triplet (user, positive item, negative item):
$$L = -\log \sigma(f(u, i^+) - f(u, i^-))$$</p>
<p>Where f is the prediction score (e.g., $u_u^T v_i$).</p>
<p><strong>Training</strong>: Sample triplets and optimize.</p>
<hr>
<h2 id="factorization-machines">Factorization Machines</h2>
<h3 id="beyond-matrix-factorization">Beyond Matrix Factorization</h3>
<p>Matrix factorization only captures user-item interactions.</p>
<p><strong>Factorization Machines</strong> generalize to any features:
$$f(x) = \mu + \sum_{j=1}^d w_j x_j + \sum_{j &lt; k} (v_j \cdot v_k) x_j x_k$$</p>
<p>Where:</p>
<ul>
<li>x: Feature vector (one-hot user, one-hot item, plus any other features)</li>
<li>v_j: Embedding for feature j</li>
</ul>
<h3 id="advantages">Advantages</h3>
<ul>
<li>Can incorporate <strong>side information</strong>: User demographics, item attributes, context (time, location)</li>
<li>Handles <strong>cold start</strong> better</li>
<li>Same framework for different feature types</li>
</ul>
<h3 id="connection-to-mf">Connection to MF</h3>
<p>When features are just user and item IDs:
$$f = \mu + b_u + c_i + u_u^T v_i$$</p>
<p>Exactly matrix factorization with biases!</p>
<hr>
<h2 id="the-cold-start-problem">The Cold Start Problem</h2>
<h3 id="the-challenge">The Challenge</h3>
<p>New users or items have no interaction history.</p>
<h3 id="solutions">Solutions</h3>
<p><strong>Content-based</strong>: Use features instead of collaborative signal</p>
<ul>
<li>New user: Ask preferences, use demographics</li>
<li>New item: Use item attributes, description</li>
</ul>
<p><strong>Hybrid methods</strong>: Combine collaborative and content-based</p>
<p><strong>Active learning</strong>: Ask strategic questions to new users</p>
<p><strong>Transfer learning</strong>: Leverage data from related domains</p>
<hr>
<h2 id="exploration-exploitation-trade-off">Exploration-Exploitation Trade-off</h2>
<h3 id="the-problem">The Problem</h3>
<p>If we only recommend what users already like, they never discover new interests.</p>
<p><strong>Counterfactual</strong>: Users might love items they never see!</p>
<h3 id="approaches">Approaches</h3>
<p><strong>Multi-Armed Bandits</strong>:</p>
<ul>
<li><strong>Thompson Sampling</strong>: Sample from posterior, act greedily</li>
<li><strong>UCB</strong>: Optimism in the face of uncertainty</li>
</ul>
<p><strong>Contextual Bandits</strong>: Personalized exploration based on user features</p>
<p><strong>Diversity</strong>: Ensure recommendations aren&#39;t all the same type</p>
<hr>
<h2 id="deep-learning-for-recsys">Deep Learning for RecSys</h2>
<h3 id="neural-collaborative-filtering">Neural Collaborative Filtering</h3>
<p>Replace dot product with neural network:
$$\hat{y}<em>{ui} = f</em>{neural}([u_u; v_i])$$</p>
<p><strong>Benefit</strong>: Captures non-linear interactions.</p>
<h3 id="sequential-recommendations">Sequential Recommendations</h3>
<p>Model user&#39;s sequence of interactions with RNN/Transformer:
$$h_t = f(x_t, h_{t-1})$$</p>
<p><strong>Benefit</strong>: Captures temporal dynamics.</p>
<h3 id="two-tower-models">Two-Tower Models</h3>
<p>Separate encoders for users and items:</p>
<ul>
<li>Fast serving (pre-compute item embeddings)</li>
<li>Easy to add features</li>
</ul>
<hr>
<h2 id="evaluation-metrics">Evaluation Metrics</h2>
<h3 id="for-rating-prediction">For Rating Prediction</h3>
<ul>
<li><strong>RMSE</strong>: $\sqrt{\frac{1}{N}\sum (y - \hat{y})^2}$</li>
<li><strong>MAE</strong>: $\frac{1}{N}\sum |y - \hat{y}|$</li>
</ul>
<h3 id="for-ranking">For Ranking</h3>
<ul>
<li><strong>Precision@K</strong>: Fraction of top-K recommendations that are relevant</li>
<li><strong>Recall@K</strong>: Fraction of relevant items in top-K</li>
<li><strong>NDCG</strong>: Accounts for position (higher rank = more important)</li>
<li><strong>MAP</strong>: Mean average precision</li>
</ul>
<h3 id="offline-vs-online">Offline vs. Online</h3>
<p><strong>Offline</strong>: Historical data, fast to compute but may not reflect real preferences.</p>
<p><strong>Online (A/B testing)</strong>: Real users, gold standard but expensive and slow.</p>
<hr>
<h2 id="summary">Summary</h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Key Idea</th>
<th>Best For</th>
</tr>
</thead>
<tbody><tr>
<td><strong>User-Based CF</strong></td>
<td>Similar users ‚Üí similar items</td>
<td>Small, stable user base</td>
</tr>
<tr>
<td><strong>Item-Based CF</strong></td>
<td>Similar items</td>
<td>Most practical CF</td>
</tr>
<tr>
<td><strong>Matrix Factorization</strong></td>
<td>Low-rank approximation</td>
<td>General purpose</td>
</tr>
<tr>
<td><strong>BPR</strong></td>
<td>Learn to rank</td>
<td>Implicit feedback</td>
</tr>
<tr>
<td><strong>Factorization Machines</strong></td>
<td>Feature interactions</td>
<td>Side information</td>
</tr>
<tr>
<td><strong>Neural Methods</strong></td>
<td>Non-linear patterns</td>
<td>Large data, complex patterns</td>
</tr>
</tbody></table>
<h3 id="practical-recommendations">Practical Recommendations</h3>
<ol>
<li><strong>Start simple</strong>: Matrix factorization with biases often hard to beat</li>
<li><strong>Add biases</strong>: Critical for good performance</li>
<li><strong>Handle implicit correctly</strong>: Use ranking losses, not rating losses</li>
<li><strong>Address cold start</strong>: Hybrid methods or feature-based fallback</li>
<li><strong>Evaluate carefully</strong>: Ranking metrics often more meaningful than RMSE</li>
<li><strong>Consider fairness</strong>: Avoid filter bubbles, ensure diverse recommendations</li>
</ol>

        </article>
        <nav class="page-navigation">
        <a href="probml-19-ssl.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Self-Supervised and Semi-Supervised Learning</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>