<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Exemplar-Based Methods | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="probml-16-exemplar.html" class="active">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">ProbML</a>
          </div>
          <h1 class="page-title">Exemplar-Based Methods</h1>
          <div class="page-meta"><span class="tag">ProbML</span></div>
        </header>
        <article class="content">
          <h1 id="exemplar-based-methods">Exemplar-Based Methods</h1>
<p>Exemplar methods (also called instance-based or memory-based) keep training data around and use it directly for prediction. The classic example is K-Nearest Neighbors (KNN).</p>
<h2 id="the-big-picture">The Big Picture</h2>
<p><strong>Parametric models</strong>: Learn parameters Œ∏, discard training data at test time.</p>
<ul>
<li>Parameters: Fixed, doesn&#39;t grow with data</li>
</ul>
<p><strong>Non-parametric models</strong>: Keep training data, use it directly.</p>
<ul>
<li>Model complexity grows with data size</li>
<li>Can adapt to arbitrary complexity</li>
</ul>
<hr>
<h2 id="instance-based-learning">Instance-Based Learning</h2>
<h3 id="the-approach">The Approach</h3>
<ol>
<li>Store training examples</li>
<li>At test time, find similar training examples</li>
<li>Predict based on their labels</li>
</ol>
<p><strong>Key ingredient</strong>: A good <strong>distance/similarity measure</strong>.</p>
<hr>
<h2 id="k-nearest-neighbors-knn">K-Nearest Neighbors (KNN)</h2>
<h3 id="classification">Classification</h3>
<p>Find K closest training points; vote on the label:
$$p(y = c | x, D) = \frac{1}{K} \sum_{i \in N_K(x)} I{y_i = c}$$</p>
<p><strong>Hyperparameter K</strong>:</p>
<ul>
<li>K = 1: Highly flexible, noisy</li>
<li>K = N: Predicts majority class always</li>
<li>Typical: K = 5-10, or tune via cross-validation</li>
</ul>
<h3 id="regression">Regression</h3>
<p>Average the labels of K nearest neighbors:
$$\hat{y} = \frac{1}{K} \sum_{i \in N_K(x)} y_i$$</p>
<h3 id="distance-metrics">Distance Metrics</h3>
<p><strong>Euclidean distance</strong>:
$$d(x, x&#39;) = |x - x&#39;|_2 = \sqrt{\sum_j (x_j - x&#39;_j)^2}$$</p>
<p><strong>Mahalanobis distance</strong> (accounts for correlations):
$$d_M(x, x&#39;) = \sqrt{(x - x&#39;)^T M (x - x&#39;)}$$</p>
<p>Where M is a positive definite matrix (often $M = \Sigma^{-1}$).</p>
<p><strong>If M = I</strong>: Reduces to Euclidean distance.</p>
<hr>
<h2 id="the-curse-of-dimensionality">The Curse of Dimensionality</h2>
<h3 id="the-problem">The Problem</h3>
<p>In high dimensions, distances become meaningless:</p>
<ul>
<li>All points become approximately equidistant</li>
<li>Local neighborhoods become empty</li>
<li>Need exponentially more data to fill space</li>
</ul>
<h3 id="example">Example</h3>
<p>Consider the fraction of volume within 10% of the edges:</p>
<ul>
<li>1D: 20%</li>
<li>10D: 89%</li>
<li>100D: 99.99999...%</li>
</ul>
<p>Almost all points are near the boundary!</p>
<h3 id="solutions">Solutions</h3>
<ol>
<li><strong>Dimensionality reduction</strong> (PCA, autoencoders)</li>
<li><strong>Feature selection</strong></li>
<li><strong>Metric learning</strong> (learn a better distance)</li>
</ol>
<hr>
<h2 id="computational-efficiency">Computational Efficiency</h2>
<h3 id="naive-approach">Naive Approach</h3>
<p>For each query, compute distance to all N training points.</p>
<ul>
<li>Time: O(Nd) per query</li>
<li>Infeasible for large datasets</li>
</ul>
<h3 id="approximate-nearest-neighbors">Approximate Nearest Neighbors</h3>
<p><strong>KD-Trees</strong>:</p>
<ul>
<li>Binary tree that partitions space</li>
<li>O(log N) for low dimensions</li>
<li>Degrades in high dimensions</li>
</ul>
<p><strong>Locality-Sensitive Hashing (LSH)</strong>:</p>
<ul>
<li>Hash similar items to same bucket</li>
<li>Sublinear query time</li>
<li>Approximate, not exact</li>
</ul>
<hr>
<h2 id="open-set-recognition">Open Set Recognition</h2>
<p><strong>Standard classification</strong>: All test classes seen during training.</p>
<p><strong>Open set</strong>: New, unseen classes may appear at test time.</p>
<p><strong>KNN advantage</strong>: Can handle novel classes naturally by looking at nearest neighbors.</p>
<p><strong>Applications</strong>:</p>
<ul>
<li>Person re-identification</li>
<li>Anomaly detection</li>
<li>Few-shot learning</li>
</ul>
<hr>
<h2 id="learning-distance-metrics">Learning Distance Metrics</h2>
<h3 id="motivation">Motivation</h3>
<p>Euclidean distance may not reflect true similarity.</p>
<p><strong>Goal</strong>: Learn a distance metric M that captures task-relevant similarity.</p>
<h3 id="large-margin-nearest-neighbors-lmnn">Large Margin Nearest Neighbors (LMNN)</h3>
<p>Learn M such that:</p>
<ol>
<li>Points with same label are close</li>
<li>Points with different labels are far (by margin m)</li>
</ol>
<p><strong>Constraint</strong>: $M = W^T W$ ensures positive definiteness.</p>
<hr>
<h2 id="deep-metric-learning">Deep Metric Learning</h2>
<h3 id="the-idea">The Idea</h3>
<p>Learn an embedding function $f(x; \theta)$ such that:</p>
<ul>
<li>Similar examples are close in embedding space</li>
<li>Dissimilar examples are far</li>
</ul>
<h3 id="siamese-networks">Siamese Networks</h3>
<p>Two copies of same network process two inputs.</p>
<p><strong>Contrastive Loss</strong>:
$$L = I{y_i = y_j} \cdot d(x_i, x_j)^2 + I{y_i \neq y_j} \cdot [m - d(x_i, x_j)]_+^2$$</p>
<ul>
<li>Same class: Minimize distance</li>
<li>Different class: Push apart (up to margin m)</li>
</ul>
<h3 id="triplet-loss">Triplet Loss</h3>
<p>Use triplets: (anchor, positive, negative)</p>
<p>$$L = [m + d(a, p) - d(a, n)]_+$$</p>
<p><strong>Goal</strong>: Anchor should be closer to positive than negative by margin m.</p>
<h3 id="hard-negative-mining">Hard Negative Mining</h3>
<p>Random negatives are too easy (already far from anchor).</p>
<p><strong>Solution</strong>: Sample hard negatives that are close to anchor but from different class.</p>
<h3 id="connection-to-cosine-similarity">Connection to Cosine Similarity</h3>
<p>For normalized embeddings:
$$|e_1 - e_2|^2 = 2(1 - \cos\theta)$$</p>
<p>Euclidean distance and cosine similarity are equivalent for unit vectors.</p>
<hr>
<h2 id="kernel-density-estimation">Kernel Density Estimation</h2>
<h3 id="the-idea-1">The Idea</h3>
<p>Estimate the probability density by placing kernels at each data point:
$$\hat{p}(x) = \frac{1}{N} \sum_{n=1}^N K_h(x - x_n)$$</p>
<p><strong>Gaussian kernel</strong>:
$$K_h(x) = \frac{1}{h\sqrt{2\pi}} \exp\left(-\frac{x^2}{2h^2}\right)$$</p>
<h3 id="bandwidth-h">Bandwidth h</h3>
<p>Controls smoothness:</p>
<ul>
<li>Small h: Spiky, overfitting</li>
<li>Large h: Over-smooth, underfitting</li>
</ul>
<p>Choose via cross-validation.</p>
<h3 id="connection-to-gmm">Connection to GMM</h3>
<p>KDE is like a GMM where:</p>
<ul>
<li>Each point is its own cluster center</li>
<li>All clusters have same (spherical) covariance</li>
<li>No mixing proportions to learn</li>
</ul>
<h3 id="kde-for-classification">KDE for Classification</h3>
<p>Use Bayes rule with class-conditional densities:
$$p(y = c | x) \propto \pi_c \cdot \hat{p}(x | y = c)$$</p>
<hr>
<h2 id="kde-vs-knn">KDE vs KNN</h2>
<p><strong>Connection</strong>: Both use local neighborhoods.</p>
<ul>
<li><strong>KDE</strong>: Fixed bandwidth, variable number of neighbors</li>
<li><strong>KNN</strong>: Variable bandwidth (grows until K neighbors), fixed number of neighbors</li>
</ul>
<p><strong>Dual view</strong>: KNN adapts to local density automatically.</p>
<hr>
<h2 id="summary">Summary</h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Key Idea</th>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody><tr>
<td><strong>KNN</strong></td>
<td>Vote of K nearest neighbors</td>
<td>Simple, no training</td>
<td>Slow at test time</td>
</tr>
<tr>
<td><strong>Metric Learning</strong></td>
<td>Learn task-specific distance</td>
<td>Better than Euclidean</td>
<td>Requires labeled pairs</td>
</tr>
<tr>
<td><strong>Deep Metric</strong></td>
<td>Embed + distance</td>
<td>Handles high dimensions</td>
<td>Needs lots of data</td>
</tr>
<tr>
<td><strong>KDE</strong></td>
<td>Kernels at each point</td>
<td>Density estimation</td>
<td>Curse of dimensionality</td>
</tr>
</tbody></table>
<h3 id="when-to-use-exemplar-methods">When to Use Exemplar Methods</h3>
<p><strong>Good for</strong>:</p>
<ul>
<li>Few training examples per class (few-shot learning)</li>
<li>Classes change over time (no retraining needed)</li>
<li>Interpretable predictions (&quot;similar to example X&quot;)</li>
<li>Baseline before trying complex methods</li>
</ul>
<p><strong>Challenges</strong>:</p>
<ul>
<li>High-dimensional data (curse of dimensionality)</li>
<li>Large training sets (computational cost)</li>
<li>Need good distance metric</li>
</ul>

        </article>
        <nav class="page-navigation">
        <a href="probml-15-rnn.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Recurrent Neural Networks and Transformers</span>
        </a>
        <a href="probml-18-trees.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Decision Trees and Ensembles</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>