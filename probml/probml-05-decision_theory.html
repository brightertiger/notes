<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Decision Theory | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">SLP Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regex</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">Tokenization</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vectors</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="probml-00.html">ProbML Notes</a></li>
            <li class="nav-item"><a href="probml-01-introduction.html">Introduction</a></li>
            <li class="nav-item"><a href="probml-02-probability.html">Probability</a></li>
            <li class="nav-item"><a href="probml-03-probability.html">Probability (Advanced Topics)</a></li>
            <li class="nav-item"><a href="probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="probml-05-decision_theory.html" class="active">Decision Theory</a></li>
            <li class="nav-item"><a href="probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="probml-13-ffnn.html">Feed Forward Neural Networks</a></li>
            <li class="nav-item"><a href="probml-14-cnn.html">Convolution NN</a></li>
            <li class="nav-item"><a href="probml-15-rnn.html">Recurrent Neural Networks</a></li>
            <li class="nav-item"><a href="probml-16-exemplar.html">Exemplar Methods</a></li>
            <li class="nav-item"><a href="probml-18-trees.html">Trees</a></li>
            <li class="nav-item"><a href="probml-19-ssl.html">SSL</a></li>
            <li class="nav-item"><a href="probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">ProbML</a>
          </div>
          <h1 class="page-title">Decision Theory</h1>
          <div class="page-meta"><span class="tag">ProbML</span></div>
        </header>
        <article class="content">
          <h1 id="decision-theory">Decision Theory</h1>
<ul>
<li><p>Decision theory provides a formal framework for making optimal decisions under uncertainty</p>
</li>
<li><p>Optimal Policy specifies which action to take for each possible observation to minimize risk or maximize utility</p>
</li>
<li><p>Risk Neutrality vs Risk Aversion</p>
<ul>
<li>Risk neutrality: Agent values expected outcomes (e.g., $50 = 0.5 \times $100)</li>
<li>Risk aversion: Agent prefers certain outcomes to uncertain ones with same expected value</li>
<li>Risk preference: Agent prefers uncertainty (gambling) over certainty</li>
</ul>
</li>
<li><p>Decision Rules for Classification</p>
<ul>
<li><p>Zero-One loss: Penalizes misclassification with a unit cost</p>
<ul>
<li>$l_{01}(y, \hat y) = I{y \ne \hat y}$</li>
<li>Optimal policy minimizes risk by choosing the most probable class:<ul>
<li>$R(y | x) = P(y \ne \hat y | x) = 1 - P(y = \hat y | x)$</li>
<li>$\pi(x) = \arg \max P(y | x)$</li>
</ul>
</li>
</ul>
</li>
<li><p>Cost-Sensitive Classification</p>
<ul>
<li>Different error types have different consequences<ul>
<li>False Positive (Type I error): Predict positive when truth is negative</li>
<li>False Negative (Type II error): Predict negative when truth is positive</li>
</ul>
</li>
<li>Different costs can be assigned: $l_{01} \neq c \times l_{10}$</li>
<li>Choose label 1 if the expected cost is lower:<ul>
<li>$p(y=0|x) \times l_{01} &lt; p(y=1|x) \times c \times l_{10}$</li>
</ul>
</li>
<li>The cost ratio c shifts the decision boundary</li>
</ul>
</li>
<li><p>Rejection Option (Abstention)</p>
<ul>
<li>Sometimes it&#39;s better to abstain from making a decision</li>
<li>Three possible actions: predict 0, predict 1, or reject</li>
<li>Cost parameters:<ul>
<li>$\lambda_e$: Cost of making an error</li>
<li>$\lambda_r$: Cost of rejection/abstention</li>
</ul>
</li>
<li>No decision is made when model confidence is below threshold:<ul>
<li>Abstain when $\max_y P(y|x) &lt; 1 - \frac{\lambda_r}{\lambda_e}$</li>
</ul>
</li>
<li>This creates bands of uncertainty where expert input might be required</li>
</ul>
</li>
</ul>
</li>
<li><p>ROC Curves</p>
<ul>
<li><p>Summarize performance across various thresholds</p>
</li>
<li><p>Confusion Matrix</p>
<ul>
<li>Give a threshold $\tau$</li>
<li>Confusion Matrix
- Positive, Negative: Model Prediction
- True, False: Actual Labels
- TP, TN: Correct Predictions
- FP: Model predicts 1, Ground Truth is 0
- FN: Model predicts 0, Ground Truth is 1<ul>
<li>Ratios from Confusion Matrix<ul>
<li>TPR, Sensitivity, Recall<ul>
<li>TP / (TP + FN)</li>
<li>Accuracy in positive predictions</li>
</ul>
</li>
<li>FPR, Type 1 Error rate<ul>
<li>FP / (FP + TN)</li>
<li>Error in Negative Predictions</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>ROC Curve is a plot between FPR (x-axis) and TPR (y-axis) across various thresholds</p>
</li>
<li><p>AUC is a numerical summary of ROC</p>
</li>
<li><p>Equal Error Rate is where ROC crosses -45 degree line.</p>
</li>
<li><p>ROC Curve is insensitive to class imbalance</p>
<ul>
<li>FPR consists of TN in denominator</li>
<li>If TN &gt;&gt; TP, metric becomes insensitive to FPR</li>
</ul>
</li>
<li><p>Precision-Recall Curves</p>
<ul>
<li>The negatives are not model specific but system specific</li>
<li>For a search query, retrieve 50 vs 500 items.  (or tiles vs list)</li>
<li>Precision<ul>
<li>TP / TP + FP</li>
</ul>
</li>
<li>Recall <ul>
<li>TP / TP + FN</li>
</ul>
</li>
<li>There is no dependency on TN</li>
<li>Precision curve has distortions. Smooth it out by interpolation.</li>
<li>To summarize the performance by a scalar<ul>
<li>Precision @ K</li>
<li>Average Precision: Area under interpolated precision curve</li>
<li>mAP or Mean Average Precision is mean of AP across different PR curves (say different queries)</li>
</ul>
</li>
<li>F-Score<ul>
<li>Weighted harmonic mean between precision and recall</li>
<li>${1 \over F} = {1 \over 1 + \beta^2} {1 \over P} + {\beta^2 \over 1 + \beta^2} {1 \over R}$</li>
<li>Harmonic mean imposes more penalty if either precision or recall fall to a very low level</li>
</ul>
</li>
</ul>
</li>
<li><p>Class Imbalance</p>
<ul>
<li>ROC curves are not sensitive to class imbalance. Does not matter which class is defined as 1 or 0.</li>
<li>PR curves are sensitive to class imbalance. Switching classes impacts performance.<ul>
<li>$P = {TP \over TP + FP}$</li>
</ul>
</li>
<li>PR-AUC is more appropriate in case class imbalance</li>
<li>Multiclass problems can be treated as multiple binary classes. One class vs Rest.</li>
<li>Decision threshold calibration finds the optimal threshold for desired trade-off between precision and recall</li>
</ul>
</li>
</ul>
</li>
<li><p>Regression</p>
<ul>
<li>Metrics<ul>
<li>Mean Squared Error: ${1 \over N} \sum (y - \hat y)^2$</li>
<li>Mean Absolute Error: ${1 \over N} \sum |y - \hat y|$</li>
<li>MAE is robust to outliers - proportional to regression with Laplace conditional distribution</li>
<li>MSE has simple calculus and proportional to regression with Gaussian conditional distribution </li>
<li>Huber loss: MSE below cutoff, MAE above cutoff</li>
<li>Loss functions are usually calibrated with a proper scoring rule</li>
<li>Log score of the predicted density at the true value</li>
</ul>
</li>
<li>Quantile Loss<ul>
<li>Probability that value y is less than q quantile is q</li>
<li>$P(y \le y_q) = q$</li>
<li>Linear function with positive slope for over-prediction and negative slope for under-prediction</li>
<li>Slope depends on the quantile q</li>
</ul>
</li>
</ul>
</li>
<li><p>Calibration</p>
<ul>
<li>Property that predicted certainty of events match the frequency of their occurrence</li>
<li>Reliability Diagrams<ul>
<li>x-axis: Predicted Probability</li>
<li>y-axis: Actual Probability</li>
</ul>
</li>
</ul>
</li>
<li><p>Bayes Model Selection and Averaging</p>
<ul>
<li><p>Bayesian approach to model selection</p>
<ul>
<li>Choose $m \in M$ to maximize posterior $p(m|D)$</li>
<li>$p(m|D) \propto p(D|m)p(m)$</li>
<li>Use uniform prior over model classes</li>
<li>$p(D|m) = \int p(D|\theta,m)p(\theta|m)d\theta$</li>
<li>Integration over all possible parameter values</li>
<li>In practice, models might be compared using BIC or AIC</li>
<li>BIC approximates Bayes factor</li>
<li>AIC approximates Cross-validation</li>
</ul>
</li>
<li><p>Akaike&#39;s Information Criterion</p>
<ul>
<li>Maximizes predictive density of held out data</li>
<li>Approximating out-of-sample generalization</li>
<li>As trained model fits the training data</li>
<li>$AIC = -2LL + 2C$</li>
<li>LL is log-likelihood</li>
<li>C is number of parameters in the model</li>
</ul>
</li>
<li><p>Bayesian Information Criteria</p>
<ul>
<li>Biases in favor of simpler models</li>
<li>$BIC = -2LL + C\log N$</li>
<li>LL is log-likelihood</li>
<li>C is number of parameters in the model</li>
<li>N is the number of observations</li>
</ul>
</li>
<li><p>Minimum Description Length</p>
<ul>
<li>Minimum number of bits needed to describe the model and the data</li>
<li>If we describe model in $L_1$ bits, and then describe the dataset using the model in $L_2$ bits. </li>
<li>The MDL for the model is $L_1 + L_2$</li>
</ul>
</li>
<li><p>Approximation of Bayesian Model Comparison</p>
<ul>
<li>Integrated likelihood is analytically intractable</li>
<li>Approximate the integral by Laplace approximation</li>
<li>Uses 2nd order taylor expansion around MLE / MAP</li>
<li>The model is approximated as gaussian with constant determinant as n increases</li>
<li>AIC is an approximation of KL-Divergence between the the true model and a fitted model</li>
<li>Simple to compute</li>
<li>AIC = $2 \times \text{dof} - 2 \times LL$ (dof: degrees of freedom)</li>
<li>Sample error can result in overfitting</li>
<li>AIC_C: when sample size is small<ul>
<li>AIC_C = AIC + ${2C(C+1) \over N-C-1}$</li>
<li>C is additional penalty for increasing number of parameters</li>
</ul>
</li>
</ul>
</li>
<li><p>Widely Applicable / Watanabe-Akaike Information Criterion </p>
<ul>
<li>Penalizes models based on effective degrees of freedom</li>
<li>$C = 2 \times \text{dof}$</li>
</ul>
</li>
</ul>
</li>
<li><p>Frequentist Decision Theory</p>
<ul>
<li>Risk of an estimator is the expected loss when applying the estimator to data sampled from likelihood function $p( y,x | \theta)$</li>
<li>Bayes Risk<ul>
<li>True generating function unknown</li>
<li>Assume a prior and then average it out</li>
</ul>
</li>
<li>Maximum Risk<ul>
<li>Minimize the maximum risk</li>
</ul>
</li>
<li>Consistent Estimator<ul>
<li>Recovers true parameter in the limit of infinite data</li>
</ul>
</li>
<li>Empirical Risk Minimization<ul>
<li>Population Risk<ul>
<li>Expectation of the loss function w.r.t. true distribution</li>
<li>True distribution is unknown</li>
<li>$R(f, \theta^*) = \mathbf{E}[l(\theta^*, \pi(D))]$</li>
</ul>
</li>
<li>Empirical Risk<ul>
<li>Approximate the expectation of loss by using training data samples</li>
<li>$R(f, D) = \mathbf{E}[l(y, \pi(x))]$</li>
</ul>
</li>
<li>Empirical Risk Minimizaiton<ul>
<li>Optimize empirical risk over hypothesis space of functions</li>
<li>$f_{ERM} = \arg \min_H R(f,D)$</li>
</ul>
</li>
<li>Approximation Error<ul>
<li>Risk that the chosen true parameters don&#39;t lie in the hypothesis space</li>
</ul>
</li>
<li>Estimation Error<ul>
<li>Error due to having finite training set</li>
<li>Difference between training error and test error</li>
<li>Generalization Gap</li>
</ul>
</li>
<li>Regularized Risk<ul>
<li>Add complexity penalty</li>
<li>$R_\lambda(f,D) = R(f,D) + \lambda C(f)$</li>
<li>Complexity term resembles the prior term in MAP estimation</li>
</ul>
</li>
<li>Structural Risk<ul>
<li>Empirical underestimates population risk</li>
<li>Structural risk minimization is to pick the right level of model complexity by minimizing regularized risk and cross-validation</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Statistical Learning Theory</p>
<ul>
<li>Upper bound on generalization error with certain probability</li>
<li>PAC (probably approximately correct) learnable</li>
<li>Hoeffding&#39;s Inequality<ul>
<li>Upper bound on generalization error</li>
</ul>
</li>
<li>VC Dimension<ul>
<li>Measures the degrees of freedom of a hypothesis class</li>
</ul>
</li>
</ul>
</li>
<li><p>Frequentist Hypothesis Testing</p>
<ul>
<li>Null vs Alternate Hypothesis</li>
<li>Likelihood Ratio Test<ul>
<li>$p(D| H_0) / p(D| H_1)$</li>
</ul>
</li>
<li>Null Hypothesis Significance Testing<ul>
<li>Type-1 Error<ul>
<li>P(Reject H0 | H0 is True)</li>
<li>Significance of the test</li>
<li>$\alpha$</li>
</ul>
</li>
<li>Type-2 Error<ul>
<li>P(Accept H0 | H1 is True)</li>
<li>$\beta$</li>
<li>Power of the test is $1 - \beta$</li>
</ul>
</li>
<li>Most powerful test is the one with highest power given a level of significance</li>
<li>Neyman-Pearson lemma: Likelihood ratio test is the most powerful test</li>
<li>p-value<ul>
<li>Probability, under the null hypothesis, of observing a test statistic larger that that actually observed</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </article>
        <nav class="page-navigation">
        <a href="probml-04-statistics.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Statistics</span>
        </a>
        <a href="probml-06-information_theory.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Information Theory</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>