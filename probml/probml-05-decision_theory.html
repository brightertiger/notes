
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Decision Theory | My Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap">
  <!-- MathJax for LaTeX support -->
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: {
        enableMenu: false
      }
    };
  </script>
</head>
<body>
  <div class="container">
    <header>
      <h1>Decision Theory</h1>
      
      <a href="../index.html" class="home-link">‚Üê Back to Home</a>
    </header>
    <main class="content">
      <h1 id="decision-theory">Decision Theory</h1>
<ul>
<li><p>Decision theory provides a formal framework for making optimal decisions under uncertainty</p>
</li>
<li><p>Optimal Policy specifies which action to take for each possible observation to minimize risk or maximize utility</p>
</li>
<li><p>Risk Neutrality vs Risk Aversion</p>
<ul>
<li>Risk neutrality: Agent values expected outcomes (e.g., $50 = 0.5 \times $100)</li>
<li>Risk aversion: Agent prefers certain outcomes to uncertain ones with same expected value</li>
<li>Risk preference: Agent prefers uncertainty (gambling) over certainty</li>
</ul>
</li>
<li><p>Decision Rules for Classification</p>
<ul>
<li><p>Zero-One loss: Penalizes misclassification with a unit cost</p>
<ul>
<li>$l_{01}(y, \hat y) = I{y \ne \hat y}$</li>
<li>Optimal policy minimizes risk by choosing the most probable class:<ul>
<li>$R(y | x) = P(y \ne \hat y | x) = 1 - P(y = \hat y | x)$</li>
<li>$\pi(x) = \arg \max P(y | x)$</li>
</ul>
</li>
</ul>
</li>
<li><p>Cost-Sensitive Classification</p>
<ul>
<li>Different error types have different consequences<ul>
<li>False Positive (Type I error): Predict positive when truth is negative</li>
<li>False Negative (Type II error): Predict negative when truth is positive</li>
</ul>
</li>
<li>Different costs can be assigned: $l_{01} \neq c \times l_{10}$</li>
<li>Choose label 1 if the expected cost is lower:<ul>
<li>$p(y=0|x) \times l_{01} &lt; p(y=1|x) \times c \times l_{10}$</li>
</ul>
</li>
<li>The cost ratio c shifts the decision boundary</li>
</ul>
</li>
<li><p>Rejection Option (Abstention)</p>
<ul>
<li>Sometimes it&#39;s better to abstain from making a decision</li>
<li>Three possible actions: predict 0, predict 1, or reject</li>
<li>Cost parameters:<ul>
<li>$\lambda_e$: Cost of making an error</li>
<li>$\lambda_r$: Cost of rejection/abstention</li>
</ul>
</li>
<li>No decision is made when model confidence is below threshold:<ul>
<li>Abstain when $\max_y P(y|x) &lt; 1 - \frac{\lambda_r}{\lambda_e}$</li>
</ul>
</li>
<li>This creates bands of uncertainty where expert input might be required</li>
</ul>
</li>
</ul>
</li>
<li><p>ROC Curves</p>
<ul>
<li><p>Summarize performance across various thresholds</p>
</li>
<li><p>Confusion Matrix</p>
<ul>
<li>Give a threshold $\tau$</li>
<li>Confusion Matrix
- Positive, Negative: Model Prediction
- True, False: Actual Labels
- TP, TN: Correct Predictions
- FP: Model predicts 1, Ground Truth is 0
- FN: Model predicts 0, Ground Truth is 1<ul>
<li>Ratios from Confusion Matrix<ul>
<li>TPR, Sensitivity, Recall<ul>
<li>TP / (TP + FN)</li>
<li>Accuracy in positive predictions</li>
</ul>
</li>
<li>FPR, Type 1 Error rate<ul>
<li>FP / (FP + TN)</li>
<li>Error in Negative Predictions</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>ROC Curve is a plot between FPR (x-axis) and TPR (y-axis) across various thresholds</p>
</li>
<li><p>AUC is a numerical summary of ROC</p>
</li>
<li><p>Equal Error Rate is where ROC crosses -45 degree line.</p>
</li>
<li><p>ROC Curve is insensitive to class imbalance</p>
<ul>
<li>FPR consists of TN in denominator</li>
<li>If TN &gt;&gt; TP, metric becomes insensitive to FPR</li>
</ul>
</li>
<li><p>Precision-Recall Curves</p>
<ul>
<li>The negatives are not model specific but system specific</li>
<li>For a search query, retrieve 50 vs 500 items.  (or tiles vs list)</li>
<li>Precision<ul>
<li>TP / TP + FP</li>
</ul>
</li>
<li>Recall <ul>
<li>TP / TP + FN</li>
</ul>
</li>
<li>There is no dependency on TN</li>
<li>Precision curve has distortions. Smooth it out by interpolation.</li>
<li>To summarize the performance by a scalar<ul>
<li>Precision @ K</li>
<li>Average Precision: Area under interpolated precision curve</li>
<li>mAP or Mean Average Precision is mean of AP across different PR curves (say different queries)</li>
</ul>
</li>
<li>F-Score<ul>
<li>Weighted harmonic mean between precision and recall</li>
<li>${1 \over F} = {1 \over 1 + \beta^2} {1 \over P} + {\beta^2 \over 1 + \beta^2} {1 \over R}$</li>
<li>Harmonic mean imposes more penalty if either precision or recall fall to a very low level</li>
</ul>
</li>
</ul>
</li>
<li><p>Class Imbalance</p>
<ul>
<li>ROC curves are not sensitive to class imbalance. Does not matter which class is defined as 1 or 0.</li>
<li>PR curves are sensitive to class imbalance. Switching classes impacts performance.<ul>
<li>$P = {TP \over TP + FP}$</li>
</ul>
</li>
<li>PR-AUC is more appropriate in case class imbalance</li>
<li>Multiclass problems can be treated as multiple binary classes. One class vs Rest.</li>
<li>Decision threshold calibration finds the optimal threshold for desired trade-off between precision and recall</li>
</ul>
</li>
</ul>
</li>
<li><p>Regression</p>
<ul>
<li>Metrics<ul>
<li>Mean Squared Error: ${1 \over N} \sum (y - \hat y)^2$</li>
<li>Mean Absolute Error: ${1 \over N} \sum |y - \hat y|$</li>
<li>MAE is robust to outliers - proportional to regression with Laplace conditional distribution</li>
<li>MSE has simple calculus and proportional to regression with Gaussian conditional distribution </li>
<li>Huber loss: MSE below cutoff, MAE above cutoff</li>
<li>Loss functions are usually calibrated with a proper scoring rule</li>
<li>Log score of the predicted density at the true value</li>
</ul>
</li>
<li>Quantile Loss<ul>
<li>Probability that value y is less than q quantile is q</li>
<li>$P(y \le y_q) = q$</li>
<li>Linear function with positive slope for over-prediction and negative slope for under-prediction</li>
<li>Slope depends on the quantile q</li>
</ul>
</li>
</ul>
</li>
<li><p>Calibration</p>
<ul>
<li>Property that predicted certainty of events match the frequency of their occurrence</li>
<li>Reliability Diagrams<ul>
<li>x-asis: Predicted Probability</li>
<li>y-asis: Actual Probability</li>
</ul>
</li>
</ul>
</li>
<li><p>Bayes Model Selection and Averaging</p>
<ul>
<li><p>Bayesian approach to model selection</p>
<ul>
<li>Choose $m \in M$ to maximize posterior $p(m|D)$</li>
<li>$p(m|D) \propto p(D|m)p(m)$</li>
<li>Use uniform prior over model classes</li>
<li>$p(D|m) = \int p(D|\theta,m)p(\theta|m)d\theta$</li>
<li>Integration over all possible parameter values</li>
<li>In practice, models might be compared using BIC or AIC</li>
<li>BIC approximates Bayes factor</li>
<li>AIC approximates Cross-validation</li>
</ul>
</li>
<li><p>Akaike&#39;s Information Criterion</p>
<ul>
<li>Maximizes predictive density of held out data</li>
<li>Approximating out-of-sample generalization</li>
<li>As trained model fits the training data</li>
<li>$AIC = -2LL + 2C$</li>
<li>LL is log-likelihood</li>
<li>C is number of parameters in the model</li>
</ul>
</li>
<li><p>Bayesian Information Criteria</p>
<ul>
<li>Biases in favor of simpler models</li>
<li>$BIC = -2LL + C\log N$</li>
<li>LL is log-likelihood</li>
<li>C is number of parameters in the model</li>
<li>N is the number of observations</li>
</ul>
</li>
<li><p>Minimum Description Length</p>
<ul>
<li>Minimum number of bits needed to describe the model and the data</li>
<li>If we describe model in $L_1$ bits, and then describe the dataset using the model in $L_2$ bits. </li>
<li>The MDL for the model is $L_1 + L_2$</li>
</ul>
</li>
<li><p>Approximation of Bayesian Model Comparison</p>
<ul>
<li>Integrated likelihood is analytically intractable</li>
<li>Approximate the integral by Laplace approximation</li>
<li>Uses 2nd order taylor expansion around MLE / MAP</li>
<li>The model is approximated as gaussian with constant determinant as n increases</li>
<li>AIC is an approximation of KL-Divergence between the the true model and a fitted model</li>
<li>Simple to compute</li>
<li>AIC = $2 \times \text{dof} - 2 \times LL$ (dof: degrees of freedom)</li>
<li>Sample error can result in overfitting</li>
<li>AIC_C: when sample size is small<ul>
<li>AIC_C = AIC + ${2C(C+1) \over N-C-1}$</li>
<li>C is additional penalty for increasing number of parameters</li>
</ul>
</li>
</ul>
</li>
<li><p>Widely Applicable / Watanabe-Akaike Information Criterion </p>
<ul>
<li>Penalizes models based on effective degrees of freedom</li>
<li>$C = 2 \times \text{dof}$</li>
</ul>
</li>
</ul>
</li>
<li><p>Frequentist Decision Theory</p>
<ul>
<li>Risk of an estimator is the expected loss when applying the estimator to data sampled from likelihood function $p( y,x | \theta)$</li>
<li>Bayes Risk<ul>
<li>True generating function unknown</li>
<li>Assume a prior and then average it out</li>
</ul>
</li>
<li>Maximum Risk<ul>
<li>Minimize the maximum risk</li>
</ul>
</li>
<li>Consistent Estimator<ul>
<li>Recovers true parameter in the limit of infinite data</li>
</ul>
</li>
<li>Empirical Risk Minimization<ul>
<li>Population Risk<ul>
<li>Expectation of the loss function w.r.t. true distribution</li>
<li>True distribution is unknown</li>
<li>$R(f, \theta^*) = \mathbf{E}[l(\theta^*, \pi(D))]$</li>
</ul>
</li>
<li>Empirical Risk<ul>
<li>Approximate the expectation of loss by using training data samples</li>
<li>$R(f, D) = \mathbf{E}[l(y, \pi(x))]$</li>
</ul>
</li>
<li>Empirical Risk Minimizaiton<ul>
<li>Optimize empirical risk over hypothesis space of functions</li>
<li>$f_{ERM} = \arg \min_H R(f,D)$</li>
</ul>
</li>
<li>Approximation Error<ul>
<li>Risk that the chosen true parameters don&#39;t lie in the hypothesis space</li>
</ul>
</li>
<li>Estimation Error<ul>
<li>Error due to having finite training set</li>
<li>Difference between training error and test error</li>
<li>Generalization Gap</li>
</ul>
</li>
<li>Regularized Risk<ul>
<li>Add complexity penalty</li>
<li>$R_\lambda(f,D) = R(f,D) + \lambda C(f)$</li>
<li>Complexity term resembles the prior term in MAP estimation</li>
</ul>
</li>
<li>Structural Risk<ul>
<li>Empirical underestimates population risk</li>
<li>Structural risk minimization is to pick the right level of model complexity by minimizing regularized risk and cross-validation</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Statistical Learning Theory</p>
<ul>
<li>Upper bound on generalization error with certain probability</li>
<li>PAC (probably approximately correct) learnable</li>
<li>Hoeffding&#39;s Inequality<ul>
<li>Upper bound on generalization error</li>
</ul>
</li>
<li>VC Dimension<ul>
<li>Measures the degrees of freedom of a hypothesis class</li>
</ul>
</li>
</ul>
</li>
<li><p>Frequentist Hypothesis Testing</p>
<ul>
<li>Null vs Alternate Hypothesis</li>
<li>Likelihood Ratio Test<ul>
<li>$p(D| H_0) / p(D| H_1)$</li>
</ul>
</li>
<li>Null Hypothesis Significance Testing<ul>
<li>Type-1 Error<ul>
<li>P(Reject H0 | H0 is True)</li>
<li>Significance of the test</li>
<li>$\alpha$</li>
</ul>
</li>
<li>Type-2 Error<ul>
<li>P(Accept H0 | H1 is True)</li>
<li>$\beta$</li>
<li>Power of the test is $1 - \beta$</li>
</ul>
</li>
<li>Most powerful test is the one with highest power given a level of significance</li>
<li>Neyman-Pearson lemma: Likelihood ratio test is the most powerful test</li>
<li>p-value<ul>
<li>Probability, under the null hypothesis, of observing a test statistic larger that that actually observed</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

    </main>
    <footer>
      <p>Generated with Markdown Notes Static Site Generator</p>
    </footer>
  </div>
</body>
</html>
  