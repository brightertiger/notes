<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Feed-Forward Neural Networks | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="probml-13-ffnn.html" class="active">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">ProbML</a>
          </div>
          <h1 class="page-title">Feed-Forward Neural Networks</h1>
          <div class="page-meta"><span class="tag">ProbML</span></div>
        </header>
        <article class="content">
          <h1 id="feed-forward-neural-networks">Feed-Forward Neural Networks</h1>
<p>Neural networks are powerful function approximators that learn hierarchical representations. This chapter covers the fundamentals of deep learning.</p>
<h2 id="the-big-picture">The Big Picture</h2>
<p><strong>Key insight</strong>: Compose simple functions to create complex ones.
$$f(x) = f_L(f_{L-1}(...f_2(f_1(x))...))$$</p>
<p>Each layer transforms its input, extracting progressively more abstract features.</p>
<hr>
<h2 id="from-linear-models-to-neural-networks">From Linear Models to Neural Networks</h2>
<h3 id="limitations-of-linear-models">Limitations of Linear Models</h3>
<p>Linear models: $f(x) = Wx + b$</p>
<p><strong>Problem</strong>: Can only represent linear decision boundaries.</p>
<h3 id="feature-engineering">Feature Engineering</h3>
<p>One solution: Transform features first:
$$f(x) = W\phi(x) + b$$</p>
<p>Where $\phi(x)$ are hand-crafted features (polynomials, interactions, etc.).</p>
<p><strong>Problem</strong>: Requires domain expertise; doesn&#39;t scale.</p>
<h3 id="the-neural-network-solution">The Neural Network Solution</h3>
<p><strong>Learn the features automatically!</strong>
$$f(x) = W_L \cdot \sigma(W_{L-1} \cdot \sigma(...\sigma(W_1 x + b_1)...) + b_{L-1}) + b_L$$</p>
<p>Each layer learns a useful transformation.</p>
<hr>
<h2 id="activation-functions">Activation Functions</h2>
<p>Non-linear functions applied after each layer. Without them, the network would collapse to a single linear transformation.</p>
<h3 id="sigmoid">Sigmoid</h3>
<p>$$\sigma(x) = \frac{1}{1 + e^{-x}}$$</p>
<ul>
<li>Output: (0, 1)</li>
<li><strong>Problem</strong>: Vanishing gradients (saturates for large |x|)</li>
<li><strong>Problem</strong>: Not zero-centered</li>
</ul>
<h3 id="tanh">Tanh</h3>
<p>$$\tanh(x) = \frac{e^{2x} - 1}{e^{2x} + 1}$$</p>
<ul>
<li>Output: (-1, 1)</li>
<li>Zero-centered (better than sigmoid)</li>
<li>Still has vanishing gradient problem</li>
</ul>
<h3 id="relu-rectified-linear-unit">ReLU (Rectified Linear Unit)</h3>
<p>$$\text{ReLU}(x) = \max(0, x)$$</p>
<ul>
<li>Output: [0, ‚àû)</li>
<li><strong>Pros</strong>: Non-saturating, computationally efficient, sparse activations</li>
<li><strong>Cons</strong>: &quot;Dead ReLU&quot; ‚Äî neurons can get stuck at 0</li>
</ul>
<h3 id="leaky-relu">Leaky ReLU</h3>
<p>$$\text{LeakyReLU}(x) = \max(\alpha x, x)$$</p>
<ul>
<li>Small slope Œ± (e.g., 0.01) for negative inputs</li>
<li>Prevents dead neurons</li>
<li><strong>Parametric ReLU (PReLU)</strong>: Learn Œ±</li>
</ul>
<h3 id="gelu-gaussian-error-linear-unit">GELU (Gaussian Error Linear Unit)</h3>
<p>$$\text{GELU}(x) = x \cdot \Phi(x)$$</p>
<p>Where Œ¶ is the Gaussian CDF.</p>
<ul>
<li>Smooth approximation of ReLU</li>
<li>Used in transformers (BERT, GPT)</li>
</ul>
<h3 id="swish">Swish</h3>
<p>$$\text{Swish}(x) = x \cdot \sigma(x)$$</p>
<ul>
<li>Self-gated</li>
<li>Works well in deep networks</li>
</ul>
<hr>
<h2 id="the-xor-problem">The XOR Problem</h2>
<p>A classic example showing why we need hidden layers.</p>
<p><strong>XOR truth table</strong>:</p>
<table>
<thead>
<tr>
<th>x‚ÇÅ</th>
<th>x‚ÇÇ</th>
<th>y</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
</tbody></table>
<p><strong>No single line can separate the classes!</strong></p>
<p>With one hidden layer (2 neurons), a neural network can solve XOR by:</p>
<ol>
<li>First layer creates two linear separators</li>
<li>Second layer combines them</li>
</ol>
<hr>
<h2 id="universal-approximation-theorem">Universal Approximation Theorem</h2>
<p><strong>Statement</strong>: A neural network with a single hidden layer of sufficient width can approximate any continuous function on a compact domain to arbitrary precision.</p>
<p><strong>Implication</strong>: Neural networks are extremely powerful function approximators.</p>
<p><strong>In practice</strong>: Deep (many layers) is often better than wide (many neurons per layer):</p>
<ul>
<li>More parameter efficient</li>
<li>Learns hierarchical representations</li>
<li>Better generalization</li>
</ul>
<hr>
<h2 id="backpropagation">Backpropagation</h2>
<p>The algorithm that makes training deep networks possible.</p>
<h3 id="the-chain-rule">The Chain Rule</h3>
<p>For composed functions $f = f_1 \circ f_2 \circ ... \circ f_L$:
$$\frac{\partial L}{\partial \theta_l} = \frac{\partial L}{\partial z_L} \cdot \frac{\partial z_L}{\partial z_{L-1}} \cdot ... \cdot \frac{\partial z_{l+1}}{\partial z_l} \cdot \frac{\partial z_l}{\partial \theta_l}$$</p>
<h3 id="forward-pass">Forward Pass</h3>
<p>Compute activations layer by layer, storing intermediate values.</p>
<h3 id="backward-pass">Backward Pass</h3>
<p>Compute gradients layer by layer, from output to input:
$$\frac{\partial L}{\partial z_l} = \frac{\partial L}{\partial z_{l+1}} \cdot \frac{\partial z_{l+1}}{\partial z_l}$$</p>
<h3 id="automatic-differentiation">Automatic Differentiation</h3>
<p>Modern frameworks (PyTorch, TensorFlow) build a computational graph and compute gradients automatically.</p>
<p><strong>Forward mode</strong>: Efficient when few inputs, many outputs
<strong>Reverse mode (backprop)</strong>: Efficient when many inputs, few outputs (typical in ML)</p>
<h3 id="example-cross-entropy-gradient">Example: Cross-Entropy Gradient</h3>
<p>For softmax + cross-entropy:
$$\frac{\partial L}{\partial a_c} = p_c - y_c$$</p>
<p>Beautifully simple: just the prediction error!</p>
<h3 id="example-relu-gradient">Example: ReLU Gradient</h3>
<p>$$\frac{\partial}{\partial x}\text{ReLU}(x) = \begin{cases} 1 &amp; x &gt; 0 \ 0 &amp; x \leq 0 \end{cases}$$</p>
<p>Gradient flows unchanged through positive regions, is blocked through negative.</p>
<hr>
<h2 id="training-challenges">Training Challenges</h2>
<h3 id="vanishing-gradients">Vanishing Gradients</h3>
<p><strong>Problem</strong>: Gradients shrink exponentially in deep networks.</p>
<p><strong>Cause</strong>: Chained derivatives of saturating activations (sigmoid, tanh).</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Use non-saturating activations (ReLU and variants)</li>
<li>Residual connections</li>
<li>Careful initialization</li>
<li>Batch/layer normalization</li>
</ul>
<h3 id="exploding-gradients">Exploding Gradients</h3>
<p><strong>Problem</strong>: Gradients grow exponentially.</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Gradient clipping: $g \leftarrow \min(1, \frac{\tau}{|g|}) g$</li>
<li>Careful initialization</li>
</ul>
<h3 id="mathematical-perspective">Mathematical Perspective</h3>
<p>Gradient through L layers:
$$\frac{\partial L}{\partial z_1} = \prod_{l=1}^{L-1} J_l \cdot g_L$$</p>
<p>If eigenvalues of Jacobians are:</p>
<ul>
<li>&lt; 1: Gradients vanish</li>
<li><blockquote>
<p>1: Gradients explode</p>
</blockquote>
</li>
</ul>
<hr>
<h2 id="residual-connections">Residual Connections</h2>
<p><strong>Key innovation</strong> (ResNet): Add skip connections.</p>
<p>$$z_{l+1} = z_l + f_l(z_l)$$</p>
<p><strong>Benefits</strong>:</p>
<ul>
<li>Gradients flow directly through skip connection</li>
<li>Learn small perturbations instead of full transformations</li>
<li>Enables training very deep networks (100+ layers)</li>
</ul>
<p><strong>Gradient flow</strong>:
$$\frac{\partial L}{\partial z_l} = \frac{\partial L}{\partial z_L}\left(1 + \frac{\partial}{\partial z_l}\sum_{i=l}^{L-1} f_i(z_i)\right)$$</p>
<p>The &quot;1&quot; term ensures gradients always flow, even if the other term vanishes.</p>
<hr>
<h2 id="initialization">Initialization</h2>
<p>Poor initialization can prevent learning entirely.</p>
<h3 id="the-problem">The Problem</h3>
<p>If weights are too large or too small:</p>
<ul>
<li>Activations explode or vanish</li>
<li>Gradients explode or vanish</li>
</ul>
<h3 id="xavierglorot-initialization">Xavier/Glorot Initialization</h3>
<p>For linear activations:
$$w \sim \mathcal{N}\left(0, \frac{2}{n_{in} + n_{out}}\right)$$</p>
<p><strong>Maintains variance</strong> of activations and gradients across layers.</p>
<h3 id="he-initialization">He Initialization</h3>
<p>For ReLU activations:
$$w \sim \mathcal{N}\left(0, \frac{2}{n_{in}}\right)$$</p>
<p>Accounts for ReLU killing half the activations.</p>
<hr>
<h2 id="regularization">Regularization</h2>
<h3 id="early-stopping">Early Stopping</h3>
<p>Stop training when validation error starts increasing.</p>
<ul>
<li>Implicit regularization</li>
<li>Prevents overfitting</li>
</ul>
<h3 id="weight-decay-l2">Weight Decay (L2)</h3>
<p>Add penalty on weight magnitudes:
$$L = L_{data} + \lambda \sum_l |W_l|^2$$</p>
<p>Equivalent to Gaussian prior on weights (MAP estimation).</p>
<h3 id="dropout">Dropout</h3>
<p>Randomly &quot;drop&quot; neurons during training with probability p.</p>
<p>$$h_i = \begin{cases} 0 &amp; \text{with probability } p \ h_i / (1-p) &amp; \text{otherwise} \end{cases}$$</p>
<p><strong>Interpretation</strong>:</p>
<ul>
<li>Prevents co-adaptation of neurons</li>
<li>Approximate ensemble of subnetworks</li>
<li>At test time: use full network (or Monte Carlo dropout for uncertainty)</li>
</ul>
<h3 id="data-augmentation">Data Augmentation</h3>
<p>Create modified versions of training data:</p>
<ul>
<li>Images: rotations, flips, crops, color jitter</li>
<li>Text: synonym replacement, back-translation</li>
</ul>
<hr>
<h2 id="layer-normalization">Layer Normalization</h2>
<p>Normalize activations to stabilize training:</p>
<p>$$\hat{z} = \frac{z - \mu}{\sqrt{\sigma^2 + \epsilon}}$$
$$\tilde{z} = \gamma \hat{z} + \beta$$</p>
<p>Where Œ≥ and Œ≤ are learnable parameters.</p>
<p><strong>Batch Norm</strong>: Normalize across batch dimension
<strong>Layer Norm</strong>: Normalize across feature dimension (better for RNNs, Transformers)</p>
<hr>
<h2 id="summary">Summary</h2>
<table>
<thead>
<tr>
<th>Component</th>
<th>Purpose</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Layers</strong></td>
<td>Transform representations</td>
</tr>
<tr>
<td><strong>Activations</strong></td>
<td>Add non-linearity</td>
</tr>
<tr>
<td><strong>Backprop</strong></td>
<td>Compute gradients efficiently</td>
</tr>
<tr>
<td><strong>Residual connections</strong></td>
<td>Enable deep networks</td>
</tr>
<tr>
<td><strong>Normalization</strong></td>
<td>Stabilize training</td>
</tr>
<tr>
<td><strong>Dropout</strong></td>
<td>Prevent overfitting</td>
</tr>
<tr>
<td><strong>Initialization</strong></td>
<td>Start training successfully</td>
</tr>
</tbody></table>
<h3 id="practical-recipe">Practical Recipe</h3>
<ol>
<li>Start with standard architecture (ResNet, etc.)</li>
<li>Use ReLU or GELU activations</li>
<li>Xavier/He initialization</li>
<li>Adam optimizer</li>
<li>Batch/Layer normalization</li>
<li>Dropout if overfitting</li>
<li>Data augmentation for images</li>
<li>Early stopping based on validation loss</li>
</ol>

        </article>
        <nav class="page-navigation">
        <a href="probml-11-linear_regression.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Linear Regression</span>
        </a>
        <a href="probml-14-cnn.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Convolutional Neural Networks</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>