<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Probability Foundations | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="probml-02-probability.html" class="active">Probability Foundations</a></li>
            <li class="nav-item"><a href="probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">ProbML</a>
          </div>
          <h1 class="page-title">Probability Foundations</h1>
          <div class="page-meta"><span class="tag">ProbML</span></div>
        </header>
        <article class="content">
          <h1 id="probability-foundations">Probability Foundations</h1>
<p>Probability is the mathematical language of uncertainty. In machine learning, it provides the foundation for reasoning about noisy data, model uncertainty, and making predictions. This chapter covers the essential probability concepts you&#39;ll use throughout ML.</p>
<h2 id="two-views-of-probability">Two Views of Probability</h2>
<h3 id="frequentist-view">Frequentist View</h3>
<p>Probability is the <strong>long-run relative frequency</strong> of an event in repeated experiments.</p>
<p><strong>Example</strong>: The probability of heads is 0.5 because if you flip a coin many times, about half will be heads.</p>
<p><strong>Limitation</strong>: What about events that can&#39;t be repeated? (e.g., &quot;probability it rains tomorrow&quot;)</p>
<h3 id="bayesian-view">Bayesian View</h3>
<p>Probability is a <strong>quantification of subjective uncertainty</strong> or degree of belief.</p>
<p><strong>Example</strong>: &quot;I&#39;m 70% confident it will rain tomorrow&quot; represents my current belief given available evidence.</p>
<p><strong>Advantage</strong>: Can express uncertainty about any proposition, including model parameters.</p>
<h3 id="two-types-of-uncertainty">Two Types of Uncertainty</h3>
<p><strong>Epistemic Uncertainty (Model Uncertainty)</strong></p>
<ul>
<li>Uncertainty due to lack of knowledge</li>
<li>Can be reduced with more data</li>
<li>Example: Uncertainty about which model is correct</li>
</ul>
<p><strong>Aleatoric Uncertainty (Data Uncertainty)</strong></p>
<ul>
<li>Uncertainty due to inherent randomness</li>
<li>Cannot be reduced even with infinite data</li>
<li>Example: Outcome of a fair coin flip</li>
</ul>
<hr>
<h2 id="basic-probability-rules">Basic Probability Rules</h2>
<h3 id="events-and-probabilities">Events and Probabilities</h3>
<p>An <strong>event</strong> A is some state of the world that either holds or doesn&#39;t.</p>
<p><strong>Probability axioms</strong>:</p>
<ul>
<li>$0 \leq P(A) \leq 1$ (probabilities are between 0 and 1)</li>
<li>$P(A) + P(\bar{A}) = 1$ (something happens or it doesn&#39;t)</li>
<li>$P(\Omega) = 1$ (something must happen)</li>
</ul>
<h3 id="joint-probability">Joint Probability</h3>
<p>The probability that <strong>both</strong> A and B occur:
$$P(A, B) = P(A \cap B)$$</p>
<p><strong>Special case ‚Äî Independence</strong>: If A and B are independent:
$$P(A, B) = P(A) \cdot P(B)$$</p>
<p><strong>Inclusion-Exclusion Principle</strong>:
$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$</p>
<h3 id="conditional-probability">Conditional Probability</h3>
<p>The probability of B <strong>given that</strong> A has occurred:
$$P(B | A) = \frac{P(A \cap B)}{P(A)} \quad \text{where } P(A) &gt; 0$$</p>
<p><strong>Intuition</strong>: We restrict our attention to the &quot;world where A happened&quot; and ask how likely B is in that world.</p>
<p><strong>Example</strong>: P(has cancer | positive test) ‚â† P(positive test | has cancer)</p>
<p>This is a common source of confusion called the <strong>base rate fallacy</strong>!</p>
<hr>
<h2 id="random-variables">Random Variables</h2>
<p>A <strong>random variable</strong> is a function that maps outcomes from a sample space to real numbers. This allows mathematical manipulation of random events.</p>
<h3 id="discrete-random-variables">Discrete Random Variables</h3>
<p>Take values from a countable set (integers, categories).</p>
<ul>
<li>Example: Number of customers, dice roll, coin flip</li>
</ul>
<h3 id="continuous-random-variables">Continuous Random Variables</h3>
<p>Take values from an uncountable set (real numbers, intervals).</p>
<ul>
<li>Example: Height, temperature, time</li>
</ul>
<hr>
<h2 id="probability-distributions">Probability Distributions</h2>
<h3 id="probability-mass-function-pmf">Probability Mass Function (PMF)</h3>
<p>For discrete random variables, the PMF gives the probability of each value:
$$p(x) = P(X = x)$$</p>
<p><strong>Properties</strong>:</p>
<ul>
<li>$0 \leq p(x) \leq 1$ for all x</li>
<li>$\sum_x p(x) = 1$ (probabilities sum to 1)</li>
</ul>
<h3 id="probability-density-function-pdf">Probability Density Function (PDF)</h3>
<p>For continuous random variables, the PDF describes relative likelihood:
$$P(a \leq X \leq b) = \int_a^b f(x) dx$$</p>
<p><strong>Important</strong>: For continuous variables, $P(X = x) = 0$ for any specific x!</p>
<p><strong>Properties</strong>:</p>
<ul>
<li>$f(x) \geq 0$ (but can be greater than 1!)</li>
<li>$\int_{-\infty}^{\infty} f(x) dx = 1$</li>
</ul>
<h3 id="cumulative-distribution-function-cdf">Cumulative Distribution Function (CDF)</h3>
<p>The probability that X is less than or equal to x:
$$F_X(x) = P(X \leq x)$$</p>
<p><strong>Properties</strong>:</p>
<ul>
<li>Monotonically non-decreasing</li>
<li>$\lim_{x \to -\infty} F_X(x) = 0$</li>
<li>$\lim_{x \to \infty} F_X(x) = 1$</li>
<li>$P(a \leq X \leq b) = F_X(b) - F_X(a)$</li>
</ul>
<p><strong>Relationship</strong>: PDF is the derivative of CDF.</p>
<h3 id="inverse-cdf-quantile-function">Inverse CDF (Quantile Function)</h3>
<p>Given a probability, find the value:
$$F^{-1}(q) = \inf{x : F(x) \geq q}$$</p>
<p><strong>Common quantiles</strong>:</p>
<ul>
<li>$F^{-1}(0.5)$ = median</li>
<li>$F^{-1}(0.25)$, $F^{-1}(0.75)$ = lower and upper quartiles</li>
</ul>
<hr>
<h2 id="working-with-multiple-variables">Working with Multiple Variables</h2>
<h3 id="marginal-distribution">Marginal Distribution</h3>
<p>Given joint distribution $p(X, Y)$, the marginal distribution of X:
$$p(X = x) = \sum_y p(X = x, Y = y)$$</p>
<p><strong>Intuition</strong>: &quot;Sum out&quot; the variable you don&#39;t care about.</p>
<h3 id="conditional-distribution">Conditional Distribution</h3>
<p>$$p(Y = y | X = x) = \frac{p(X = x, Y = y)}{p(X = x)}$$</p>
<h3 id="product-rule">Product Rule</h3>
<p>$$p(X, Y) = p(Y | X) \cdot p(X) = p(X | Y) \cdot p(Y)$$</p>
<h3 id="chain-rule">Chain Rule</h3>
<p>For multiple variables:
$$p(X_1, X_2, X_3) = p(X_1) \cdot p(X_2 | X_1) \cdot p(X_3 | X_1, X_2)$$</p>
<h3 id="independence">Independence</h3>
<p>X and Y are <strong>independent</strong> if:
$$X \perp Y \Leftrightarrow p(X, Y) = p(X) \cdot p(Y)$$</p>
<p><strong>Equivalently</strong>: Knowing X tells you nothing about Y.</p>
<h3 id="conditional-independence">Conditional Independence</h3>
<p>X and Y are <strong>conditionally independent</strong> given Z if:
$$X \perp Y | Z \Leftrightarrow p(X, Y | Z) = p(X | Z) \cdot p(Y | Z)$$</p>
<p><strong>Example</strong>: Given the weather, whether I carry an umbrella is independent of whether you carry one. But marginally (without knowing the weather), they&#39;re correlated!</p>
<hr>
<h2 id="summary-statistics">Summary Statistics</h2>
<h3 id="expected-value-mean">Expected Value (Mean)</h3>
<p>The &quot;center&quot; of a distribution ‚Äî the average value weighted by probability:
$$\mathbb{E}[X] = \sum_x x \cdot p(x) \quad \text{or} \quad \int_{-\infty}^{\infty} x \cdot f(x) dx$$</p>
<p><strong>Linearity of Expectation</strong> (extremely useful!):
$$\mathbb{E}[aX + b] = a\mathbb{E}[X] + b$$
$$\mathbb{E}[X + Y] = \mathbb{E}[X] + \mathbb{E}[Y] \quad \text{(always, even if not independent!)}$$</p>
<h3 id="variance">Variance</h3>
<p>How spread out the distribution is:
$$\text{Var}(X) = \mathbb{E}[(X - \mu)^2] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$$</p>
<p><strong>Properties</strong>:</p>
<ul>
<li>$\text{Var}(aX + b) = a^2 \text{Var}(X)$</li>
<li>$\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) + 2\text{Cov}(X, Y)$</li>
</ul>
<h3 id="mode">Mode</h3>
<p>The most probable value:
$$\text{mode} = \arg\max_x p(x)$$</p>
<hr>
<h2 id="laws-of-iterated-expectations">Laws of Iterated Expectations</h2>
<h3 id="law-of-total-expectation">Law of Total Expectation</h3>
<p>$$\mathbb{E}[X] = \mathbb{E}[\mathbb{E}[X | Y]]$$</p>
<p><strong>Intuition</strong>: The overall average equals the average of conditional averages.</p>
<h3 id="law-of-total-variance">Law of Total Variance</h3>
<p>$$\text{Var}(X) = \mathbb{E}[\text{Var}(X | Y)] + \text{Var}(\mathbb{E}[X | Y])$$</p>
<p><strong>Interpretation</strong>:</p>
<ul>
<li>First term: Average variance within groups</li>
<li>Second term: Variance between group means</li>
</ul>
<hr>
<h2 id="bayes-rule">Bayes&#39; Rule</h2>
<p>The foundation of Bayesian inference:
$$P(H | Y) = \frac{P(Y | H) \cdot P(H)}{P(Y)}$$</p>
<p><strong>Components</strong>:</p>
<ul>
<li>$P(H)$: <strong>Prior</strong> ‚Äî belief before seeing data</li>
<li>$P(Y | H)$: <strong>Likelihood</strong> ‚Äî probability of data given hypothesis</li>
<li>$P(H | Y)$: <strong>Posterior</strong> ‚Äî updated belief after seeing data</li>
<li>$P(Y)$: <strong>Evidence</strong> ‚Äî normalizing constant</li>
</ul>
<p><strong>The Bayesian Recipe</strong>:
$$\text{Posterior} \propto \text{Prior} \times \text{Likelihood}$$</p>
<hr>
<h2 id="common-distributions">Common Distributions</h2>
<h3 id="bernoulli-distribution">Bernoulli Distribution</h3>
<p>Models a single binary outcome:
$$Y \sim \text{Ber}(\theta)$$
$$p(Y = y) = \theta^y (1 - \theta)^{1-y} \quad \text{for } y \in {0, 1}$$</p>
<ul>
<li>Mean: $\theta$</li>
<li>Variance: $\theta(1 - \theta)$</li>
</ul>
<h3 id="binomial-distribution">Binomial Distribution</h3>
<p>Models number of successes in N independent Bernoulli trials:
$$p(k | N, \theta) = \binom{N}{k} \theta^k (1 - \theta)^{N-k}$$</p>
<ul>
<li>Mean: $N\theta$</li>
<li>Variance: $N\theta(1 - \theta)$</li>
</ul>
<h3 id="logistic-function">Logistic Function</h3>
<p>The <strong>sigmoid</strong> function maps any real number to (0, 1):
$$\sigma(a) = \frac{1}{1 + e^{-a}}$$</p>
<p><strong>Properties</strong>:</p>
<ul>
<li>$\sigma(-a) = 1 - \sigma(a)$</li>
<li>$\sigma&#39;(a) = \sigma(a)(1 - \sigma(a))$</li>
</ul>
<p><strong>Log-odds (logit)</strong>: The inverse transformation:
$$a = \log\frac{p}{1-p}$$</p>
<p><strong>Usage</strong>: Binary classification ‚Äî map raw scores to probabilities:
$$p(y = 1 | x, \theta) = \sigma(f(x, \theta))$$</p>
<h3 id="categorical-distribution">Categorical Distribution</h3>
<p>Generalizes Bernoulli to multiple categories:
$$\text{Cat}(y | \theta) = \prod_{c=1}^C \theta_c^{I(y=c)}$$</p>
<p><strong>Constraints</strong>:</p>
<ul>
<li>$0 \leq \theta_c \leq 1$</li>
<li>$\sum_c \theta_c = 1$</li>
</ul>
<h3 id="softmax-function">Softmax Function</h3>
<p>Maps raw logits to valid categorical probabilities:
$$\text{softmax}(a)<em>c = \frac{e^{a_c}}{\sum</em>{j=1}^C e^{a_j}}$$</p>
<p><strong>Properties</strong>:</p>
<ul>
<li>Output sums to 1</li>
<li>Each output is in (0, 1)</li>
<li>Invariant to adding constant to all inputs</li>
</ul>
<p><strong>Temperature Scaling</strong>: Divide by temperature T before softmax:</p>
<ul>
<li>T ‚Üí 0: &quot;Winner takes all&quot; (approaches argmax)</li>
<li>T ‚Üí ‚àû: Approaches uniform distribution</li>
</ul>
<h3 id="log-sum-exp-trick">Log-Sum-Exp Trick</h3>
<p>For numerical stability when computing softmax:
$$\log \sum_c e^{a_c} = m + \log \sum_c e^{a_c - m}$$</p>
<p>where $m = \max_c a_c$. This prevents overflow!</p>
<h3 id="gaussian-normal-distribution">Gaussian (Normal) Distribution</h3>
<p>The workhorse of statistics:
$$\mathcal{N}(x | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)$$</p>
<p><strong>Properties</strong>:</p>
<ul>
<li>Mean, median, and mode all equal Œº</li>
<li>Symmetric around the mean</li>
<li>68-95-99.7 rule: ~68% within 1œÉ, ~95% within 2œÉ, ~99.7% within 3œÉ</li>
</ul>
<p><strong>Why Gaussian is so common</strong>:</p>
<ol>
<li>Maximum entropy distribution for given mean and variance</li>
<li>Central Limit Theorem: Sum of many independent RVs ‚Üí Gaussian</li>
<li>Mathematical convenience (conjugate to itself)</li>
</ol>
<h3 id="students-t-distribution">Student&#39;s t-Distribution</h3>
<p>More robust alternative to Gaussian (heavier tails):</p>
<ul>
<li>PDF decays polynomially, not exponentially</li>
<li>Parameter ŒΩ (degrees of freedom) controls tail weight</li>
<li>As ŒΩ ‚Üí ‚àû, approaches Gaussian</li>
<li>Better for handling outliers</li>
</ul>
<hr>
<h2 id="transformations-of-random-variables">Transformations of Random Variables</h2>
<h3 id="discrete-case">Discrete Case</h3>
<p>If Y = f(X), the PMF of Y:
$$p_Y(y) = \sum_{x : f(x) = y} p_X(x)$$</p>
<h3 id="continuous-case-change-of-variables">Continuous Case (Change of Variables)</h3>
<p>If Y = f(X) where f is monotonic and differentiable:
$$p_Y(y) = p_X(f^{-1}(y)) \cdot \left|\frac{d f^{-1}(y)}{dy}\right|$$</p>
<p>The absolute value of the derivative (Jacobian in multivariate case) accounts for how the transformation stretches or compresses probability.</p>
<h3 id="convolution">Convolution</h3>
<p>For Y = X‚ÇÅ + X‚ÇÇ (sum of independent RVs):
$$p_Y(y) = \int p_{X_1}(x_1) \cdot p_{X_2}(y - x_1) dx_1$$</p>
<p><strong>Key result</strong>: Sum of Gaussians is Gaussian!</p>
<hr>
<h2 id="central-limit-theorem">Central Limit Theorem</h2>
<p>One of the most important theorems in statistics:</p>
<p>If $X_1, X_2, ..., X_N$ are i.i.d. with mean Œº and variance œÉ¬≤, then as N ‚Üí ‚àû:
$$\bar{X} = \frac{1}{N}\sum_{i=1}^N X_i \xrightarrow{d} \mathcal{N}\left(\mu, \frac{\sigma^2}{N}\right)$$</p>
<p><strong>Implications</strong>:</p>
<ul>
<li>Sample means are approximately Gaussian (for large N)</li>
<li>Justifies using Gaussian assumptions in many settings</li>
<li>Variance of sample mean decreases as 1/N</li>
</ul>
<hr>
<h2 id="monte-carlo-approximation">Monte Carlo Approximation</h2>
<p>When exact computation is intractable, <strong>sample</strong>!</p>
<p>To estimate $\mathbb{E}[f(X)]$ where $X \sim p(x)$:</p>
<ol>
<li>Draw samples $x_1, x_2, ..., x_N \sim p(x)$</li>
<li>Approximate: $\mathbb{E}[f(X)] \approx \frac{1}{N}\sum_{i=1}^N f(x_i)$</li>
</ol>
<p>This is unbiased and converges by the Law of Large Numbers.</p>
<hr>
<h2 id="summary">Summary</h2>
<table>
<thead>
<tr>
<th>Concept</th>
<th>Key Formula</th>
<th>Intuition</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Conditional Probability</strong></td>
<td>$P(B|A) = P(A,B)/P(A)$</td>
<td>Probability in restricted world</td>
</tr>
<tr>
<td><strong>Bayes&#39; Rule</strong></td>
<td>$P(H|Y) \propto P(Y|H)P(H)$</td>
<td>Update beliefs with evidence</td>
</tr>
<tr>
<td><strong>Expected Value</strong></td>
<td>$\mathbb{E}[X] = \sum x \cdot p(x)$</td>
<td>Probability-weighted average</td>
</tr>
<tr>
<td><strong>Variance</strong></td>
<td>$\text{Var}(X) = \mathbb{E}[(X-\mu)^2]$</td>
<td>Spread of distribution</td>
</tr>
<tr>
<td><strong>Sigmoid</strong></td>
<td>$\sigma(a) = 1/(1+e^{-a})$</td>
<td>Map reals to (0,1)</td>
</tr>
<tr>
<td><strong>Softmax</strong></td>
<td>$e^{a_c}/\sum e^{a_j}$</td>
<td>Map vector to probabilities</td>
</tr>
<tr>
<td><strong>CLT</strong></td>
<td>$\bar{X} \to \mathcal{N}(\mu, \sigma^2/N)$</td>
<td>Sample means are Gaussian</td>
</tr>
</tbody></table>

        </article>
        <nav class="page-navigation">
        <a href="probml-01-introduction.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Introduction to Machine Learning</span>
        </a>
        <a href="probml-03-probability.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Probability: Advanced Topics</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>