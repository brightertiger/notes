<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Self-Supervised and Semi-Supervised Learning | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="probml-19-ssl.html" class="active">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">ProbML</a>
          </div>
          <h1 class="page-title">Self-Supervised and Semi-Supervised Learning</h1>
          <div class="page-meta"><span class="tag">ProbML</span></div>
        </header>
        <article class="content">
          <h1 id="self-supervised-and-semi-supervised-learning">Self-Supervised and Semi-Supervised Learning</h1>
<p>These techniques leverage unlabeled data to improve learning. In a world where labels are expensive but data is abundant, these methods are increasingly important.</p>
<h2 id="the-big-picture">The Big Picture</h2>
<p><strong>The label bottleneck</strong>: Labeling data is expensive and time-consuming.</p>
<p><strong>The opportunity</strong>: Vast amounts of unlabeled data are available.</p>
<p><strong>The goal</strong>: Learn useful representations from unlabeled data that transfer to downstream tasks.</p>
<hr>
<h2 id="data-augmentation">Data Augmentation</h2>
<h3 id="the-idea">The Idea</h3>
<p>Create modified versions of training examples that preserve the label.</p>
<p><strong>Effect</strong>: Expands training set, improves robustness.</p>
<h3 id="common-augmentations">Common Augmentations</h3>
<p><strong>Images</strong>:</p>
<ul>
<li>Rotation, flipping, cropping</li>
<li>Color jitter, brightness changes</li>
<li>Random erasing, cutout</li>
</ul>
<p><strong>Text</strong>:</p>
<ul>
<li>Synonym replacement</li>
<li>Back-translation</li>
<li>Random insertion/deletion</li>
</ul>
<p><strong>Audio</strong>:</p>
<ul>
<li>Pitch shifting</li>
<li>Time stretching</li>
<li>Adding noise</li>
</ul>
<h3 id="theoretical-view-vicinal-risk-minimization">Theoretical View: Vicinal Risk Minimization</h3>
<p>Instead of minimizing risk at exact training points, minimize in a <strong>vicinity</strong> around them:</p>
<p>$$R = \int L(y, f(x&#39;)) p(x&#39; | x) dx&#39;$$</p>
<p>Where p(x&#39;|x) is the augmentation distribution.</p>
<hr>
<h2 id="transfer-learning">Transfer Learning</h2>
<h3 id="the-problem">The Problem</h3>
<p>Task A has lots of data; Task B has little data.</p>
<h3 id="the-solution">The Solution</h3>
<ol>
<li><strong>Pretrain</strong> on large source dataset (Task A)</li>
<li><strong>Fine-tune</strong> on small target dataset (Task B)</li>
</ol>
<h3 id="fine-tuning-strategies">Fine-tuning Strategies</h3>
<p><strong>Feature extraction</strong>: Freeze pretrained layers, train only new head.</p>
<ul>
<li>Best when: Very small target data, similar domains</li>
</ul>
<p><strong>Full fine-tuning</strong>: Update all parameters.</p>
<ul>
<li>Best when: More target data, different domains</li>
</ul>
<p><strong>Gradual unfreezing</strong>: Start from top layers, progressively unfreeze.</p>
<ul>
<li>Often best practice</li>
</ul>
<h3 id="parameter-efficient-fine-tuning">Parameter-Efficient Fine-tuning</h3>
<p>For large models, updating all parameters is expensive.</p>
<p><strong>Adapters</strong>: Small bottleneck layers inserted between frozen transformer blocks.</p>
<p><strong>LoRA</strong>: Low-rank updates to weight matrices.</p>
<p><strong>Prompt tuning</strong>: Learn soft prompts while keeping model frozen.</p>
<hr>
<h2 id="self-supervised-learning">Self-Supervised Learning</h2>
<h3 id="the-core-idea">The Core Idea</h3>
<p>Create supervisory signals from the data itself ‚Äî no human labels needed.</p>
<h3 id="pretext-tasks">Pretext Tasks</h3>
<p><strong>Reconstruction</strong>: Predict masked or corrupted parts</p>
<ul>
<li>Autoencoders</li>
<li>Masked language modeling (BERT)</li>
<li>Masked image modeling (MAE)</li>
</ul>
<p><strong>Contrastive</strong>: Learn to distinguish similar from dissimilar</p>
<ul>
<li>Positive pairs: Same image, different augmentations</li>
<li>Negative pairs: Different images</li>
</ul>
<p><strong>Predictive</strong>: Predict properties of the data</p>
<ul>
<li>Rotation prediction (images)</li>
<li>Next word prediction (language)</li>
</ul>
<hr>
<h2 id="contrastive-learning">Contrastive Learning</h2>
<h3 id="the-framework">The Framework</h3>
<ol>
<li>Create two views of same example (via augmentation)</li>
<li>Push their representations together</li>
<li>Push representations of different examples apart</li>
</ol>
<h3 id="simclr-simple-contrastive-learning">SimCLR (Simple Contrastive Learning)</h3>
<p><strong>Pretraining</strong>:</p>
<ol>
<li>Take image x</li>
<li>Apply two augmentations: $x_1, x_2$</li>
<li>Encode both: $z_1 = g(f(x_1)), z_2 = g(f(x_2))$</li>
<li>Contrastive loss (NT-Xent):
$$L = -\log \frac{\exp(\text{sim}(z_1, z_2)/\tau)}{\sum_{k \neq i} \exp(\text{sim}(z_1, z_k)/\tau)}$$</li>
</ol>
<p><strong>Fine-tuning</strong>: Discard projection head g, fine-tune encoder f.</p>
<h3 id="key-insights">Key Insights</h3>
<ul>
<li><strong>Projection head</strong> is crucial during pretraining but discarded after</li>
<li><strong>Large batch sizes</strong> help (more negatives)</li>
<li><strong>Strong augmentation</strong> is important</li>
</ul>
<h3 id="challenges">Challenges</h3>
<ul>
<li>Need many negative examples</li>
<li>Batch size dependence</li>
<li>Risk of feature collapse</li>
</ul>
<hr>
<h2 id="non-contrastive-methods">Non-Contrastive Methods</h2>
<h3 id="byol-bootstrap-your-own-latent">BYOL (Bootstrap Your Own Latent)</h3>
<p>No negative examples needed!</p>
<p><strong>Architecture</strong>:</p>
<ul>
<li><strong>Online network</strong> (student): Updated by gradient descent</li>
<li><strong>Target network</strong> (teacher): Exponential moving average of online</li>
</ul>
<p><strong>Loss</strong>: Predict target representation from online prediction.</p>
<p><strong>Why no collapse?</strong> Asymmetric architecture + momentum updates prevent trivial solutions.</p>
<h3 id="masked-autoencoders-mae">Masked Autoencoders (MAE)</h3>
<p>Inspired by BERT&#39;s success:</p>
<ol>
<li>Mask large portion of image (75%!)</li>
<li>Encode visible patches</li>
<li>Decode to reconstruct masked patches</li>
<li>For downstream: Use only encoder</li>
</ol>
<p><strong>Why mask so much?</strong> Forces learning high-level features, not just copying.</p>
<hr>
<h2 id="semi-supervised-learning">Semi-Supervised Learning</h2>
<p>Use both labeled and unlabeled data together.</p>
<h3 id="self-training">Self-Training</h3>
<ol>
<li>Train on labeled data</li>
<li>Predict on unlabeled data (pseudo-labels)</li>
<li>Add confident predictions to training set</li>
<li>Repeat</li>
</ol>
<p><strong>Connection to EM</strong>: Pseudo-labels are the E-step!</p>
<h3 id="noise-student-training">Noise Student Training</h3>
<p>Self-training + noise:</p>
<ol>
<li>Train teacher on labeled data</li>
<li>Generate pseudo-labels for unlabeled data</li>
<li>Train student on all data with noise (dropout, augmentation)</li>
<li>Student becomes new teacher; repeat</li>
</ol>
<p><strong>Key insight</strong>: Noise makes student more robust than teacher.</p>
<h3 id="consistency-regularization">Consistency Regularization</h3>
<p>Model predictions should be consistent under small input changes:
$$L = L_{supervised} + \lambda \cdot d(f(x), f(\text{aug}(x)))$$</p>
<p><strong>FixMatch</strong>: Combine pseudo-labeling with consistency.</p>
<hr>
<h2 id="label-propagation">Label Propagation</h2>
<h3 id="graph-based-approach">Graph-Based Approach</h3>
<ol>
<li>Build graph: Nodes = data points, edges = similarity</li>
<li>Propagate labels from labeled to unlabeled nodes</li>
<li>Use resulting labels for training</li>
</ol>
<h3 id="algorithm">Algorithm</h3>
<ul>
<li>T: Transition matrix (normalized edge weights)</li>
<li>Y: Label matrix (N √ó C)</li>
<li>Iterate: $Y = TY$ until convergence</li>
<li>Use propagated labels for supervised learning</li>
</ul>
<h3 id="assumptions">Assumptions</h3>
<ul>
<li>Similar points should have similar labels</li>
<li>Cluster structure in data reflects label structure</li>
</ul>
<hr>
<h2 id="generative-self-supervised-learning">Generative Self-Supervised Learning</h2>
<h3 id="variational-autoencoders-vae">Variational Autoencoders (VAE)</h3>
<p><strong>Generative model</strong>:</p>
<ol>
<li>Sample latent: $z \sim p(z)$</li>
<li>Generate: $x \sim p(x|z)$</li>
</ol>
<p><strong>Training</strong>: Maximize ELBO (Evidence Lower Bound)
$$\log p(x) \geq \mathbb{E}<em>{q(z|x)}[\log p(x|z)] - D</em>{KL}(q(z|x) | p(z))$$</p>
<p><strong>Use for SSL</strong>: Learn representations z for downstream tasks.</p>
<h3 id="gans">GANs</h3>
<p><strong>Generator</strong>: Maps noise to data.
<strong>Discriminator</strong>: Distinguishes real from fake.</p>
<p><strong>Semi-supervised extension</strong>: Discriminator predicts K classes + &quot;fake&quot;.</p>
<hr>
<h2 id="active-learning">Active Learning</h2>
<h3 id="the-idea-1">The Idea</h3>
<p>If we must label data, label the most informative examples.</p>
<h3 id="strategies">Strategies</h3>
<p><strong>Uncertainty sampling</strong>: Label examples where model is least confident.
$$x^* = \arg\max_x H[p(y|x)]$$</p>
<p><strong>BALD</strong> (Bayesian Active Learning by Disagreement): Label where model&#39;s predictions are most diverse (across ensemble/dropout samples).</p>
<p><strong>Query by committee</strong>: Multiple models vote; label where they disagree.</p>
<hr>
<h2 id="few-shot-learning">Few-Shot Learning</h2>
<h3 id="the-challenge">The Challenge</h3>
<p>Learn to classify new classes from very few examples (1-5 per class).</p>
<h3 id="meta-learning-approach">Meta-Learning Approach</h3>
<p>Train model to learn quickly:</p>
<ul>
<li><strong>Training</strong>: Many &quot;episodes&quot; with different class subsets</li>
<li><strong>Testing</strong>: New classes, few examples each</li>
</ul>
<h3 id="metric-learning-approach">Metric Learning Approach</h3>
<p>Learn embedding space where similarity = class membership.</p>
<ul>
<li>Prototypical networks: Classify by nearest class prototype</li>
<li>Matching networks: Weighted nearest neighbor</li>
</ul>
<hr>
<h2 id="weak-supervision">Weak Supervision</h2>
<h3 id="when-labels-are-imperfect">When Labels Are Imperfect</h3>
<ul>
<li>Noisy labels (some wrong)</li>
<li>Soft labels (probability distributions)</li>
<li>Aggregate from multiple labelers</li>
</ul>
<h3 id="label-smoothing">Label Smoothing</h3>
<p>Instead of hard labels [0, 1, 0]:
$$y_{smooth} = (1 - \epsilon) \cdot y + \epsilon / K$$</p>
<p>Prevents overconfidence, improves calibration.</p>
<hr>
<h2 id="summary">Summary</h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Uses Unlabeled Data</th>
<th>Key Idea</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Data Augmentation</strong></td>
<td>No (extends labeled)</td>
<td>Transform while preserving label</td>
</tr>
<tr>
<td><strong>Transfer Learning</strong></td>
<td>Pre-training stage</td>
<td>Leverage large datasets</td>
</tr>
<tr>
<td><strong>Contrastive Learning</strong></td>
<td>Yes</td>
<td>Pull similar, push dissimilar</td>
</tr>
<tr>
<td><strong>Non-Contrastive</strong></td>
<td>Yes</td>
<td>Predict across views (no negatives)</td>
</tr>
<tr>
<td><strong>Semi-Supervised</strong></td>
<td>Yes (with some labels)</td>
<td>Pseudo-labels + consistency</td>
</tr>
<tr>
<td><strong>Active Learning</strong></td>
<td>Selects what to label</td>
<td>Query most informative</td>
</tr>
</tbody></table>
<h3 id="practical-recommendations">Practical Recommendations</h3>
<ol>
<li><strong>Always use data augmentation</strong> ‚Äî almost free improvement</li>
<li><strong>Start with pretrained models</strong> ‚Äî rarely worth training from scratch</li>
<li><strong>Try self-training</strong> for semi-supervised (simple and effective)</li>
<li><strong>Large-scale pretraining</strong> for best representations (if compute allows)</li>
<li><strong>Match pretraining and downstream domains</strong> for best transfer</li>
</ol>

        </article>
        <nav class="page-navigation">
        <a href="probml-18-trees.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Decision Trees and Ensembles</span>
        </a>
        <a href="probml-21-recsys.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Recommendation Systems</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>