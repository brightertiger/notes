<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Decision Trees and Ensembles | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="probml-18-trees.html" class="active">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">ProbML</a>
          </div>
          <h1 class="page-title">Decision Trees and Ensembles</h1>
          <div class="page-meta"><span class="tag">ProbML</span></div>
        </header>
        <article class="content">
          <h1 id="decision-trees-and-ensembles">Decision Trees and Ensembles</h1>
<p>Decision trees are intuitive models that partition the feature space into regions. While single trees are prone to overfitting, ensemble methods (Random Forests, Boosting) combine many trees for powerful, robust predictions.</p>
<h2 id="the-big-picture">The Big Picture</h2>
<p><strong>Trees</strong>: Recursively partition space with simple rules.</p>
<ul>
<li>Highly interpretable</li>
<li>But high variance (unstable)</li>
</ul>
<p><strong>Ensembles</strong>: Combine many trees.</p>
<ul>
<li>Random Forests: Reduce variance through averaging</li>
<li>Boosting: Reduce bias through sequential correction</li>
</ul>
<hr>
<h2 id="decision-tree-structure">Decision Tree Structure</h2>
<h3 id="the-model">The Model</h3>
<p>$$f(x) = \sum_{j=1}^{J} w_j \cdot I{x \in R_j}$$</p>
<p>Where:</p>
<ul>
<li>$R_j$ are disjoint regions (leaves)</li>
<li>$w_j$ is the prediction for region j</li>
<li>I{} is indicator function</li>
</ul>
<h3 id="building-a-tree">Building a Tree</h3>
<p>At each node i:</p>
<ol>
<li>Select a feature $d_i$</li>
<li>Select a threshold $t_i$</li>
<li>Split: left if $x_{d_i} \leq t_i$, right otherwise</li>
</ol>
<p><strong>Result</strong>: Axis-parallel partitions of the feature space.</p>
<h3 id="leaf-predictions">Leaf Predictions</h3>
<p><strong>Regression</strong>: Average of training labels in region
$$w_j = \frac{\sum_{n: x_n \in R_j} y_n}{\sum_{n: x_n \in R_j} 1}$$</p>
<p><strong>Classification</strong>: Majority vote or probability distribution</p>
<hr>
<h2 id="finding-optimal-splits">Finding Optimal Splits</h2>
<h3 id="the-greedy-algorithm">The Greedy Algorithm</h3>
<p>Tree optimization is NP-hard. We use a greedy approach:</p>
<p>At each node, find the best split by minimizing:
$$L = \frac{|D_L|}{|D|} C_L + \frac{|D_R|}{|D|} C_R$$</p>
<p>Where C is the cost (impurity) and D is the data reaching that node.</p>
<h3 id="splitting-criteria">Splitting Criteria</h3>
<p><strong>For Regression (MSE)</strong>:
$$C = \frac{1}{|D|}\sum_{i \in D}(y_i - \bar{y})^2$$</p>
<p><strong>For Classification</strong>:</p>
<p><em>Gini Index</em>:
$$C = \sum_{c=1}^C \hat{p}_c(1 - \hat{p}_c)$$
Probability of misclassifying a randomly chosen element.</p>
<p><em>Cross-Entropy</em>:
$$C = -\sum_{c=1}^C \hat{p}_c \log \hat{p}_c$$
Information-theoretic measure of impurity.</p>
<h3 id="why-binary-splits">Why Binary Splits?</h3>
<ul>
<li>More splits = more data fragmentation</li>
<li>Binary splits are sufficient (can always split further)</li>
<li>Simpler to optimize</li>
</ul>
<hr>
<h2 id="regularization-preventing-overfitting">Regularization (Preventing Overfitting)</h2>
<h3 id="option-1-early-stopping">Option 1: Early Stopping</h3>
<p>Stop growing when:</p>
<ul>
<li>Maximum depth reached</li>
<li>Minimum samples per leaf</li>
<li>Improvement below threshold</li>
</ul>
<p><strong>Problem</strong>: May stop too early (miss good splits downstream).</p>
<h3 id="option-2-grow-and-prune">Option 2: Grow and Prune</h3>
<ol>
<li>Grow full tree (until pure leaves or minimum samples)</li>
<li>Prune back using cost-complexity criterion:</li>
</ol>
<p>$$C_\alpha(T) = \sum_{j=1}^{|T|} N_j \cdot C_j + \alpha |T|$$</p>
<p>Where $|T|$ is number of leaves and Œ± is complexity penalty.</p>
<p>Use cross-validation to select optimal Œ±.</p>
<h3 id="handling-missing-features">Handling Missing Features</h3>
<p><strong>Categorical</strong>: Treat &quot;missing&quot; as new category.</p>
<p><strong>Continuous</strong>: Use surrogate splits ‚Äî find alternative splits that best mimic the primary split.</p>
<hr>
<h2 id="pros-and-cons-of-trees">Pros and Cons of Trees</h2>
<h3 id="advantages">Advantages</h3>
<ul>
<li><strong>Interpretable</strong>: Easy to visualize and explain</li>
<li><strong>Minimal preprocessing</strong>: Handles mixed types, no normalization needed</li>
<li><strong>Fast</strong>: Prediction is O(log nodes)</li>
<li><strong>Robust to outliers</strong>: Splits don&#39;t depend on magnitudes</li>
</ul>
<h3 id="disadvantages">Disadvantages</h3>
<ul>
<li><strong>High variance</strong>: Small data changes ‚Üí different tree</li>
<li><strong>Axis-aligned only</strong>: Can&#39;t capture diagonal boundaries efficiently</li>
<li><strong>Prone to overfitting</strong>: Without regularization</li>
</ul>
<hr>
<h2 id="ensemble-learning">Ensemble Learning</h2>
<h3 id="the-core-idea">The Core Idea</h3>
<p>Combine multiple models to reduce errors.</p>
<p><strong>Regression</strong>: Average predictions
$$\hat{y} = \frac{1}{M}\sum_{m=1}^M f_m(x)$$</p>
<p><strong>Classification</strong>: Majority vote or average probabilities</p>
<h3 id="why-ensembles-work">Why Ensembles Work</h3>
<p>For M independent classifiers each with accuracy p &gt; 0.5:
$$P(\text{majority correct}) = \sum_{k &gt; M/2} \binom{M}{k} p^k (1-p)^{M-k}$$</p>
<p>As M ‚Üí ‚àû, this probability ‚Üí 1!</p>
<p><strong>Key requirement</strong>: Classifiers must be diverse (uncorrelated errors).</p>
<h3 id="stacking">Stacking</h3>
<p>Learn weights for combining models:
$$\hat{y} = \sum_m w_m f_m(x)$$</p>
<p>Train weights on held-out data to avoid overfitting.</p>
<hr>
<h2 id="bagging-bootstrap-aggregating">Bagging (Bootstrap Aggregating)</h2>
<h3 id="algorithm">Algorithm</h3>
<ol>
<li>Create B bootstrap samples (sample with replacement)</li>
<li>Train a tree on each bootstrap sample</li>
<li>Average predictions (regression) or vote (classification)</li>
</ol>
<h3 id="key-properties">Key Properties</h3>
<ul>
<li><p>Each bootstrap sample contains ~63% of unique points:
$$P(\text{included}) = 1 - (1 - 1/N)^N \approx 1 - 1/e \approx 0.632$$</p>
</li>
<li><p><strong>OOB Error</strong>: Evaluate each tree on its out-of-bag samples (free cross-validation!)</p>
</li>
</ul>
<h3 id="variance-reduction">Variance Reduction</h3>
<p>$$\text{Var}(\bar{f}) = \rho\sigma^2 + \frac{1-\rho}{B}\sigma^2$$</p>
<p>Where œÅ is correlation between trees.</p>
<ul>
<li>More trees (larger B) ‚Üí second term vanishes</li>
<li>Less correlation (smaller œÅ) ‚Üí first term shrinks</li>
</ul>
<hr>
<h2 id="random-forests">Random Forests</h2>
<h3 id="beyond-bagging">Beyond Bagging</h3>
<p>Bagging helps, but trees from similar data are correlated.</p>
<p><strong>Random Forest innovation</strong>: Add randomness to splits.</p>
<p>At each split:</p>
<ol>
<li>Randomly select m features (typically $m = \sqrt{p}$ for classification, $m = p/3$ for regression)</li>
<li>Find best split among only those m features</li>
</ol>
<h3 id="why-it-works">Why It Works</h3>
<ul>
<li>Forces trees to use different features</li>
<li>Reduces correlation between trees</li>
<li>Combined with bagging ‚Üí powerful ensemble</li>
</ul>
<h3 id="extra-trees">Extra Trees</h3>
<p>Even more randomness:</p>
<ul>
<li>Random feature subset (like RF)</li>
<li>Random threshold selection (not optimized)</li>
<li>Faster training, often similar performance</li>
</ul>
<hr>
<h2 id="boosting">Boosting</h2>
<h3 id="the-key-idea">The Key Idea</h3>
<p>Sequentially fit weak learners, each focusing on previous mistakes.</p>
<p>$$F_m(x) = F_{m-1}(x) + \beta_m f_m(x)$$</p>
<p><strong>Boosting reduces bias</strong> (unlike bagging which reduces variance).</p>
<h3 id="adaboost">AdaBoost</h3>
<p>For classification with exponential loss:
$$L(y, F) = \exp(-yF(x)), \quad y \in {-1, +1}$$</p>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>Initialize equal weights on examples</li>
<li>For each round:<ul>
<li>Train weak learner on weighted examples</li>
<li>Increase weights on misclassified examples</li>
<li>Compute learner weight based on accuracy</li>
</ul>
</li>
</ol>
<h3 id="gradient-boosting">Gradient Boosting</h3>
<p>Generalize to any differentiable loss:</p>
<ol>
<li>Initialize: $F_0 = \arg\min_\gamma \sum L(y_i, \gamma)$</li>
<li>For m = 1 to M:<ul>
<li>Compute pseudo-residuals: $r_i = -\frac{\partial L(y_i, F)}{\partial F}|<em>{F</em>{m-1}}$</li>
<li>Fit weak learner to pseudo-residuals</li>
<li>Line search for step size</li>
<li>Update: $F_m = F_{m-1} + \eta \cdot f_m$</li>
</ul>
</li>
</ol>
<p><strong>For MSE loss</strong>: Pseudo-residuals = actual residuals!</p>
<p><strong>Regularization via shrinkage</strong>: Small learning rate Œ∑ (0.01-0.1) + more trees.</p>
<h3 id="stochastic-gradient-boosting">Stochastic Gradient Boosting</h3>
<p>Subsample data at each round:</p>
<ul>
<li>Faster training</li>
<li>Better generalization (regularization effect)</li>
</ul>
<hr>
<h2 id="xgboost">XGBoost</h2>
<h3 id="innovations">Innovations</h3>
<ol>
<li><p><strong>Regularized objective</strong>:
$$L = \sum L(y_i, F(x_i)) + \gamma J + \frac{\lambda}{2}\sum w_j^2$$</p>
</li>
<li><p><strong>Second-order approximation</strong> (use Hessian):
$$L \approx \sum [g_i F(x_i) + \frac{1}{2}h_i F(x_i)^2] + \text{regularization}$$</p>
</li>
<li><p><strong>Optimal leaf weights</strong>:
$$w_j^* = -\frac{G_j}{H_j + \lambda}$$</p>
</li>
</ol>
<p>Where $G_j = \sum_{i \in j} g_i$ and $H_j = \sum_{i \in j} h_i$.</p>
<ol start="4">
<li><strong>Split gain</strong>:
$$\text{Gain} = \frac{G_L^2}{H_L + \lambda} + \frac{G_R^2}{H_R + \lambda} - \frac{(G_L+G_R)^2}{H_L+H_R+\lambda} - \gamma$$</li>
</ol>
<p>Œ≥ acts as regularization ‚Äî won&#39;t split unless gain exceeds Œ≥.</p>
<hr>
<h2 id="feature-importance">Feature Importance</h2>
<h3 id="mean-decrease-in-impurity">Mean Decrease in Impurity</h3>
<p>Sum the impurity decrease at all splits using feature k:
$$R_k = \sum_{\text{nodes using } k} \Delta \text{impurity}$$</p>
<p>Average across all trees.</p>
<p><strong>Caveat</strong>: Biased toward high-cardinality features.</p>
<h3 id="permutation-importance">Permutation Importance</h3>
<ol>
<li>Compute baseline accuracy</li>
<li>For each feature k:<ul>
<li>Permute (shuffle) feature k&#39;s values</li>
<li>Compute accuracy drop</li>
</ul>
</li>
<li>Importance = accuracy drop</li>
</ol>
<p>More reliable but slower.</p>
<h3 id="partial-dependence-plots">Partial Dependence Plots</h3>
<p>Visualize effect of feature on prediction:
$$\bar{f}<em>k(x_k) = \frac{1}{N}\sum</em>{i=1}^N f(x_k, x_{i,-k})$$</p>
<p>Average over all other features.</p>
<hr>
<h2 id="summary">Summary</h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Reduces</th>
<th>Training</th>
<th>Key Hyperparameters</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Single Tree</strong></td>
<td>‚Äî</td>
<td>Fast</td>
<td>max_depth, min_samples</td>
</tr>
<tr>
<td><strong>Bagging</strong></td>
<td>Variance</td>
<td>Parallel</td>
<td>n_estimators</td>
</tr>
<tr>
<td><strong>Random Forest</strong></td>
<td>Variance</td>
<td>Parallel</td>
<td>n_estimators, max_features</td>
</tr>
<tr>
<td><strong>Boosting</strong></td>
<td>Bias</td>
<td>Sequential</td>
<td>n_estimators, learning_rate, max_depth</td>
</tr>
</tbody></table>
<h3 id="practical-recommendations">Practical Recommendations</h3>
<ol>
<li><strong>Start with Random Forest</strong>: Works well with minimal tuning</li>
<li><strong>Try XGBoost/LightGBM</strong>: Often best for tabular data</li>
<li><strong>Tune carefully</strong>: Learning rate and n_estimators together</li>
<li><strong>Early stopping</strong>: Monitor validation error</li>
<li><strong>Feature importance</strong>: Helps interpretability</li>
</ol>

        </article>
        <nav class="page-navigation">
        <a href="probml-16-exemplar.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Exemplar-Based Methods</span>
        </a>
        <a href="probml-19-ssl.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Self-Supervised and Semi-Supervised Learning</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>