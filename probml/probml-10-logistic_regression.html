<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Logistic Regression | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="probml-10-logistic_regression.html" class="active">Logistic Regression</a></li>
            <li class="nav-item"><a href="probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">ProbML</a>
          </div>
          <h1 class="page-title">Logistic Regression</h1>
          <div class="page-meta"><span class="tag">ProbML</span></div>
        </header>
        <article class="content">
          <h1 id="logistic-regression">Logistic Regression</h1>
<p>Logistic regression is the foundational discriminative model for classification. Despite its name, it&#39;s a classification algorithm (not regression) that directly models posterior class probabilities.</p>
<h2 id="the-big-picture">The Big Picture</h2>
<p>Unlike generative models (which model how data is generated per class), logistic regression directly models:
$$p(y | x, \theta)$$</p>
<p>This &quot;discriminative&quot; approach focuses on the decision boundary rather than the full data distribution.</p>
<hr>
<h2 id="binary-logistic-regression">Binary Logistic Regression</h2>
<h3 id="the-model">The Model</h3>
<p>For binary classification with $y \in {0, 1}$:</p>
<p>$$p(y = 1 | x, \theta) = \sigma(w^T x + b)$$</p>
<p>Where œÉ is the <strong>sigmoid function</strong>:
$$\sigma(a) = \frac{1}{1 + e^{-a}}$$</p>
<p>The probability of class 0 is simply:
$$p(y = 0 | x, \theta) = 1 - \sigma(w^T x + b) = \sigma(-(w^T x + b))$$</p>
<h3 id="alternative-notation">Alternative Notation</h3>
<p>For $y \in {-1, +1}$:
$$p(y | x, \theta) = \sigma(y \cdot (w^T x + b))$$</p>
<p>This compact form handles both classes with one equation.</p>
<h3 id="the-decision-boundary">The Decision Boundary</h3>
<p>Predict $y = 1$ if $p(y = 1 | x) &gt; 0.5$, which occurs when:
$$w^T x + b &gt; 0$$</p>
<p>This is a <strong>hyperplane</strong> in feature space ‚Äî logistic regression produces linear decision boundaries.</p>
<p><strong>Geometric interpretation</strong>:</p>
<ul>
<li><strong>w</strong> is the normal vector to the decision boundary</li>
<li><strong>b</strong> determines the offset from the origin</li>
<li>Distance from boundary relates to confidence</li>
</ul>
<hr>
<h2 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h2>
<h3 id="the-likelihood">The Likelihood</h3>
<p>For dataset ${(x_i, y_i)}<em>{i=1}^N$:
$$L(\theta) = \prod</em>{i=1}^N p(y_i | x_i, \theta)$$</p>
<h3 id="negative-log-likelihood-binary-cross-entropy">Negative Log-Likelihood (Binary Cross-Entropy)</h3>
<p>$$\text{NLL}(\theta) = -\sum_{i=1}^N [y_i \log \hat{y}_i + (1 - y_i) \log(1 - \hat{y}_i)]$$</p>
<p>Where $\hat{y}_i = \sigma(w^T x_i + b)$.</p>
<p>For $y \in {-1, +1}$ notation:
$$\text{NLL}(\theta) = \sum_{i=1}^N \log(1 + \exp(-y_i(w^T x_i + b)))$$</p>
<h3 id="computing-the-gradient">Computing the Gradient</h3>
<p>The gradient has a beautiful form:
$$\nabla_w \text{NLL} = \sum_{i=1}^N (\hat{y}_i - y_i) x_i = X^T(\hat{y} - y)$$</p>
<p><strong>Intuition</strong>: The gradient is a weighted sum of input vectors, where the weights are the prediction errors.</p>
<h3 id="optimization">Optimization</h3>
<p><strong>Good news</strong>: The NLL is <strong>convex</strong> (Hessian is positive semi-definite).</p>
<p><strong>Methods</strong>:</p>
<ol>
<li><strong>Gradient Descent / SGD</strong>: Simple, works for large datasets</li>
<li><strong>Newton&#39;s Method</strong>: Faster convergence for smaller problems</li>
<li><strong>IRLS</strong>: Iteratively Reweighted Least Squares ‚Äî Newton&#39;s method specialized for logistic regression</li>
</ol>
<hr>
<h2 id="regularization">Regularization</h2>
<h3 id="the-overfitting-problem">The Overfitting Problem</h3>
<p>MLE can overfit, especially with:</p>
<ul>
<li>High-dimensional features</li>
<li>Small datasets</li>
<li>Linearly separable data (weights ‚Üí ‚àû)</li>
</ul>
<h3 id="l2-regularization-ridge">L2 Regularization (Ridge)</h3>
<p>Add Gaussian prior on weights:
$$p(w) = \mathcal{N}(0, \lambda^{-1} I)$$</p>
<p><strong>Regularized objective</strong>:
$$L(w) = \text{NLL}(w) + \lambda |w|^2$$</p>
<p><strong>Effect</strong>: Penalizes large weights, improves generalization.</p>
<h3 id="l1-regularization-lasso">L1 Regularization (Lasso)</h3>
<p>Use Laplace prior for sparse solutions:
$$L(w) = \text{NLL}(w) + \lambda |w|_1$$</p>
<p><strong>Effect</strong>: Some weights become exactly zero ‚Äî automatic feature selection.</p>
<h3 id="practical-notes">Practical Notes</h3>
<ul>
<li><strong>Standardize features</strong> before applying regularization (features should be on same scale)</li>
<li><strong>Don&#39;t regularize the bias</strong> term</li>
<li>Choose Œª via cross-validation</li>
</ul>
<hr>
<h2 id="multinomial-logistic-regression">Multinomial Logistic Regression</h2>
<h3 id="extending-to-multiple-classes">Extending to Multiple Classes</h3>
<p>For $y \in {1, 2, ..., C}$:</p>
<p>$$p(y = c | x, \theta) = \frac{\exp(w_c^T x + b_c)}{\sum_{j=1}^C \exp(w_j^T x + b_j)} = \text{softmax}(a)_c$$</p>
<p>Where $a_c = w_c^T x + b_c$ are the <strong>logits</strong>.</p>
<h3 id="overparameterization">Overparameterization</h3>
<p>Note: We have C sets of weights, but only C-1 are needed (one class can be the reference).</p>
<p>For binary case with softmax:
$$p(y = 0 | x) = \frac{e^{a_0}}{e^{a_0} + e^{a_1}} = \sigma(a_0 - a_1)$$</p>
<p>This reduces to standard logistic regression with $w = w_0 - w_1$.</p>
<h3 id="maximum-entropy-classifier">Maximum Entropy Classifier</h3>
<p>When features depend on both x and the class c:
$$p(y = c | x, w) \propto \exp(w^T \phi(x, c))$$</p>
<p>This is common in NLP where features might include &quot;word X appears AND class is Y&quot;.</p>
<hr>
<h2 id="handling-special-cases">Handling Special Cases</h2>
<h3 id="hierarchical-classification">Hierarchical Classification</h3>
<p>When classes have taxonomy (e.g., animal ‚Üí mammal ‚Üí dog):</p>
<p><strong>Label smearing</strong>: Propagate positive labels to parent categories.</p>
<p><strong>Approach</strong>: Multi-label classification where an example can belong to multiple levels.</p>
<h3 id="many-classes">Many Classes</h3>
<p><strong>Hierarchical softmax</strong>: Organize classes in a tree; predict by traversing tree.</p>
<ul>
<li>Reduces computation from O(C) to O(log C)</li>
<li>Put frequent classes near root</li>
</ul>
<h3 id="class-imbalance">Class Imbalance</h3>
<p>When some classes are much more common:</p>
<p><strong>Resampling strategies</strong>:
$$p_c = \frac{N_c^q}{\sum_j N_j^q}$$</p>
<ul>
<li>q = 1: Instance-balanced (original distribution)</li>
<li>q = 0: Class-balanced (equal weight per class)</li>
<li>q = 0.5: Square-root sampling (compromise)</li>
</ul>
<hr>
<h2 id="robust-logistic-regression">Robust Logistic Regression</h2>
<h3 id="handling-outliers-and-label-noise">Handling Outliers and Label Noise</h3>
<p>Standard logistic regression is sensitive to mislabeled examples.</p>
<p><strong>Mixture model approach</strong>:
$$p(y | x) = \pi \cdot \text{Ber}(0.5) + (1 - \pi) \cdot \text{Ber}(\sigma(w^T x + b))$$</p>
<p>Mix predictions with uniform noise ‚Äî mislabeled points have less impact.</p>
<h3 id="bi-tempered-logistic-loss">Bi-tempered Logistic Loss</h3>
<p>Two modifications for robustness:</p>
<ol>
<li><strong>Tempered cross-entropy</strong>: Handles outliers far from boundary</li>
<li><strong>Tempered softmax</strong>: Handles noise near boundary</li>
</ol>
<h3 id="probit-regression">Probit Regression</h3>
<p>Replace sigmoid with Gaussian CDF (probit function):
$$p(y = 1 | x) = \Phi(w^T x + b)$$</p>
<p>Similar shape to logistic but different tails ‚Äî can be more robust in some cases.</p>
<hr>
<h2 id="summary">Summary</h2>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Key Points</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Model</strong></td>
<td>$p(y=1|x) = \sigma(w^Tx + b)$</td>
</tr>
<tr>
<td><strong>Loss</strong></td>
<td>Binary cross-entropy (NLL)</td>
</tr>
<tr>
<td><strong>Optimization</strong></td>
<td>Convex ‚Äî guaranteed global optimum</td>
</tr>
<tr>
<td><strong>Boundary</strong></td>
<td>Linear (hyperplane)</td>
</tr>
<tr>
<td><strong>Regularization</strong></td>
<td>L2 (shrink) or L1 (sparse)</td>
</tr>
<tr>
<td><strong>Multiclass</strong></td>
<td>Softmax over C classes</td>
</tr>
<tr>
<td><strong>Robustness</strong></td>
<td>Mixture models, tempered losses</td>
</tr>
</tbody></table>
<h3 id="when-to-use-logistic-regression">When to Use Logistic Regression</h3>
<p><strong>Good for</strong>:</p>
<ul>
<li>Binary and multiclass classification</li>
<li>When interpretability matters (coefficients are meaningful)</li>
<li>As a baseline before trying complex models</li>
<li>When computational resources are limited</li>
</ul>
<p><strong>Limitations</strong>:</p>
<ul>
<li>Linear decision boundaries</li>
<li>May underfit complex data</li>
<li>Sensitive to outliers (without modifications)</li>
</ul>
<p><strong>Pro tip</strong>: Start with logistic regression. If it works well, you may not need anything more complex!</p>

        </article>
        <nav class="page-navigation">
        <a href="probml-09-discriminant_analysis.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Discriminant Analysis</span>
        </a>
        <a href="probml-11-linear_regression.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Linear Regression</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>