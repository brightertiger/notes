<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Random Forests | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="eslr-15-random-forest.html" class="active">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../probml/probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="../probml/probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="../probml/probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="../probml/probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="../probml/probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="../probml/probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="../probml/probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="../probml/probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="../probml/probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="../probml/probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">ESLR</a>
          </div>
          <h1 class="page-title">Random Forests</h1>
          <div class="page-meta"><span class="tag">ESLR</span></div>
        </header>
        <article class="content">
          <h1 id="random-forests">Random Forests</h1>
<p>Random Forests are one of the most successful and widely-used machine learning algorithms. They combine the simplicity of decision trees with the power of ensemble methods to create a robust, accurate, and easy-to-use classifier.</p>
<h2 id="the-big-picture">The Big Picture</h2>
<p>Decision trees are intuitive and interpretable, but they have a major flaw: <strong>high variance</strong>. A small change in the training data can produce a completely different tree. Random Forests solve this by building many trees and averaging their predictions.</p>
<p><strong>Key insight</strong>: Many imperfect models, when combined intelligently, can outperform a single &quot;perfect&quot; model.</p>
<hr>
<h2 id="why-averaging-helps">Why Averaging Helps</h2>
<h3 id="the-bias-variance-view">The Bias-Variance View</h3>
<p>Individual decision trees (especially deep ones) have:</p>
<ul>
<li><strong>Low bias</strong>: They can fit complex patterns</li>
<li><strong>High variance</strong>: They&#39;re sensitive to the specific training data</li>
</ul>
<p>Averaging reduces variance while maintaining low bias!</p>
<h3 id="mathematical-intuition">Mathematical Intuition</h3>
<p>Consider B random variables (predictions from B trees), each with:</p>
<ul>
<li>Individual variance: $\sigma^2$</li>
<li>Pairwise correlation: $\rho$</li>
</ul>
<p>The variance of their average is:</p>
<p>$$\text{Var}\left(\frac{1}{B}\sum_{b=1}^B X_b\right) = \rho\sigma^2 + \frac{(1-\rho)}{B}\sigma^2$$</p>
<p><strong>Two key insights</strong>:</p>
<ol>
<li><strong>More trees (larger B) always helps</strong>: The second term shrinks toward 0</li>
<li><strong>Lower correlation ($\rho$) helps even more</strong>: The first term shrinks</li>
</ol>
<p><strong>This is why Random Forests can&#39;t overfit by adding more trees!</strong> (Unlike boosting, which can overfit with too many rounds.)</p>
<hr>
<h2 id="bagging-the-foundation">Bagging: The Foundation</h2>
<p><strong>Bagging</strong> (Bootstrap Aggregating) is the simpler ancestor of Random Forests.</p>
<h3 id="the-algorithm">The Algorithm</h3>
<ol>
<li><strong>Bootstrap</strong>: Draw B samples of size N with replacement from training data</li>
<li><strong>Train</strong>: Fit a decision tree to each bootstrap sample</li>
<li><strong>Aggregate</strong>: <ul>
<li>Regression: Average predictions</li>
<li>Classification: Majority vote</li>
</ul>
</li>
</ol>
<h3 id="why-bootstrap">Why Bootstrap?</h3>
<p>Each bootstrap sample is slightly different from the original data:</p>
<ul>
<li>Contains ~63.2% of unique observations</li>
<li>Some observations appear multiple times</li>
<li>Others don&#39;t appear at all</li>
</ul>
<p>This variation creates diversity among trees ‚Äî different trees make different mistakes!</p>
<hr>
<h2 id="random-forests-adding-more-randomness">Random Forests: Adding More Randomness</h2>
<p>Random Forests add an additional source of randomness to further <strong>decorrelate</strong> the trees.</p>
<h3 id="the-key-innovation">The Key Innovation</h3>
<p>At each split, instead of considering all features, consider only a <strong>random subset</strong> of m features.</p>
<p><strong>Why this helps</strong>: </p>
<ul>
<li>In bagging, if one feature is very strong, every tree uses it at the root ‚Üí trees are correlated</li>
<li>Random feature selection forces trees to use different features ‚Üí less correlation</li>
</ul>
<h3 id="typical-values-for-m">Typical Values for m</h3>
<table>
<thead>
<tr>
<th>Task</th>
<th>Recommended m</th>
</tr>
</thead>
<tbody><tr>
<td>Classification</td>
<td>$\sqrt{p}$</td>
</tr>
<tr>
<td>Regression</td>
<td>$p/3$</td>
</tr>
</tbody></table>
<p>Where p = total number of features.</p>
<h3 id="the-complete-algorithm">The Complete Algorithm</h3>
<ol>
<li>For b = 1 to B trees:<ul>
<li>Draw a bootstrap sample of size N</li>
<li>Grow a tree:<ul>
<li>At each node, randomly select m features</li>
<li>Find the best split among those m features</li>
<li>Split the node</li>
<li>Repeat until stopping criterion (min node size)</li>
</ul>
</li>
</ul>
</li>
<li>Output: Ensemble of B trees</li>
</ol>
<p>For prediction:</p>
<ul>
<li><strong>Regression</strong>: $\hat{f}(x) = \frac{1}{B}\sum_{b=1}^B \hat{f}_b(x)$</li>
<li><strong>Classification</strong>: $\hat{G}(x) = \text{majority vote of } \hat{G}_b(x)$</li>
</ul>
<hr>
<h2 id="out-of-bag-oob-error">Out-of-Bag (OOB) Error</h2>
<p>One of the most elegant features of Random Forests: <strong>free cross-validation</strong>!</p>
<h3 id="the-idea">The Idea</h3>
<p>Each bootstrap sample leaves out ~36.8% of observations. For each observation i:</p>
<ol>
<li>Find all trees where i was NOT in the training sample</li>
<li>Use only those trees to predict for i</li>
<li>This is an honest prediction ‚Äî no leakage!</li>
</ol>
<h3 id="oob-error-estimate">OOB Error Estimate</h3>
<p>$$\text{OOB Error} = \frac{1}{N}\sum_{i=1}^N L(y_i, \hat{y}_i^{\text{OOB}})$$</p>
<p>Where $\hat{y}_i^{\text{OOB}}$ is the prediction using only trees that didn&#39;t train on observation i.</p>
<h3 id="properties">Properties</h3>
<ul>
<li><strong>Essentially equivalent to leave-one-out cross-validation</strong></li>
<li><strong>Computed for free</strong> during training</li>
<li><strong>No need for separate validation set</strong></li>
<li><strong>Honest estimate</strong> of generalization error</li>
</ul>
<hr>
<h2 id="variable-importance">Variable Importance</h2>
<p>Random Forests provide built-in measures of which features matter most.</p>
<h3 id="method-1-mean-decrease-in-impurity">Method 1: Mean Decrease in Impurity</h3>
<p>For each feature j:</p>
<ol>
<li>At each split on feature j, record the decrease in impurity (Gini, entropy, or MSE)</li>
<li>Sum across all splits and all trees</li>
<li>Normalize</li>
</ol>
<p><strong>Pros</strong>: Fast, computed during training
<strong>Cons</strong>: Biased toward high-cardinality categorical features</p>
<h3 id="method-2-permutation-importance">Method 2: Permutation Importance</h3>
<p>A more reliable approach:</p>
<ol>
<li>Compute OOB accuracy for the original data</li>
<li>For each feature j:<ul>
<li>Randomly shuffle (permute) feature j&#39;s values</li>
<li>Recompute OOB accuracy</li>
<li>Record the decrease in accuracy</li>
</ul>
</li>
<li>Average across all trees</li>
</ol>
<p><strong>Interpretation</strong>: If shuffling feature j destroys accuracy, that feature was important!</p>
<p><strong>Pros</strong>: Unbiased, captures complex dependencies
<strong>Cons</strong>: Computationally more expensive</p>
<h3 id="when-variables-are-correlated">When Variables Are Correlated</h3>
<p>Both importance measures can be misleading with correlated features:</p>
<ul>
<li>Importance may be split among correlated features</li>
<li>A single feature from a correlated group may show low importance</li>
</ul>
<hr>
<h2 id="proximity-measures">Proximity Measures</h2>
<p>Random Forests can measure similarity between observations.</p>
<h3 id="computing-proximities">Computing Proximities</h3>
<p>For each pair of observations (i, k):</p>
<ol>
<li>Count how often they end up in the same terminal node</li>
<li>Across all trees</li>
<li>Normalize by number of trees</li>
</ol>
<p>This creates an N √ó N <strong>proximity matrix</strong>.</p>
<h3 id="uses">Uses</h3>
<ul>
<li><strong>Visualization</strong>: Use multidimensional scaling (MDS) to plot proximities in 2D</li>
<li><strong>Outlier detection</strong>: Points with low average proximity to their class may be outliers</li>
<li><strong>Missing value imputation</strong>: Fill in missing values using weighted averages of similar observations</li>
<li><strong>Clustering</strong>: Use proximity as a similarity measure</li>
</ul>
<hr>
<h2 id="advantages-of-random-forests">Advantages of Random Forests</h2>
<h3 id="1-accuracy">1. Accuracy</h3>
<ul>
<li>Often among the best performing methods &quot;out of the box&quot;</li>
<li>Works well on many types of data without much tuning</li>
</ul>
<h3 id="2-robustness">2. Robustness</h3>
<ul>
<li>Handles missing values gracefully</li>
<li>Not sensitive to outliers (median of trees is robust)</li>
<li>Works with both categorical and continuous features</li>
</ul>
<h3 id="3-scalability">3. Scalability</h3>
<ul>
<li>Parallelizes naturally (trees are independent)</li>
<li>Handles large datasets efficiently</li>
</ul>
<h3 id="4-interpretability-relative-to-other-ensembles">4. Interpretability (relative to other ensembles)</h3>
<ul>
<li>Variable importance provides insights</li>
<li>Individual trees can be examined</li>
<li>Proximities enable visualization</li>
</ul>
<h3 id="5-built-in-validation">5. Built-in Validation</h3>
<ul>
<li>OOB error provides honest generalization estimate</li>
<li>No need for separate cross-validation</li>
</ul>
<hr>
<h2 id="limitations">Limitations</h2>
<h3 id="1-less-interpretable-than-single-trees">1. Less Interpretable Than Single Trees</h3>
<ul>
<li>Can&#39;t see a single &quot;path&quot; to a prediction</li>
<li>Hard to extract simple rules</li>
</ul>
<h3 id="2-memory-and-speed">2. Memory and Speed</h3>
<ul>
<li>Storing many trees requires memory</li>
<li>Prediction time scales with number of trees</li>
</ul>
<h3 id="3-extrapolation">3. Extrapolation</h3>
<ul>
<li>Can&#39;t extrapolate beyond the range of training data</li>
<li>Predictions for extreme values will be bounded by training range</li>
</ul>
<h3 id="4-imbalanced-classes">4. Imbalanced Classes</h3>
<ul>
<li>May be biased toward majority class</li>
<li>May need class weights or sampling adjustments</li>
</ul>
<hr>
<h2 id="hyperparameter-tuning">Hyperparameter Tuning</h2>
<p>Random Forests have relatively few hyperparameters:</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Description</th>
<th>Typical Values</th>
<th>Effect</th>
</tr>
</thead>
<tbody><tr>
<td><strong>n_estimators</strong></td>
<td>Number of trees</td>
<td>100-1000</td>
<td>More is better (diminishing returns)</td>
</tr>
<tr>
<td><strong>max_features</strong></td>
<td>Features per split</td>
<td>sqrt(p), p/3</td>
<td>Lower ‚Üí more diversity, more variance</td>
</tr>
<tr>
<td><strong>max_depth</strong></td>
<td>Tree depth</td>
<td>None (grow full), or 10-30</td>
<td>Deeper ‚Üí lower bias, higher variance</td>
</tr>
<tr>
<td><strong>min_samples_leaf</strong></td>
<td>Min samples in leaf</td>
<td>1-10</td>
<td>Higher ‚Üí smoother, simpler trees</td>
</tr>
</tbody></table>
<h3 id="practical-tips">Practical Tips</h3>
<ol>
<li><strong>Start with defaults</strong>: They often work well!</li>
<li><strong>More trees rarely hurt</strong>: Just costs compute time</li>
<li><strong>max_features is most important</strong>: Try sqrt(p), log(p), and p/3</li>
<li><strong>Deeper trees are fine</strong>: Unlike single trees, Random Forests resist overfitting</li>
</ol>
<hr>
<h2 id="random-forests-vs-boosting">Random Forests vs. Boosting</h2>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Random Forests</th>
<th>Gradient Boosting</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Training</strong></td>
<td>Parallel (independent trees)</td>
<td>Sequential (each tree corrects errors)</td>
</tr>
<tr>
<td><strong>Overfitting</strong></td>
<td>Very resistant</td>
<td>Can overfit with too many rounds</td>
</tr>
<tr>
<td><strong>Tuning</strong></td>
<td>Minimal tuning needed</td>
<td>Requires careful tuning</td>
</tr>
<tr>
<td><strong>Accuracy</strong></td>
<td>Very good</td>
<td>Often slightly better (with tuning)</td>
</tr>
<tr>
<td><strong>Speed</strong></td>
<td>Fast (parallelizable)</td>
<td>Slower (sequential)</td>
</tr>
<tr>
<td><strong>Interpretability</strong></td>
<td>Variable importance</td>
<td>Less interpretable</td>
</tr>
</tbody></table>
<h3 id="when-to-use-which">When to Use Which?</h3>
<p><strong>Random Forests</strong>: </p>
<ul>
<li>Quick baseline</li>
<li>Limited tuning time</li>
<li>Parallel computing available</li>
<li>Want OOB error estimate</li>
</ul>
<p><strong>Gradient Boosting</strong>:</p>
<ul>
<li>Maximum accuracy needed</li>
<li>Time for hyperparameter tuning</li>
<li>Tabular data competitions</li>
</ul>
<hr>
<h2 id="summary">Summary</h2>
<h3 id="key-takeaways">Key Takeaways</h3>
<ol>
<li><p><strong>Averaging reduces variance</strong>: The fundamental principle behind Random Forests</p>
</li>
<li><p><strong>Decorrelation is key</strong>: Random feature selection makes trees diverse</p>
</li>
<li><p><strong>Can&#39;t overfit by adding trees</strong>: Unlike many methods, more trees is (almost) always better</p>
</li>
<li><p><strong>OOB error is free cross-validation</strong>: Built-in generalization estimate</p>
</li>
<li><p><strong>Variable importance provides insights</strong>: Understand which features matter</p>
</li>
<li><p><strong>Minimal tuning required</strong>: Works well out of the box</p>
</li>
</ol>
<h3 id="the-random-forest-workflow">The Random Forest Workflow</h3>
<pre><code>1. Train Random Forest with default settings
2. Check OOB error for baseline performance
3. Examine variable importance for insights
4. If needed, tune max_features and min_samples_leaf
5. For final model, use more trees (500-1000)
</code></pre>
<p>Random Forests remain one of the best &quot;first try&quot; algorithms for tabular data ‚Äî accurate, robust, and easy to use!</p>

        </article>
        <nav class="page-navigation">
        <a href="eslr-10-boosting.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Boosting and Additive Trees</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>