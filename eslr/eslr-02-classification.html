<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Classification | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="eslr-02-classification.html" class="active">Classification</a></li>
            <li class="nav-item"><a href="eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../probml/probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="../probml/probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="../probml/probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="../probml/probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="../probml/probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="../probml/probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="../probml/probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="../probml/probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="../probml/probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="../probml/probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">ESLR</a>
          </div>
          <h1 class="page-title">Classification</h1>
          <div class="page-meta"><span class="tag">ESLR</span></div>
        </header>
        <article class="content">
          <h1 id="classification">Classification</h1>
<p>Classification is one of the most common machine learning tasks. Unlike regression (where we predict a continuous number), in classification we predict which <em>category</em> or <em>class</em> an observation belongs to. Examples include: spam vs. not spam, disease vs. healthy, or recognizing handwritten digits (0-9).</p>
<h2 id="the-big-picture">The Big Picture</h2>
<p>In classification, we want to:</p>
<ol>
<li><strong>Learn decision boundaries</strong> that separate different classes</li>
<li><strong>Estimate probabilities</strong> of class membership</li>
<li><strong>Make predictions</strong> for new observations</li>
</ol>
<p>The methods in this chapter produce <em>linear</em> decision boundaries ‚Äî the simplest and most interpretable kind.</p>
<hr>
<h2 id="decision-boundaries">Decision Boundaries</h2>
<h3 id="what-is-a-decision-boundary">What is a Decision Boundary?</h3>
<p>A decision boundary is the dividing line (or surface, in higher dimensions) between regions assigned to different classes. When a new observation falls on one side, we predict one class; on the other side, we predict the other class.</p>
<h3 id="linear-decision-boundaries">Linear Decision Boundaries</h3>
<p>The boundary is linear if it&#39;s described by a linear equation in the features:</p>
<p>$${x : w_0 + w_1 x_1 + w_2 x_2 + ... + w_p x_p = 0}$$</p>
<p>In 2D, this is a straight line. In 3D, it&#39;s a plane. In higher dimensions, it&#39;s a <strong>hyperplane</strong>.</p>
<h3 id="how-do-we-get-linear-boundaries">How Do We Get Linear Boundaries?</h3>
<p>The decision boundary is linear if any of these conditions hold:</p>
<ul>
<li>The discriminant function $\delta_k(x)$ is linear in x</li>
<li>The posterior probability $P(G=k|X=x)$ is linear in x</li>
<li>Some monotonic transformation of the above is linear</li>
</ul>
<h3 id="discriminant-functions">Discriminant Functions</h3>
<p>We learn a <strong>discriminant function</strong> $\delta_k(x)$ for each class k. To classify a new point x:</p>
<ul>
<li>Compute $\delta_k(x)$ for all classes</li>
<li>Assign x to the class with the highest discriminant value</li>
</ul>
<p>For linear discriminant functions:</p>
<p>$$\delta_k(x) = \beta_{0k} + \beta_k^T x$$</p>
<p>The decision boundary between classes k and l is where $\delta_k(x) = \delta_l(x)$:</p>
<p>$${x : (\beta_{0k} - \beta_{0l}) + (\beta_k - \beta_l)^T x = 0}$$</p>
<p>This is clearly linear in x ‚Äî an affine set (hyperplane not necessarily through the origin).</p>
<hr>
<h2 id="example-logistic-regression-boundary">Example: Logistic Regression Boundary</h2>
<p>Binary logistic regression models probabilities as:</p>
<p>$$P(G=1|X=x) = \frac{\exp(\beta^T x)}{1 + \exp(\beta^T x)} = \frac{1}{1 + \exp(-\beta^T x)}$$</p>
<p>$$P(G=0|X=x) = \frac{1}{1 + \exp(\beta^T x)}$$</p>
<p>The <strong>log-odds</strong> (also called logit) is:</p>
<p>$$\log\left(\frac{P(G=1|x)}{P(G=0|x)}\right) = \beta^T x$$</p>
<p>This is linear in x! So the decision boundary ‚Äî where both classes are equally likely ‚Äî is:</p>
<p>$${x : \beta^T x = 0}$$</p>
<hr>
<h2 id="linear-probability-model-and-why-it-fails">Linear Probability Model (and Why It Fails)</h2>
<h3 id="the-approach">The Approach</h3>
<p>One simple idea: encode classes as numbers (0/1 for binary, or indicator matrix Y for multiclass) and just run linear regression!</p>
<p>$$\hat{\beta} = (X^TX)^{-1}X^TY$$
$$\hat{Y} = X\hat{\beta}$$</p>
<h3 id="problems-with-this-approach">Problems with This Approach</h3>
<ol>
<li><p><strong>Predictions outside [0,1]</strong>: Linear regression can predict negative probabilities or probabilities greater than 1 ‚Äî nonsense for probabilities!</p>
</li>
<li><p><strong>Class masking</strong>: With multiple classes and few features, one class can be &quot;dominated&quot; everywhere. Imagine three classes where class 2 always has lower predicted values than classes 1 or 3 ‚Äî the model will never predict class 2!</p>
</li>
</ol>
<p><strong>Takeaway</strong>: Use methods designed for classification, not regression hacks.</p>
<hr>
<h2 id="linear-discriminant-analysis-lda">Linear Discriminant Analysis (LDA)</h2>
<p>LDA is one of the oldest and most elegant classification methods. It takes a generative approach: model how the data is <em>generated</em> for each class, then use Bayes&#39; theorem to classify.</p>
<h3 id="the-generative-model">The Generative Model</h3>
<p>Assume that within each class k, the data follows a multivariate normal distribution:</p>
<p>$$X | G=k \sim N(\mu_k, \Sigma)$$</p>
<p><strong>Key assumption for LDA</strong>: All classes share the same covariance matrix Œ£ (but have different means Œº_k).</p>
<h3 id="using-bayes-theorem">Using Bayes&#39; Theorem</h3>
<p>Once we model how data is generated, we can compute:</p>
<p>$$P(G=k | X=x) = \frac{f_k(x) \times \pi_k}{\sum_l f_l(x) \times \pi_l}$$</p>
<p>Where:</p>
<ul>
<li>$\pi_k$ = prior probability of class k (how common is this class?)</li>
<li>$f_k(x)$ = class-conditional density (how likely is x given class k?)</li>
</ul>
<h3 id="the-discriminant-function">The Discriminant Function</h3>
<p>For a multivariate normal:</p>
<p>$$f_k(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}} \exp\left{-\frac{1}{2}(x - \mu_k)^T\Sigma^{-1}(x - \mu_k)\right}$$</p>
<p>Taking the log and simplifying (using the common Œ£ assumption), we get the <strong>linear discriminant function</strong>:</p>
<p>$$\delta_k(x) = x^T\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + \log\pi_k$$</p>
<p>This is linear in x, hence &quot;Linear&quot; Discriminant Analysis.</p>
<h3 id="decision-boundary">Decision Boundary</h3>
<p>The log-odds between classes k and l is:</p>
<p>$$\log\frac{P(G=k|x)}{P(G=l|x)} = C + x^T\Sigma^{-1}(\mu_k - \mu_l)$$</p>
<p>Linear in x! The constant terms combine nicely because Œ£ is shared across classes.</p>
<h3 id="estimation-in-practice">Estimation in Practice</h3>
<p>We estimate the parameters from training data:</p>
<ul>
<li><strong>Prior</strong>: $\hat{\pi}_k = N_k / N$ (proportion of each class)</li>
<li><strong>Mean</strong>: $\hat{\mu}<em>k = \frac{1}{N_k}\sum</em>{i \in \text{class } k} x_i$ (class centroid)</li>
<li><strong>Covariance</strong>: $\hat{\Sigma} = \frac{1}{N}\sum_k\sum_{i \in \text{class } k}(x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T$ (pooled covariance)</li>
</ul>
<hr>
<h2 id="quadratic-discriminant-analysis-qda">Quadratic Discriminant Analysis (QDA)</h2>
<h3 id="relaxing-the-equal-covariance-assumption">Relaxing the Equal Covariance Assumption</h3>
<p>What if each class has its own covariance structure? QDA allows:</p>
<p>$$X | G=k \sim N(\mu_k, \Sigma_k)$$</p>
<h3 id="quadratic-discriminant-function">Quadratic Discriminant Function</h3>
<p>Now the discriminant becomes:</p>
<p>$$\delta_k(x) = -\frac{1}{2}\log|\Sigma_k| - \frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k) + \log\pi_k$$</p>
<p>This is <strong>quadratic</strong> in x ‚Äî the decision boundaries are curved (ellipses, parabolas, hyperbolas).</p>
<h3 id="trade-offs-lda-vs-qda">Trade-offs: LDA vs QDA</h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>LDA</th>
<th>QDA</th>
</tr>
</thead>
<tbody><tr>
<td>Boundaries</td>
<td>Linear</td>
<td>Quadratic (curved)</td>
</tr>
<tr>
<td>Parameters</td>
<td>~Kp means + p(p+1)/2 covariance</td>
<td>~Kp means + Kp(p+1)/2 covariances</td>
</tr>
<tr>
<td>Flexibility</td>
<td>Less flexible</td>
<td>More flexible</td>
</tr>
<tr>
<td>Variance</td>
<td>Lower (fewer parameters)</td>
<td>Higher (more parameters)</td>
</tr>
<tr>
<td>Best when</td>
<td>Classes have similar spread</td>
<td>Classes have different shapes/orientations</td>
</tr>
</tbody></table>
<h3 id="regularized-discriminant-analysis">Regularized Discriminant Analysis</h3>
<p>A compromise between LDA and QDA:</p>
<p>$$\hat{\Sigma}_k(\alpha) = \alpha\Sigma_k + (1-\alpha)\Sigma$$</p>
<ul>
<li>Œ± = 0: LDA (shared covariance)</li>
<li>Œ± = 1: QDA (class-specific covariance)</li>
<li>0 &lt; Œ± &lt; 1: Smooth interpolation</li>
</ul>
<p>Choose Œ± via cross-validation.</p>
<hr>
<h2 id="naive-bayes-classifier">Naive Bayes Classifier</h2>
<h3 id="the-independence-assumption">The Independence Assumption</h3>
<p>Naive Bayes makes a strong (and usually wrong!) assumption: given the class, all features are <strong>conditionally independent</strong>:</p>
<p>$$f_k(x) = \prod_{j=1}^p f_{kj}(x_j)$$</p>
<h3 id="why-naive">Why &quot;Naive&quot;?</h3>
<p>This assumption rarely holds in practice. If predicting whether an email is spam, the presence of &quot;free&quot; and &quot;money&quot; are probably correlated. But Naive Bayes ignores this.</p>
<h3 id="why-does-it-still-work">Why Does It Still Work?</h3>
<p>Despite the wrong assumption, Naive Bayes often works well because:</p>
<ol>
<li>We only need to get the <em>ranking</em> of classes right, not exact probabilities</li>
<li>The errors from the independence assumption may cancel out</li>
<li>It&#39;s extremely fast and scales to high dimensions</li>
</ol>
<h3 id="the-log-odds">The Log-Odds</h3>
<p>$$\log\frac{P(G=k|x)}{P(G=l|x)} = \log\frac{\pi_k}{\pi_l} + \sum_{j=1}^p\log\frac{f_{kj}(x_j)}{f_{lj}(x_j)}$$</p>
<p>Each feature contributes independently to the log-odds ‚Äî simple and interpretable!</p>
<hr>
<h2 id="logistic-regression">Logistic Regression</h2>
<p>Logistic regression is the workhorse of classification. Unlike LDA (which models P(X|G)), logistic regression directly models P(G|X).</p>
<h3 id="the-model">The Model</h3>
<p>For binary classification:</p>
<p>$$P(G=1|X=x) = \frac{\exp(\beta^T x)}{1 + \exp(\beta^T x)} = \sigma(\beta^T x)$$</p>
<p>$$P(G=0|X=x) = \frac{1}{1 + \exp(\beta^T x)} = 1 - \sigma(\beta^T x)$$</p>
<p>Where œÉ(¬∑) is the <strong>sigmoid function</strong> ‚Äî it squashes any real number into (0,1).</p>
<h3 id="why-the-sigmoid">Why the Sigmoid?</h3>
<p>The sigmoid ensures probabilities are always between 0 and 1, and the log-odds is linear:</p>
<p>$$\log\frac{P(G=1|x)}{P(G=0|x)} = \beta^T x$$</p>
<h3 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h3>
<p>We find Œ≤ by maximizing the probability of the observed labels. The log-likelihood is:</p>
<p>$$\ell(\beta) = \sum_{i=1}^N \left[y_i \log p(x_i, \beta) + (1-y_i)\log(1-p(x_i, \beta))\right]$$</p>
<p>Or equivalently:</p>
<p>$$\ell(\beta) = \sum_{i=1}^N \left[y_i(\beta^T x_i) - \log(1 + \exp(\beta^T x_i))\right]$$</p>
<h3 id="finding-the-maximum">Finding the Maximum</h3>
<p>Taking the derivative (the <strong>score function</strong>):</p>
<p>$$\frac{\partial \ell}{\partial \beta} = \sum_{i=1}^N x_i(y_i - p(x_i, \beta)) = X^T(y - p)$$</p>
<p>Setting this to zero: the predicted probabilities should &quot;balance&quot; with the actual labels.</p>
<h3 id="optimization-newton-raphson--irls">Optimization: Newton-Raphson / IRLS</h3>
<p>The log-likelihood is non-linear in Œ≤ ‚Äî no closed-form solution. We use iterative methods.</p>
<p>The <strong>Hessian</strong> (second derivative) is:</p>
<p>$$\frac{\partial^2 \ell}{\partial \beta \partial \beta^T} = -\sum_{i=1}^N x_i x_i^T p(x_i, \beta)(1-p(x_i, \beta)) = -X^T W X$$</p>
<p>Where W is diagonal with $W_{ii} = p_i(1-p_i)$.</p>
<p><strong>Good news</strong>: The Hessian is negative definite, so the log-likelihood is <strong>concave</strong> ‚Äî there&#39;s a unique global maximum!</p>
<p>The algorithm is called <strong>Iteratively Reweighted Least Squares (IRLS)</strong> because each iteration looks like a weighted least squares problem.</p>
<h3 id="measuring-goodness-of-fit-deviance">Measuring Goodness of Fit: Deviance</h3>
<p><strong>Deviance</strong> measures how far our model is from a &quot;perfect&quot; model:</p>
<p>$$\text{Deviance} = -2(\log L_M - \log L_S)$$</p>
<p>Where:</p>
<ul>
<li>$L_M$ = likelihood of our model</li>
<li>$L_S$ = likelihood of the saturated model (perfect fit)</li>
</ul>
<p>To compare models, look at the change in deviance (larger drops = better improvement).</p>
<h3 id="regularization">Regularization</h3>
<p>Just like in regression, we can add penalties:</p>
<ul>
<li><strong>L2 penalty</strong> (Ridge): Shrinks coefficients, helps with correlated predictors</li>
<li><strong>L1 penalty</strong> (Lasso): Shrinks some coefficients to exactly zero</li>
</ul>
<p>$$\text{Maximize: } \ell(\beta) - \lambda\sum_{j=1}^p |\beta_j|$$</p>
<p>Note: Don&#39;t penalize the intercept!</p>
<hr>
<h2 id="lda-vs-logistic-regression">LDA vs. Logistic Regression</h2>
<p>Both produce linear decision boundaries, but they differ in important ways:</p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>LDA</th>
<th>Logistic Regression</th>
</tr>
</thead>
<tbody><tr>
<td>Approach</td>
<td>Generative (models P(X|G))</td>
<td>Discriminative (models P(G|X))</td>
</tr>
<tr>
<td>Likelihood</td>
<td>Full joint likelihood</td>
<td>Conditional likelihood</td>
</tr>
<tr>
<td>Assumptions</td>
<td>Normal class distributions, equal covariances</td>
<td>Just that log-odds is linear</td>
</tr>
<tr>
<td>Efficiency</td>
<td>More efficient if assumptions hold</td>
<td>More robust to violations</td>
</tr>
<tr>
<td>Outliers</td>
<td>Sensitive (Gaussian assumption)</td>
<td>More robust</td>
</tr>
<tr>
<td>When to use</td>
<td>Small samples, well-behaved data</td>
<td>Large samples, suspect non-normality</td>
</tr>
</tbody></table>
<p><strong>Rule of thumb</strong>: If you have small samples and trust the Gaussian assumption, LDA can be more efficient. Otherwise, logistic regression is safer.</p>
<hr>
<h2 id="perceptron-learning-algorithm">Perceptron Learning Algorithm</h2>
<p>The perceptron is a historically important algorithm (precursor to neural networks) that finds a separating hyperplane.</p>
<h3 id="the-objective">The Objective</h3>
<p>Minimize the total distance of misclassified points to the decision boundary:</p>
<p>$$D(\beta) = -\sum_{i \in \text{misclassified}} y_i(x_i^T\beta)$$</p>
<h3 id="algorithm">Algorithm</h3>
<p>Use stochastic gradient descent:</p>
<ol>
<li>Initialize Œ≤</li>
<li>For each misclassified point i:<ul>
<li>Update: $\beta \leftarrow \beta + \eta \cdot y_i \cdot x_i$</li>
</ul>
</li>
<li>Repeat until convergence</li>
</ol>
<h3 id="limitations">Limitations</h3>
<ul>
<li><strong>Multiple solutions</strong>: If data is separable, many hyperplanes work. The final answer depends on initialization and order of updates.</li>
<li><strong>No convergence guarantee</strong>: If data is NOT separable, the algorithm never converges ‚Äî it just bounces around forever.</li>
</ul>
<p>This motivates the <strong>Support Vector Machine</strong>, which finds the unique &quot;best&quot; separating hyperplane.</p>
<hr>
<h2 id="maximum-margin-classifiers">Maximum Margin Classifiers</h2>
<h3 id="the-margin-idea">The Margin Idea</h3>
<p>If data is linearly separable, many hyperplanes separate the classes. Which is &quot;best&quot;?</p>
<p><strong>Intuition</strong>: Choose the hyperplane with the largest <strong>margin</strong> ‚Äî the distance from the hyperplane to the nearest points of either class. This gives the most &quot;room for error&quot; on new data.</p>
<h3 id="mathematical-formulation">Mathematical Formulation</h3>
<p>We want to maximize the margin M:</p>
<p>$$\max_{\beta, |\beta|=1} M \quad \text{subject to} \quad y_i(x_i^T\beta) \geq M \quad \forall i$$</p>
<p><strong>Reformulation</strong> (dropping the norm constraint):</p>
<p>$$\min_\beta \frac{1}{2}|\beta|^2 \quad \text{subject to} \quad y_i(x_i^T\beta) \geq 1 \quad \forall i$$</p>
<p>This is a <strong>quadratic program</strong> ‚Äî convex optimization with a unique solution.</p>
<h3 id="lagrangian-formulation">Lagrangian Formulation</h3>
<p>$$L = \frac{1}{2}|\beta|^2 - \sum_{i=1}^N \alpha_i\left[y_i(x_i^T\beta) - 1\right]$$</p>
<p>Taking the derivative with respect to Œ≤:</p>
<p>$$\beta = \sum_{i=1}^N \alpha_i y_i x_i$$</p>
<p><strong>Key insight</strong>: The solution is a linear combination of the training points! But only points where the constraint is active (on the margin boundary) contribute ‚Äî these are called <strong>support vectors</strong>.</p>
<p>For most points, $\alpha_i = 0$. Only a few points determine the decision boundary. This makes SVMs efficient and robust.</p>
<hr>
<h2 id="summary-choosing-a-classification-method">Summary: Choosing a Classification Method</h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Best For</th>
<th>Strengths</th>
<th>Weaknesses</th>
</tr>
</thead>
<tbody><tr>
<td><strong>LDA</strong></td>
<td>Small samples, Gaussian data</td>
<td>Efficient, stable</td>
<td>Assumes normality</td>
</tr>
<tr>
<td><strong>QDA</strong></td>
<td>Different class shapes</td>
<td>Flexible boundaries</td>
<td>More parameters</td>
</tr>
<tr>
<td><strong>Naive Bayes</strong></td>
<td>High dimensions, text data</td>
<td>Fast, scales well</td>
<td>Wrong independence assumption</td>
</tr>
<tr>
<td><strong>Logistic Regression</strong></td>
<td>General purpose</td>
<td>Robust, probabilistic</td>
<td>Requires tuning</td>
</tr>
<tr>
<td><strong>Perceptron</strong></td>
<td>Historical interest</td>
<td>Simple</td>
<td>Unstable, no probabilities</td>
</tr>
<tr>
<td><strong>SVM (Max Margin)</strong></td>
<td>Separable data, small samples</td>
<td>Unique solution, sparse</td>
<td>Hard to interpret</td>
</tr>
</tbody></table>
<p>For most practical applications, <strong>logistic regression</strong> is a great starting point. For high-dimensional text classification, <strong>Naive Bayes</strong> is surprisingly effective. For small samples with well-behaved data, <strong>LDA</strong> is worth trying.</p>

        </article>
        <nav class="page-navigation">
        <a href="eslr-01-regression.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Linear Regression</span>
        </a>
        <a href="eslr-03-kernel-methods.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Kernel Methods</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>