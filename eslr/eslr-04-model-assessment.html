<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Model Assessment and Selection | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="eslr-04-model-assessment.html" class="active">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../probml/probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="../probml/probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="../probml/probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="../probml/probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="../probml/probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="../probml/probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="../probml/probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="../probml/probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="../probml/probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="../probml/probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">ESLR</a>
          </div>
          <h1 class="page-title">Model Assessment and Selection</h1>
          <div class="page-meta"><span class="tag">ESLR</span></div>
        </header>
        <article class="content">
          <h1 id="model-assessment-and-selection">Model Assessment and Selection</h1>
<p>This chapter addresses one of the most important questions in machine learning: <strong>How do we know if our model is any good?</strong> Training error can be misleading, so we need principled ways to estimate how well our model will perform on new, unseen data.</p>
<h2 id="the-big-picture">The Big Picture</h2>
<p>When building models, we face a fundamental tension:</p>
<ul>
<li><strong>Simple models</strong> may miss important patterns (underfitting)</li>
<li><strong>Complex models</strong> may memorize noise (overfitting)</li>
</ul>
<p>The goal is to find the sweet spot ‚Äî a model complex enough to capture real patterns but simple enough to generalize to new data.</p>
<hr>
<h2 id="understanding-generalization">Understanding Generalization</h2>
<h3 id="what-we-really-care-about">What We Really Care About</h3>
<p>We don&#39;t just want a model that fits our training data well. We want a model that predicts well on <strong>new data</strong> it hasn&#39;t seen before. This is called <strong>generalization</strong>.</p>
<h3 id="expected-test-error">Expected Test Error</h3>
<p>The quantity we want to minimize:</p>
<p>$$\text{Err}_T = E[L(Y, \hat{f}(X)) | T]$$</p>
<p>Where:</p>
<ul>
<li><strong>T</strong> = the training set used to build the model</li>
<li><strong>L</strong> = a loss function measuring prediction error</li>
<li>The expectation is over new (X, Y) pairs</li>
</ul>
<h3 id="common-loss-functions">Common Loss Functions</h3>
<p><strong>For Regression:</strong></p>
<ul>
<li><strong>Squared Error</strong>: $L(y, \hat{f}(x)) = (y - \hat{f}(x))^2$<ul>
<li>Most common; penalizes large errors heavily</li>
</ul>
</li>
<li><strong>Absolute Error</strong>: $L(y, \hat{f}(x)) = |y - \hat{f}(x)|$<ul>
<li>More robust to outliers</li>
</ul>
</li>
</ul>
<p><strong>For Classification:</strong></p>
<ul>
<li><strong>0-1 Loss</strong>: $L(y, \hat{G}(x)) = I(y \neq \hat{G}(x))$<ul>
<li>Simply counts misclassifications</li>
</ul>
</li>
<li><strong>Deviance (Log-loss)</strong>: $L(y, \hat{p}(x)) = -2\log\hat{p}(x)$<ul>
<li>Penalizes confident wrong predictions heavily</li>
</ul>
</li>
</ul>
<hr>
<h2 id="the-bias-variance-tradeoff">The Bias-Variance Tradeoff</h2>
<p>This is perhaps the most important concept in all of machine learning!</p>
<h3 id="decomposing-prediction-error">Decomposing Prediction Error</h3>
<p>For squared error loss, we can decompose the expected prediction error at a point $x_0$:</p>
<p>$$\text{Err}(x_0) = E[(Y - \hat{f}(x_0))^2]$$</p>
<p>Assuming $Y = f(X) + \epsilon$ where $E[\epsilon] = 0$ and $\text{Var}(\epsilon) = \sigma^2_\epsilon$:</p>
<p>$$\text{Err}(x_0) = \underbrace{\sigma^2_\epsilon}<em>{\text{Irreducible}} + \underbrace{[E[\hat{f}(x_0)] - f(x_0)]^2}</em>{\text{Bias}^2} + \underbrace{E[(\hat{f}(x_0) - E[\hat{f}(x_0)])^2]}_{\text{Variance}}$$</p>
<h3 id="what-each-term-means">What Each Term Means</h3>
<p><strong>Irreducible Error (œÉ¬≤Œµ)</strong></p>
<ul>
<li>The inherent randomness in Y that no model can predict</li>
<li>Even with infinite data and a perfect model, you can&#39;t beat this</li>
<li>Sets the floor for prediction error</li>
</ul>
<p><strong>Bias¬≤</strong></p>
<ul>
<li>How far off is our model <em>on average</em>?</li>
<li>Measures systematic error ‚Äî does the model consistently over or underpredict?</li>
<li>Simple models tend to have high bias (they make strong assumptions that may be wrong)</li>
</ul>
<p><strong>Variance</strong></p>
<ul>
<li>How much does our model <em>fluctuate</em> across different training sets?</li>
<li>If you trained on different samples, how different would your predictions be?</li>
<li>Complex models tend to have high variance (they&#39;re sensitive to specific training data)</li>
</ul>
<h3 id="the-tradeoff-visualized">The Tradeoff Visualized</h3>
<pre><code>Error
  ‚îÇ    ‚ï≤                      ‚ï±
  ‚îÇ     ‚ï≤    Total Error    ‚ï±
  ‚îÇ      ‚ï≤                 ‚ï±
  ‚îÇ       ‚ï≤_______________‚ï±
  ‚îÇ        \             /
  ‚îÇ    Bias¬≤ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ      
  ‚îÇ                ‚ï≤   ‚ï± Variance
  ‚îÇ                 ‚ï≤ ‚ï±
  ‚îÇ                  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Model Complexity
       Simple            Complex
</code></pre>
<ul>
<li><strong>Simple models</strong>: High bias, low variance (underfitting)</li>
<li><strong>Complex models</strong>: Low bias, high variance (overfitting)</li>
<li><strong>Optimal</strong>: Balance that minimizes total error</li>
</ul>
<h3 id="practical-implications">Practical Implications</h3>
<ol>
<li><p><strong>U-shaped test error curve</strong>: As complexity increases, test error first decreases (reducing bias) then increases (increasing variance)</p>
</li>
<li><p><strong>Training error is misleading</strong>: It always decreases with complexity ‚Äî it can&#39;t detect overfitting!</p>
</li>
<li><p><strong>More data helps variance</strong>: With more training data, variance decreases (we get more reliable estimates)</p>
</li>
<li><p><strong>Signal-to-noise ratio matters</strong>: When noise is high, simpler models often win</p>
</li>
</ol>
<hr>
<h2 id="bias-variance-for-linear-models">Bias-Variance for Linear Models</h2>
<p>For linear regression with p predictors:</p>
<p><strong>Variance</strong> $\propto p$</p>
<ul>
<li>More parameters = more things to estimate = more variance</li>
</ul>
<p><strong>Bias¬≤ = Model Bias¬≤ + Estimation Bias¬≤</strong></p>
<ul>
<li><strong>Model Bias</strong>: Difference between the true function and the best linear approximation</li>
<li><strong>Estimation Bias</strong>: Difference between what we estimate and the best linear approximation</li>
</ul>
<p>For <strong>OLS</strong>: Estimation bias is zero (OLS gives unbiased estimates), but variance can be high.</p>
<p>For <strong>Ridge Regression</strong>: We <em>introduce</em> estimation bias deliberately to reduce variance. If the variance reduction outweighs the bias increase, we get lower overall error!</p>
<hr>
<h2 id="why-training-error-is-optimistic">Why Training Error is Optimistic</h2>
<h3 id="the-fundamental-problem">The Fundamental Problem</h3>
<p>Training error uses the same data to fit the model and evaluate it. The model is specifically tuned to do well on this data, so training error underestimates how the model will perform on new data.</p>
<h3 id="quantifying-the-optimism">Quantifying the Optimism</h3>
<p>Define:</p>
<ul>
<li>$\bar{\text{err}}$ = average training error</li>
<li>$\text{Err}_{in}$ = expected error on training points (but with new Y values)</li>
</ul>
<p>The <strong>optimism</strong> is:</p>
<p>$$\text{op} = \text{Err}<em>{in} - \bar{\text{err}} = \frac{2}{N}\sum</em>{i=1}^N \text{Cov}(y_i, \hat{y}_i)$$</p>
<p><strong>Interpretation</strong>: Optimism measures how much each training label influences its own prediction. The more the model &quot;memorizes&quot; training labels, the more optimistic training error becomes.</p>
<h3 id="what-affects-optimism">What Affects Optimism?</h3>
<ul>
<li><strong>More parameters</strong> ‚Üí More optimism (more opportunity to fit training data specifically)</li>
<li><strong>More training samples</strong> ‚Üí Less optimism per point (each point has less influence)</li>
</ul>
<hr>
<h2 id="model-selection-vs-model-assessment">Model Selection vs. Model Assessment</h2>
<p>These are related but distinct tasks:</p>
<p><strong>Model Selection</strong></p>
<ul>
<li>Goal: Choose the best model from a set of candidates</li>
<li>Question: Which model will generalize best?</li>
<li>Uses validation data</li>
</ul>
<p><strong>Model Assessment</strong></p>
<ul>
<li>Goal: Estimate how well the chosen model performs</li>
<li>Question: What&#39;s the expected error on new data?</li>
<li>Uses test data (must be kept separate from selection!)</li>
</ul>
<h3 id="the-standard-split">The Standard Split</h3>
<table>
<thead>
<tr>
<th>Set</th>
<th>Purpose</th>
<th>Typical Size</th>
</tr>
</thead>
<tbody><tr>
<td>Training</td>
<td>Fit models</td>
<td>50-60%</td>
</tr>
<tr>
<td>Validation</td>
<td>Select best model</td>
<td>20-25%</td>
</tr>
<tr>
<td>Test</td>
<td>Estimate final performance</td>
<td>20-25%</td>
</tr>
</tbody></table>
<p><strong>Critical rule</strong>: Never use test data for model selection! This leads to optimistic error estimates.</p>
<hr>
<h2 id="analytical-estimates-of-prediction-error">Analytical Estimates of Prediction Error</h2>
<p>When data is scarce, we&#39;d rather not set aside a validation set. These methods estimate test error analytically.</p>
<h3 id="cp-statistic-mallows-cp">Cp Statistic (Mallow&#39;s Cp)</h3>
<p>$$C_p = \bar{\text{err}} + \frac{2p}{N}\hat{\sigma}^2_\epsilon$$</p>
<p>Where p = effective number of parameters.</p>
<p><strong>Interpretation</strong>: Start with training error, add a penalty for complexity. The penalty estimates the optimism.</p>
<h3 id="aic-akaike-information-criterion">AIC (Akaike Information Criterion)</h3>
<p>$$\text{AIC} = -\frac{2}{N}\ell + \frac{2p}{N}$$</p>
<p>Where $\ell$ = log-likelihood of the model.</p>
<p><strong>How to use</strong>: Fit multiple models, compute AIC for each, choose the one with lowest AIC.</p>
<p><strong>Properties</strong>:</p>
<ul>
<li>Derived from information theory (minimizing KL divergence)</li>
<li>Tends to select slightly complex models</li>
<li>Works for any model with a likelihood (not just linear)</li>
</ul>
<h3 id="bic-bayesian-information-criterion">BIC (Bayesian Information Criterion)</h3>
<p>$$\text{BIC} = -\frac{2}{N}\ell + \frac{\log N}{N} \times p$$</p>
<p><strong>Difference from AIC</strong>: BIC penalizes complexity more heavily (especially for large N).</p>
<p><strong>Properties</strong>:</p>
<ul>
<li>Derived from Bayesian model comparison</li>
<li><strong>Consistent</strong>: Will select the true model as N ‚Üí ‚àû (if it&#39;s among the candidates)</li>
<li>Tends to select simpler models than AIC</li>
</ul>
<h3 id="aic-vs-bic-when-to-use-which">AIC vs BIC: When to Use Which?</h3>
<table>
<thead>
<tr>
<th>Criterion</th>
<th>Penalty</th>
<th>Tends to Select</th>
<th>Best When</th>
</tr>
</thead>
<tbody><tr>
<td>AIC</td>
<td>2p/N</td>
<td>Larger models</td>
<td>Prediction is goal</td>
</tr>
<tr>
<td>BIC</td>
<td>(log N)p/N</td>
<td>Smaller models</td>
<td>Want to find &quot;true&quot; model</td>
</tr>
</tbody></table>
<h3 id="effective-number-of-parameters">Effective Number of Parameters</h3>
<p>For complex models, &quot;number of parameters&quot; isn&#39;t straightforward.</p>
<p><strong>General definition</strong>: For any smoother $\hat{y} = Sy$, the effective degrees of freedom is:</p>
<p>$$\text{df} = \text{trace}(S)$$</p>
<p><strong>For OLS</strong>: df = p (the number of predictors + intercept)</p>
<p><strong>For Ridge Regression</strong>:</p>
<p>$$\text{df}(\lambda) = \sum_{j=1}^p \frac{d_j^2}{d_j^2 + \lambda}$$</p>
<p>Where $d_j$ are the eigenvalues of $X^TX$. As Œª increases, effective df decreases.</p>
<hr>
<h2 id="vc-dimension">VC Dimension</h2>
<p>For non-linear models, counting parameters is inadequate. The <strong>VC (Vapnik-Chervonenkis) dimension</strong> provides a general measure of model complexity.</p>
<h3 id="the-concept-of-shattering">The Concept of Shattering</h3>
<p>A set of points is <strong>shattered</strong> by a class of functions if, for every possible labeling of the points, some function in the class can achieve that labeling perfectly.</p>
<p><strong>Example</strong>: 3 points in 2D</p>
<ul>
<li>Linear classifiers can separate any labeling of 3 non-collinear points</li>
<li>So linear classifiers in 2D can shatter 3 points</li>
<li>But they cannot shatter 4 points (XOR configuration is impossible)</li>
</ul>
<h3 id="vc-dimension-defined">VC Dimension Defined</h3>
<p>The VC dimension of a class of functions is the <strong>largest number of points</strong> that can be shattered.</p>
<p><strong>Examples</strong>:</p>
<ul>
<li>Linear classifiers in d dimensions: VC = d + 1</li>
<li>Polynomials of degree k: VC = k + 1</li>
<li>Neural networks: depends on architecture (roughly proportional to number of parameters)</li>
</ul>
<h3 id="structural-risk-minimization">Structural Risk Minimization</h3>
<p>The VC dimension gives bounds on generalization error:</p>
<p>$$\text{Test Error} \leq \text{Training Error} + \sqrt{\frac{h(\log(2N/h) + 1) - \log(\eta/4)}{N}}$$</p>
<p>Where h = VC dimension, N = sample size, Œ∑ = confidence level.</p>
<p><strong>Implication</strong>: Models with smaller VC dimension have tighter bounds ‚Äî they&#39;re more likely to generalize well.</p>
<hr>
<h2 id="cross-validation">Cross-Validation</h2>
<p>When you can&#39;t afford separate validation data (or want to use all data efficiently), <strong>cross-validation</strong> provides a powerful alternative.</p>
<h3 id="k-fold-cross-validation">K-Fold Cross-Validation</h3>
<ol>
<li>Randomly divide data into K equal parts (folds)</li>
<li>For k = 1 to K:<ul>
<li>Train on all folds except k</li>
<li>Test on fold k</li>
<li>Record error</li>
</ul>
</li>
<li>Average the K test errors</li>
</ol>
<p>$$\text{CV}<em>K = \frac{1}{N}\sum</em>{i=1}^N L(y_i, \hat{f}^{-\kappa(i)}(x_i))$$</p>
<p>Where $\hat{f}^{-\kappa(i)}$ means the model trained without observation i&#39;s fold.</p>
<h3 id="common-choices-of-k">Common Choices of K</h3>
<p><strong>K = N (Leave-One-Out CV)</strong></p>
<ul>
<li>Train on N-1 points, test on 1 point, repeat N times</li>
<li>Nearly unbiased (training set almost full size)</li>
<li>High variance (test sets overlap heavily)</li>
<li>Computationally expensive (unless there&#39;s a shortcut formula)</li>
</ul>
<p><strong>K = 5 or 10 (The Sweet Spot)</strong></p>
<ul>
<li>Good balance of bias and variance</li>
<li>Computationally reasonable</li>
<li>Empirically shown to work well</li>
<li>Most commonly recommended</li>
</ul>
<p><strong>K = 2 (Repeated Random Splits)</strong></p>
<ul>
<li>High variance</li>
<li>Not commonly used except with many repetitions</li>
</ul>
<h3 id="bias-variance-in-cross-validation">Bias-Variance in Cross-Validation</h3>
<table>
<thead>
<tr>
<th>K</th>
<th>Training Size</th>
<th>Bias</th>
<th>Variance</th>
</tr>
</thead>
<tbody><tr>
<td>Small (e.g., 2)</td>
<td>~N/2</td>
<td>High (small training sets)</td>
<td>Lower</td>
</tr>
<tr>
<td>Large (e.g., N)</td>
<td>N-1</td>
<td>Low (big training sets)</td>
<td>High (overlapping test sets)</td>
</tr>
<tr>
<td>5-10</td>
<td>~80-90% of N</td>
<td>Moderate</td>
<td>Moderate</td>
</tr>
</tbody></table>
<hr>
<h2 id="bootstrap-methods">Bootstrap Methods</h2>
<p>The bootstrap is a general technique for estimating uncertainty by resampling.</p>
<h3 id="the-bootstrap-idea">The Bootstrap Idea</h3>
<ol>
<li>Draw B samples of size N <strong>with replacement</strong> from training data</li>
<li>Fit a model to each bootstrap sample</li>
<li>Use the variation across fits to estimate uncertainty</li>
</ol>
<h3 id="key-property">Key Property</h3>
<p>Each bootstrap sample contains about 63.2% unique observations:</p>
<p>$$P(\text{observation } i \text{ is in bootstrap sample}) = 1 - \left(1 - \frac{1}{N}\right)^N \approx 1 - e^{-1} \approx 0.632$$</p>
<h3 id="estimating-prediction-error">Estimating Prediction Error</h3>
<p><strong>Naive approach</strong>: Average error on training points across bootstrap samples.</p>
<ul>
<li><strong>Problem</strong>: Observations often appear in both training and test ‚Äî leakage!</li>
</ul>
<p><strong>Out-of-Bag (OOB) error</strong>: For each observation, only use predictions from models where that observation wasn&#39;t in the training sample.</p>
<p>$$\hat{\text{Err}}^{\text{OOB}} = \frac{1}{N}\sum_{i=1}^N \frac{1}{|C^{-i}|}\sum_{b \in C^{-i}} L(y_i, \hat{f}^{*b}(x_i))$$</p>
<p>Where $C^{-i}$ = bootstrap samples not containing observation i.</p>
<p><strong>Properties of OOB error</strong>:</p>
<ul>
<li>No leakage ‚Äî honest estimate</li>
<li>Similar to leave-one-out CV</li>
<li>Free when using bagging/random forests</li>
</ul>
<h3 id="the-632-bootstrap-estimator">The .632 Bootstrap Estimator</h3>
<p>OOB error can be slightly pessimistic (training sets are only ~63% of full data). A correction:</p>
<p>$$\hat{\text{Err}}^{.632} = 0.368 \times \bar{\text{err}} + 0.632 \times \hat{\text{Err}}^{\text{OOB}}$$</p>
<p>This averages training error (too optimistic) with OOB error (slightly pessimistic).</p>
<hr>
<h2 id="summary-choosing-an-estimation-method">Summary: Choosing an Estimation Method</h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>When to Use</th>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Train/Val/Test Split</strong></td>
<td>Lots of data</td>
<td>Simple, unbiased</td>
<td>Wastes data</td>
</tr>
<tr>
<td><strong>AIC/BIC</strong></td>
<td>Comparing similar models, need speed</td>
<td>Fast, no retraining</td>
<td>Approximate, limited scope</td>
</tr>
<tr>
<td><strong>Cross-Validation</strong></td>
<td>Limited data, want reliability</td>
<td>Uses all data for training AND testing</td>
<td>Computationally expensive</td>
</tr>
<tr>
<td><strong>Bootstrap</strong></td>
<td>Need uncertainty estimates</td>
<td>Versatile, works for any statistic</td>
<td>Can be biased, requires care</td>
</tr>
</tbody></table>
<h3 id="practical-recommendations">Practical Recommendations</h3>
<ol>
<li><strong>If you have lots of data</strong>: Use a held-out test set for final assessment</li>
<li><strong>For model selection with limited data</strong>: Use 5-fold or 10-fold CV</li>
<li><strong>For very small samples</strong>: Use leave-one-out CV or bootstrap</li>
<li><strong>Quick comparisons</strong>: AIC/BIC are fast approximations</li>
<li><strong>Always</strong>: Keep a final test set untouched until the very end!</li>
</ol>
<h3 id="common-pitfalls">Common Pitfalls</h3>
<ul>
<li><strong>Using test data for selection</strong>: Invalidates your error estimate</li>
<li><strong>Ignoring the bias-variance tradeoff</strong>: Don&#39;t just minimize training error!</li>
<li><strong>Forgetting about randomness</strong>: Always think about how results would vary with different data</li>
</ul>

        </article>
        <nav class="page-navigation">
        <a href="eslr-03-kernel-methods.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Kernel Methods</span>
        </a>
        <a href="eslr-08-model-selection.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Model Inference and Averaging</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>