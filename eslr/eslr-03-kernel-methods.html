<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Kernel Methods | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="eslr-03-kernel-methods.html" class="active">Kernel Methods</a></li>
            <li class="nav-item"><a href="eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../general/gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="../general/gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="../general/gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="../general/gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../general/gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="../general/gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="../general/gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="../general/gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="../general/gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../probml/probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="../probml/probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="../probml/probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="../probml/probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="../probml/probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="../probml/probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="../probml/probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="../probml/probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="../probml/probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="../probml/probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">ESLR</a>
          </div>
          <h1 class="page-title">Kernel Methods</h1>
          <div class="page-meta"><span class="tag">ESLR</span></div>
        </header>
        <article class="content">
          <h1 id="kernel-methods">Kernel Methods</h1>
<p>Kernel methods are a powerful family of techniques that enable us to work in high-dimensional (even infinite-dimensional!) feature spaces without explicitly computing the transformations. This chapter covers density estimation, classification using kernels, and radial basis functions.</p>
<h2 id="the-big-picture">The Big Picture</h2>
<p>Many real-world patterns are non-linear. Kernel methods handle this by:</p>
<ol>
<li><strong>Implicitly mapping</strong> data to a higher-dimensional space where patterns become linear</li>
<li><strong>Using local information</strong> ‚Äî nearby points matter more than distant ones</li>
<li><strong>Avoiding the curse of dimensionality</strong> through the &quot;kernel trick&quot;</li>
</ol>
<hr>
<h2 id="kernel-density-estimation">Kernel Density Estimation</h2>
<p>Before classifying data, we often need to estimate the underlying probability distribution. Kernel density estimation (KDE) is a non-parametric way to do this.</p>
<h3 id="the-problem">The Problem</h3>
<p>Given a random sample $[x_1, x_2, ..., x_N]$ drawn from an unknown distribution $f_X(x)$, how do we estimate what $f_X$ looks like?</p>
<h3 id="a-first-attempt-histograms">A First Attempt: Histograms</h3>
<p>The simplest approach is a histogram: divide the space into bins and count how many points fall in each bin. But histograms are bumpy and depend heavily on bin placement.</p>
<h3 id="parzen-kernel-density-estimation">Parzen (Kernel) Density Estimation</h3>
<p>A smoother approach: for any point $x_0$, estimate the density by looking at how many training points are nearby:</p>
<p>$$\hat{f}_X(x_0) = \frac{1}{N\lambda} \times #{x_i \in \text{neighborhood of } x_0}$$</p>
<p>Where Œª is the <strong>bandwidth</strong> ‚Äî the width of the neighborhood.</p>
<p><strong>Problem</strong>: This is still bumpy at the neighborhood boundaries.</p>
<h3 id="gaussian-kernels-for-smooth-estimates">Gaussian Kernels for Smooth Estimates</h3>
<p>Instead of hard boundaries, use a <strong>smooth kernel</strong> that gives more weight to closer points:</p>
<p>$$\hat{f}<em>X(x_0) = \frac{1}{N} \sum</em>{i=1}^N K_\lambda(x_i, x_0)$$</p>
<p>Where the Gaussian kernel is:</p>
<p>$$K_\lambda(x_i, x_0) = \frac{1}{\lambda\sqrt{2\pi}}\exp\left(-\frac{|x_i - x_0|^2}{2\lambda^2}\right)$$</p>
<p><strong>Intuition</strong>: We&#39;re placing a small &quot;bump&quot; (Gaussian) at each data point, then adding them all up. The result is a smooth density estimate.</p>
<h3 id="the-effect-of-bandwidth">The Effect of Bandwidth</h3>
<ul>
<li><strong>Small Œª</strong>: Peaks around each data point, very bumpy (overfitting)</li>
<li><strong>Large Œª</strong>: Very smooth, may miss important features (underfitting)</li>
<li><strong>Just right Œª</strong>: Captures the true density shape</li>
</ul>
<p>Choosing bandwidth is similar to choosing model complexity ‚Äî cross-validation helps!</p>
<h3 id="mathematical-view-convolution">Mathematical View: Convolution</h3>
<p>The kernel density estimate can be viewed as a <strong>convolution</strong> of the empirical distribution (spikes at each data point) with a Gaussian kernel:</p>
<p>$$\hat{f}<em>X(x_0) = \frac{1}{N}\sum</em>{i=1}^N \phi_\lambda(x_0 - x_i)$$</p>
<p>This &quot;smears out&quot; the point masses into a smooth function.</p>
<hr>
<h2 id="kernel-density-classification">Kernel Density Classification</h2>
<p>Now we can use density estimation for classification!</p>
<h3 id="bayes-theorem-for-classification">Bayes&#39; Theorem for Classification</h3>
<p>For class j:</p>
<p>$$P(G=j | X=x_0) \propto \hat{\pi}_j \times \hat{f}_j(x_0)$$</p>
<p>Where:</p>
<ul>
<li>$\hat{\pi}_j$ = estimated prior probability (proportion of class j in training data)</li>
<li>$\hat{f}_j(x_0)$ = kernel density estimate for class j, evaluated at $x_0$</li>
</ul>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>Estimate the density separately for each class</li>
<li>Multiply by the class prior</li>
<li>Classify to the class with highest product</li>
</ol>
<h3 id="a-subtlety-where-density-matters">A Subtlety: Where Density Matters</h3>
<p>Learning separate class densities everywhere can be misleading. Consider:</p>
<ul>
<li>In dense regions (many training points), estimates are reliable</li>
<li>In sparse regions (few training points), estimates are noisy</li>
</ul>
<p><strong>Key insight</strong>: The density estimates only matter near the <strong>decision boundary</strong>. Far from the boundary, one class dominates anyway.</p>
<hr>
<h2 id="naive-bayes-classifier">Naive Bayes Classifier</h2>
<h3 id="when-dimensions-are-high">When Dimensions Are High</h3>
<p>In high dimensions, kernel density estimation struggles ‚Äî you need exponentially more data to fill the space (curse of dimensionality).</p>
<p><strong>Naive Bayes</strong> makes a strong simplifying assumption: given the class, features are <strong>conditionally independent</strong>.</p>
<p>$$f_j(x) = \prod_{p=1}^P f_{jp}(x_p)$$</p>
<h3 id="breaking-down-the-independence-assumption">Breaking Down the Independence Assumption</h3>
<p>Instead of estimating one P-dimensional density (hard), we estimate P one-dimensional densities (easy!).</p>
<p><strong>For continuous features</strong>: Use univariate Gaussian or kernel density estimates for each feature.</p>
<p><strong>For categorical features</strong>: Simply count proportions.</p>
<h3 id="the-log-odds-decomposition">The Log-Odds Decomposition</h3>
<p>$$\log\frac{P(G=k|X)}{P(G=l|X)} = \log\frac{\pi_k}{\pi_l} + \sum_{p=1}^P \log\frac{f_{kp}(x_p)}{f_{lp}(x_p)}$$</p>
<p>Each feature contributes additively to the log-odds ‚Äî simple and interpretable!</p>
<h3 id="why-naive-works">Why &quot;Naive&quot; Works</h3>
<p>The independence assumption is almost always wrong. Yet Naive Bayes often performs well because:</p>
<ol>
<li><strong>We only need rankings</strong>: For classification, we just need to rank classes correctly, not get exact probabilities</li>
<li><strong>Errors may cancel</strong>: Overestimating some terms and underestimating others can balance out</li>
<li><strong>Robustness</strong>: Fewer parameters means less overfitting</li>
</ol>
<p><strong>Best applications</strong>: Text classification (spam filtering, sentiment analysis), where features (words) are high-dimensional.</p>
<hr>
<h2 id="radial-basis-functions-rbf">Radial Basis Functions (RBF)</h2>
<p>Radial basis functions offer another approach: explicitly construct basis functions centered at various points, then fit a linear model in this new feature space.</p>
<h3 id="the-idea-of-basis-functions">The Idea of Basis Functions</h3>
<p>Instead of modeling $f(x) = \beta^T x$ (linear in original features), use:</p>
<p>$$f(x) = \sum_{j=1}^M \beta_j h_j(x)$$</p>
<p>Where $h_j(x)$ are <strong>basis functions</strong> ‚Äî transformations of the original features.</p>
<h3 id="why-higher-dimensions-help">Why Higher Dimensions Help</h3>
<p>Data that isn&#39;t linearly separable in the original space may become linearly separable in a higher-dimensional feature space.</p>
<p><strong>Classic example</strong>: XOR problem. Points at (0,0) and (1,1) are class 1; points at (0,1) and (1,0) are class 2. No line separates them in 2D, but adding the feature $x_1 \cdot x_2$ makes separation easy!</p>
<h3 id="gaussian-rbfs">Gaussian RBFs</h3>
<p>RBF uses Gaussian kernels as basis functions:</p>
<p>$$f(x) = \sum_{j=1}^M \beta_j K_{\lambda_j}(\xi_j, x)$$</p>
<p>Where:</p>
<ul>
<li>$\xi_j$ = center of the j-th basis function (a point in feature space)</li>
<li>$\lambda_j$ = width of the j-th kernel</li>
<li>$\beta_j$ = coefficient (learned by regression)</li>
</ul>
<p>More explicitly:</p>
<p>$$f(x) = \sum_{j=1}^M \beta_j \exp\left(-\frac{|x - \xi_j|^2}{2\lambda_j^2}\right)$$</p>
<h3 id="connection-to-infinite-dimensions">Connection to Infinite Dimensions</h3>
<p>The Gaussian kernel has a remarkable property: its Taylor series expansion involves polynomials of all degrees!</p>
<p>$$\exp(x) = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + ...$$</p>
<p>So using Gaussian RBFs is like using polynomial features of infinite degree ‚Äî but without the computational explosion.</p>
<h3 id="fitting-rbf-models">Fitting RBF Models</h3>
<p><strong>Full optimization</strong>: Learn $\beta$, $\xi$, and $\lambda$ by minimizing squared error.</p>
<ul>
<li>Non-linear in $\xi$ and $\lambda$</li>
<li>Requires gradient descent or other iterative methods</li>
<li>Risk of local minima</li>
</ul>
<p><strong>Simpler approach</strong>: Fix $\xi$ and $\lambda$, only learn $\beta$.</p>
<ul>
<li>Choose centers using unsupervised methods (e.g., k-means on training data)</li>
<li>Use a constant bandwidth based on average distances</li>
<li>Then it&#39;s just linear regression!</li>
</ul>
<h3 id="potential-pitfalls">Potential Pitfalls</h3>
<p>If the basis function centers don&#39;t cover the input space well, there can be &quot;holes&quot; ‚Äî regions where no kernel has significant weight, leading to poor predictions.</p>
<hr>
<h2 id="gaussian-mixture-models">Gaussian Mixture Models</h2>
<p>Mixture models extend RBFs to probabilistic density estimation.</p>
<h3 id="the-model">The Model</h3>
<p>$$f(x) = \sum_{j=1}^M \alpha_j \phi(x; \mu_j, \Sigma_j)$$</p>
<p>Where:</p>
<ul>
<li>$\phi$ = Gaussian density function</li>
<li>$\alpha_j$ = mixing proportion (how much weight to give component j)</li>
<li>$\sum_j \alpha_j = 1$</li>
<li>$\mu_j$, $\Sigma_j$ = mean and covariance of component j</li>
</ul>
<h3 id="interpretation">Interpretation</h3>
<p>The data is generated by:</p>
<ol>
<li>Randomly selecting a component j with probability $\alpha_j$</li>
<li>Drawing a point from the Gaussian $N(\mu_j, \Sigma_j)$</li>
</ol>
<p>Each component represents a &quot;cluster&quot; or subpopulation in the data.</p>
<h3 id="connection-to-rbf">Connection to RBF</h3>
<p>If we restrict covariances to be spherical ($\Sigma_j = \sigma^2 I$), mixture models reduce to radial basis expansions!</p>
<h3 id="fitting-with-maximum-likelihood">Fitting with Maximum Likelihood</h3>
<p>Parameters are fit by maximizing the likelihood of the data. This is typically done using the <strong>EM algorithm</strong> (covered in Model Selection chapter).</p>
<h3 id="classification-with-mixtures">Classification with Mixtures</h3>
<p>For classification:</p>
<ol>
<li>Fit a separate mixture model for each class</li>
<li>Apply Bayes&#39; theorem: $P(G=j|x) \propto \hat{\pi}_j \times \hat{f}_j(x)$</li>
</ol>
<p>This is more flexible than single-Gaussian LDA ‚Äî each class can have multiple modes.</p>
<hr>
<h2 id="summary-when-to-use-what">Summary: When to Use What</h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Best For</th>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Kernel Density Estimation</strong></td>
<td>Low dimensions, flexible shape</td>
<td>Non-parametric, intuitive</td>
<td>Curse of dimensionality</td>
</tr>
<tr>
<td><strong>Naive Bayes</strong></td>
<td>High dimensions, text/categorical</td>
<td>Fast, scales well</td>
<td>Wrong independence assumption</td>
</tr>
<tr>
<td><strong>RBF Networks</strong></td>
<td>Smooth non-linear functions</td>
<td>Flexible, local</td>
<td>Need to choose centers/widths</td>
</tr>
<tr>
<td><strong>Mixture Models</strong></td>
<td>Multi-modal distributions</td>
<td>Probabilistic, interpretable</td>
<td>Need to choose number of components</td>
</tr>
</tbody></table>
<h3 id="key-takeaways">Key Takeaways</h3>
<ol>
<li><strong>Kernels enable locality</strong>: Nearby points matter more than distant ones</li>
<li><strong>Bandwidth/width controls bias-variance</strong>: Too small = overfit, too large = underfit</li>
<li><strong>High dimensions are hard</strong>: Naive Bayes and the kernel trick help</li>
<li><strong>Generative vs. Discriminative</strong>: Kernel density classification is generative (models P(X|G)); logistic regression is discriminative (models P(G|X))</li>
</ol>

        </article>
        <nav class="page-navigation">
        <a href="eslr-02-classification.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Classification</span>
        </a>
        <a href="eslr-04-model-assessment.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Model Assessment and Selection</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>