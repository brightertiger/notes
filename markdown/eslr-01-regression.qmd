# Regression

## Derivation

-   $y = X\beta + \epsilon$
    -   $\epsilon \sim N(0, \sigma^2)$
-   Linear Function Approximation
    -   $E(Y|X) = f(X) = \beta_0 + \sum \beta_j x_j$
    -   Model is linear in parameters
-   Minimize Residual Sum of Squares
    -   $RSS = \sum (y_i - f(x_i))^2 = (y - X\beta)^T(y - X\beta)$
-   Optimal value of beta:
    -   ${\delta RSS \over \delta \beta} = 0$
    -   $\hat \beta = (X^T X)^{-1}(X^Ty)$
    -   $\hat y = X \hat \beta = (X(X^T X)^{-1}X^T)y = H y$
        -   H is the projection or Hat matrix

## Sampling Distribution of $\beta$

-   Deviations around the conditional mean are Gaussian
-   $Var(\hat \beta) = (X^T X)^{-1} \sigma^2$
-   Estimate of Sigma can be done by looking at sample variance
-   $\hat \sigma^2 = {1 \over N-p} \sum (y_i - \hat y_i)^2$
-   $\hat \beta \sim N(\beta, (X^T X)^{-1} \sigma^2)$

## Statistical Significance

-   $Z_i = {\beta_i \over SE_i} = \frac{\beta_i}{\hat \sigma \sqrt v_i}$
    -   v is the diagnoal element of $(X^T X)^{-1}$
-   Testing significance for a group of parameters
    -   Say categorical variables with all k variables
    -   $F = (RSS_0 - RSS_1) / (RSS_1 / N - p)$
    -   Change is RSS of the bigger model normalized by the estimate of variance

## Gauss-Markov Theorem

-   Among all the unbiased estimators, the least square estimates have lowest variance
-   $E(Y_0 - \hat Y_0))^2 = \sigma^2 + MSE(\hat f(X_0))$

## Subset Selection

-   Select only a few variables for better interpretability
-   Best subset selection os size K is the one that yields minimum RSS
-   Forward Selection
    -   Sequentially add one variable that most improves the fit
    -   QR decomposition / successive orthogonalization to look at correlation
-   Backward Selection
    -   Sequentially delete the variable that has least impact on the fit
    -   Z Score
-   Hybrid Stepwise Selection
    -   Consider both forward and backward moves at each step
    -   AIC for weighting the choices
-   Forward Stagewise Selection
    -   Add the variable most correlated with current residual
    -   Don't re-adjust the coefficients of the existing variables

## Shrinkage Methods

-   Shinkage methods result in biased estimators but a large reduction in variance
-   More continuous and dont suffer from high variability
-   Ridge Regression
    -   Impose a penalty the size of the coefficicents
    -   $\hat \beta^{\text{ridge}} = \arg \min (y - X \beta)^T(y - X \beta) + \lambda \sum \beta^2$
    -   $\hat \beta^{\text{ridge}} = \arg \min (y - X \beta)^T(y - X \beta) \; \text{subject to} \sum \beta^2 \le t$
    -   t is the budget
    -   In case of correlated variables, coefficcients are poorly determined
    -   A large positive coefficient of a variable is canceled by a large negative coefficient of the correlated variable
    -   Solution not invariant to scaling. Standardize the inputs and don't impose penalty on intercept
    -   $\hat \beta^{\text{ridge}} = (X^T X + \lambda I)^{-1}(X^Ty)$
    -   In case of correlated predictors, the original $(X^T X)$ wasn't full rank. But by adding noise to diagonal elements, the matrix can now be inverted.
    -   In case of orthonormal inputs (PCA), the ridge coefficicents are scaled versions of the original least-square estimates.
    -   $\lambda$ controls the degrees of freedom. A large value results in effectively dropping the variables.
-   Lasso Regression
    -   $\hat \beta^{\text{ridge}} = \arg \min (y - X \beta)^T(y - X \beta) + \lambda \sum |\beta|$
    -   Non-linear optimization
    -   A heavy restriction on budget makes some coefficients exactly zero
    -   Continuous subset selection
    -   Comparison between Ridge and Laso
        -   Ridge represents a disk $\beta_1^2 + \beta_2^2 <= t$
        -   Lasso represents a rhombus $|\beta_1| + |\beta_2| <= t$
        -   At optimal value, the estimated parameters can be exactly zero (corner solutions)
        -   Bayesian MAP estimates with different priors
            -   Lasso has Laplace Prior
            -   Ridge has Gaussian Prior
    -   Elastic Net
        -   $\lambda \sum \alpha \beta^2 + (1 - \alpha) |\beta|$
        -   Variable selection like Lasso
        -   Shinking coefficients like Ridge

## Partial Least Squares

-   Alternative approach to PCA deal with correlated features
-   Supervised transformation
-   Principal component regression seeks directions that have high variance
-   Partial Least Square seeks direction with high variance and high correlation with response
-   Derive new features by linear combination of raw variables re-weighted by the correlation
