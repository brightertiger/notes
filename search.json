[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DS/ML Notes",
    "section": "",
    "text": "This is a collection of notes on data science topics.\n\nGeneral DS topics from Statquest etc.\nKevin Murphy’s Probabilistic Machine Learning\nHastie’s Elements of Statistical Learning\nJurafsky’s Speech and Language Processing"
  },
  {
    "objectID": "jfsky-03-vectors.html",
    "href": "jfsky-03-vectors.html",
    "title": "36  Vectors",
    "section": "",
    "text": "Issues that make it harder for syntactic models to scale well\nLemmas and word forms (sing vs sang vs sung are infinitive forms of sing)\nWord Sense Disambiguation (mouse animal vs mouse hardware)\nSynonyms with same propositional meaning (couch vs sofa)\nWord Relatedness (coffee vs cup)\nSemantic Frames (A buy from B vs B sell to A)\nConnotation (affective meaning)"
  },
  {
    "objectID": "jfsky-03-vectors.html#vector-semantics",
    "href": "jfsky-03-vectors.html#vector-semantics",
    "title": "36  Vectors",
    "section": "36.2 Vector Semantics",
    "text": "36.2 Vector Semantics\n\nRepresent words using vectors called “embeddings”\nDerived from co-occurance matrix\nDocument Vectors\n\nTerm-Document Matrix\n|V| X |D| Dimension\nCount of times a word shows up in a document\nVector of the document in |V| dimension space\nUsed for informational retrieval\nVector Space Model\n\nWord Vectors\n\nTerm-Term Matrix\n\n|V| x |V| dimension\nNumber of times a word and context word show up in the same document\nWord-Word co-occurance matrix\nSparsity is a challenge\n\nCosine Distance\n\nNormalized Dot Product\nNormalized by the l2-norm, to control for vector size\n\\(\\cos \\theta = a.b / |a||b|\\)\n1 if vectors are in the same direction\n-1 if vectors are in opposite direction\n0 if vectors are perpendicular\nFor nomrlized vectors, it’s directly related to euclidean distance\n\\(|a - b|^2 = |a|^2 + |b|^2 - 2|a||b|\\cos\\theta = 2(1 - \\cos \\theta)\\)"
  },
  {
    "objectID": "jfsky-03-vectors.html#tf-idf",
    "href": "jfsky-03-vectors.html#tf-idf",
    "title": "36  Vectors",
    "section": "36.3 TF-IDF",
    "text": "36.3 TF-IDF\n\nTerm Frequency\n\nFrequency of word t in document d\n\\(tf_{t,d} = \\text{count}(t,d)\\)\nSmooth TF\n\\(tf_{t,d} = \\log(1 + \\text{count}(t,d))\\)\n\nDocument Frequency\n\nNumber of documents in which term t appears\n\\(df_t\\)\n\nInverse Document Frequency\n\n\\(idf_t = \\log(N / df_t)\\)\n\nTF-IDF\n\n\\(w_{t,d} = tf_{t,d} \\times idf_t\\)"
  },
  {
    "objectID": "jfsky-03-vectors.html#pmi",
    "href": "jfsky-03-vectors.html#pmi",
    "title": "36  Vectors",
    "section": "36.4 PMI",
    "text": "36.4 PMI\n\nRatio\n\nHow often to x and y actually co-occur?\nHow often will x and y co-occur if they were independent?\n\n\\(I(x,y) = \\log ({P(x,y) \\over P(x)P(y)})\\)\nRanges from negative to positive infinity\nPositive PMI max(0, PMI)"
  },
  {
    "objectID": "jfsky-03-vectors.html#vector-representation",
    "href": "jfsky-03-vectors.html#vector-representation",
    "title": "36  Vectors",
    "section": "36.5 Vector Representation",
    "text": "36.5 Vector Representation\n\nFor a given word T\n\nTerm-Document Matrix\nEach word vector has |D| dimensions\nEach cell is weighted using TF-IDF logic\n\nDocument Vector\n\nAverage of all word vecotrs appearing in the document\nSimilarity is calculated by cosine distance"
  },
  {
    "objectID": "jfsky-03-vectors.html#word2vec",
    "href": "jfsky-03-vectors.html#word2vec",
    "title": "36  Vectors",
    "section": "36.6 Word2Vec",
    "text": "36.6 Word2Vec\n\nTf-IDF and PMI generate sparse vectors\nNeed for dense and more efficient representation of words\nStatic EMbeddings\n\nSkipgram with Negative Sampling\n\nContextual Embeddings\n\nDynamic embedding for each word\nChanges with context (ex - positional embedding)\n\nSelf Supervised Learning"
  },
  {
    "objectID": "jfsky-03-vectors.html#skipgram",
    "href": "jfsky-03-vectors.html#skipgram",
    "title": "36  Vectors",
    "section": "36.7 Skipgram",
    "text": "36.7 Skipgram\n\nAlgorithm\n\nTreat tatget workd and neighbouring context word as positive samples (Window)\nRandomly sample other words from vocab as negative samples\nUse FFNN / Logistic Regression train a classifier\nUse the learned weights as embeddings\n\nPositive Examples\n\nContext Window of Size 2\nAll words +-2 from the given word\n\nNegative Examples\n\nUnigram frequency\nDownweighted to avoid sampling stop words frequently\n\\(P_{ij} \\propto f_{ij}^{0.75}\\)\n\nClassifier\n\nMaximize the similarity to positive samples\nMinimize the similarity to negative samples\n\\(L_{CE} = \\log P(+ | w,C_+) - \\sum \\log P(- | w,C_-)\\)\n\\(L_{CE} = \\log \\sigma(w . C_+) - \\sum \\log \\sigma(-w . C_-)\\)\nUse SGD to update w\n\nEach word has two separate embeddings\n\ntarget (when is shows up as w)\ncontext (when it shows up as c)\nFinal embedding is the sum of the two"
  },
  {
    "objectID": "jfsky-03-vectors.html#enhancements",
    "href": "jfsky-03-vectors.html#enhancements",
    "title": "36  Vectors",
    "section": "36.8 Enhancements",
    "text": "36.8 Enhancements\n\nUnknown / OOV words\n\nUse subwords models like FastText\nn-grams on characters\n\nGloVe\n\nGlobal vectors\nRatios of probabilities form word-word co-occurance matrix\n\nSimilarity\n\n\\(a:b :: a':b'\\)\n\\(b' = \\arg \\min \\text{distance}(x, b - a + a')\\)\n\nBias\n\nAllocation Harm\n\nUnfair to different groups\nfather-doctor, mother - housewife\n\nRepresentational Harm\n\nWrong association for marginal groups\nAfrican-american names to negative sentiment words"
  },
  {
    "objectID": "jfsky-04-sequence.html",
    "href": "jfsky-04-sequence.html",
    "title": "37  Sequence Architectures",
    "section": "",
    "text": "FFNNs cant be used because of limited context window\n\nLanguages can have longer dependencies over arbitrary context length\n\nLanguage Models assign conditional probability to the next word\n\n\\(P(W_{1:n}) = \\prod P(W_i | W_{1:i-1})\\)\n\n\nQuality of a language model is assessed by perplexity\n\n\\(PP = P(W_{1:n})^{-1/n}\\)\nInverse probability that the model assigns to the test sequence nomarlied by the length"
  },
  {
    "objectID": "jfsky-04-sequence.html#recurrent-neural-networks",
    "href": "jfsky-04-sequence.html#recurrent-neural-networks",
    "title": "37  Sequence Architectures",
    "section": "37.2 Recurrent Neural Networks",
    "text": "37.2 Recurrent Neural Networks\n\nNN architecture that contains a cycle in its network connections\nThe hidden layer output from previous step is linked to the current hidden layer output\nPredict using current intput and previous hidden state\nRemoves the fixed context dependency arising in FFNNs\nThe temporal hidden output can be persisited for infinite steps\nInference\n\n\\(h_t = g(U h_{t-1} + W x_t)\\)\n\\(y_t = V (h_t)\\)\n\nTraining\n\nChain rule for backpropagation\nOutput dependens on hidden state and hiddent state depends on previous time step\nBPTT: backpropagation through time\nIn terms of computational graph, the network is “unrolled” for the entire sequence\nFor very long sequences, use truncated BPTT\n\nRNNs and Language Models\n\nPredict next word using current word and previous hidden state\n\nRemoves the limited context problem\nUse word embeddings to enhance the model’s generalization ability\n$e_t = E x_t $\n\\(h_t = g(U h_{t-1} + W e_t)\\)\n\\(y_t = V (h_t)\\)\nOutput the probability distribution over the entire vocabulary\nLoss function: Cross entropy, difference between predictied probability and true distribution\nMinimize the error in predicting the next word\nTeacher forcing for training\n\nIn training phase, ignore the model output for predicting the next word.\nUse the actual word instead\n\nWeight tying\n\nInput embedding lookup and output probbaility matrix have same dimensions |V|\nAvoid using two different matrices, use the same one instead\n\n\nRNN Tasks\n\nSequence Labeling\n\nNER tasks, POS tagging\nAt each step predict the current tag rather than the next word\nUse softmax over tagset with CE loss function\n\nSequence Classification\n\nClassify entire sequences rather than the tokens\nUse hidden state from the last step and pass to FFNN\nBackprop will be used to update the RNN cycle links\nUse pooling to enhance performance\n\nElement-wise Mean, Max of all intermediate hidden states\n\n\nSequence Generation\n\nEncoder-decoder architecture\n\nAutoregressive generation\nUse  as the first token (BOS) and hidden state from encoder\nSample form RNN, using output softmax\nUse the embedding from the generated token as next input\nKeep sampling till  (EOS) token is sampled\n\n\nRNN Architectures\n\nStacked RNNs\n\nMultiple RNNs “stacked together”\nOutput from one layer serves as input to another layer\nDifferening levels of abstraction across layers\n\nBidirectional RNNs\n\nMany applications have full access to input sequence\nProcess the sequence from left-to-right and right-to-left\nConcatenate the output from forward and reversed passes"
  },
  {
    "objectID": "jfsky-04-sequence.html#lstm",
    "href": "jfsky-04-sequence.html#lstm",
    "title": "37  Sequence Architectures",
    "section": "37.3 LSTM",
    "text": "37.3 LSTM\n\nRNNs are hard to train\nHidden state tends to be fairly local in practice, limited long term dependencies\n\nVanishing gradients\nRepeated multiplications in backpropagation step\nSignoid derivatives between (0-0.25) and tanh derivatives between (0-1)\nDrives the gradients to zero over long sequence lengths\nInfinite memeory of hidden states\n\nLSTMs introduce context management\n\nEnable network to learn to forget information no longer needed\nPersist information for likely needed for deicisions yet to come\nUse gating mechanism (through additional weights) to control the flow of information\n\nArchitecture\n\nFeedforward layer\nSigmoid activation\nPoint-wise multiplication with the layer being gated (binary mask)\n\nInput Gate\n\nActual information\n\\(g_t = \\sigma(U h_{t-1} + W x_t)\\)\n\nAdd Gate\n\nSelect the information to keep from current context\n\\(i_t = \\sigma(U h_{t-1} + W x_t)\\)\n\\(j_t = i_t \\odot g_t\\)\n\nForget gate\n\nDelete information from context no longer needed\nWeighted sum of previous hidden state and current input\n\\(f_t = \\sigma(U h_{t-1} + W x_t)\\)\n\\(k_t = f_t \\odot c_{t-1}\\)\n\nContext\n\nSum of add and forget\n\\(c_t = j_t + k_t\\)\n\nOutput Gate\n\n\\(o_t = \\sigma(U h_{t-1} + W x_t)\\)\n\\(h_t = o_t \\odot \\tanh(c_t)\\)\n\nIn addition to hidden state, LSTMs also persist the context"
  },
  {
    "objectID": "jfsky-04-sequence.html#self-attention",
    "href": "jfsky-04-sequence.html#self-attention",
    "title": "37  Sequence Architectures",
    "section": "37.4 Self Attention",
    "text": "37.4 Self Attention\n\nLSTMs difficult to parallelize\nStill not effective for very long dependencies. Bahdanau attention etc. hacks needed.\nTransformers - Replace recurrent layers with self attention layers\nSelf Attention Mechanism\n\nMap input to output of same length\nAt step t, model has access to all inputs upto step t\n\nHelps with auto-regressive generation\n\nComputation for step t is independent of all other steps\n\nEasy parallelization\n\nCompare current input to the collection which reveals its relevance in the given context\n\\(y_3\\) is generated by comparing \\(x_3\\) to \\(x_1, x_2, x_3\\)\n\nCore of Attention Approach\n\nComparison is done using dot product operations (large value, more similar)\n\n\\(\\text{score}(x_i, x_j) = x_i . x_j\\)\n\nCompute attention weights\n\n\\(\\alpha_{ij} = \\text{softmax}(\\text{score}(x_i, x_j))\\)\n\nCompute output\n\n\\(y_i = \\sum \\alpha_{ij} x_j\\)\n\n\nSophistication wrt Transformers\n\nEach input can play three different roles\n\nQuery: When it’s being compared to other inputs (Current focus)\nKey: When it’s acting as context (previous input) fo comparison\nValue: When it’s being used to compute the output\n\nFor each role, there exists a separate embedding matrix\n\n\\(\\text{score}(x_i, x_j) = q_i . k_j / \\sqrt d\\)\n\\(y_i = \\sum \\alpha_{ij} v_j\\)\nNormalization to avoid overflow in softmax layer\n\nSince calculations are independent, use matrix multiplications\nUse masking to avoid peeking into the future\n\nTransformer Block\n\nAttention layer followed by FFNN with residual connections and layer norm\n\\(z = \\text{Layer Norm}(x + \\text{Self Attention}(x))\\)\n\\(y = \\text{Layer Norm}(z + \\text{FFNN}(z))\\)\nLayer Norm dies normalization across the hidden dimension\n\nMulti-Head Attention\n\nWords can exhibit different interrelationships (syntactic, semantic etc.)\nParallel layers to capture each of the underlying relationships\nConcatenate the output from each of the heads\n\nPositional Embeddings\n\nShuffling input order should matter\nSelf-attention logic (unlike RNNs) doesn’t respect sequence\nPositional embeddings modify the input embedddings based on the position in the sequence\nSize and Cosine functions\n\nBERT Architecture\n\nBase Model - 12 heads, 12 layers, 64 diemnsions, 768 size (12 * 64)\nLarge Model - 16 heads, 24 layers, 64 dimensions, 1024 size (16 * 64)"
  },
  {
    "objectID": "jfsky-05-encoder.html",
    "href": "jfsky-05-encoder.html",
    "title": "38  Encoder-Decoder Models",
    "section": "",
    "text": "Encoder-Decoder or Seq2Seq architecture\nCan be implemented using Transformers or RNNs\nOutput sequence is a complex transformation of the entire input sequence\nIn MT, the sequence order may not always agree.\n\nWord order topology changes from language to language (subject-verb-object)\nSame vocab maynot exists. Words map to phrases.\n\nEncoder block takes an input sequence and created a contextualized vector representation\nDecoder block uses this representation to generate the output sequence\nArchitecture\n\nEncoder: Input is sequence and output is contextualized hidden states\nContext Vector: A transformation of the contextualized hidden states\nDecoder: Uses context vector to geenrate arbitratry length sequences"
  },
  {
    "objectID": "jfsky-05-encoder.html#sequence-models",
    "href": "jfsky-05-encoder.html#sequence-models",
    "title": "38  Encoder-Decoder Models",
    "section": "38.2 Sequence Models",
    "text": "38.2 Sequence Models\n\nModels are autoregressive by nature\nAdd a BOS token  for conditional generation\nKeep sampling till EOS token \nRNN / LSTM\n\nEncoder\n\nProcess the input sequence token by token\n\nContext vector\n\nUse the final hidden state of LSTM as the context vector\n\nDecoder\n\nUse the context vector for initialization\nUse BOS token for generation\n\nDrawback: Influence of context vector wanes as longer sequences are generated\n\nSolution is to make context vecotr available for each timestep of the decoder\n\nTraining happens via teacher forcing\n\nTransformers\n\nUses Cross-Attention for decoding\nKeys and values come from encoder but query comes from decoder\nAllows decoder to attend to each token of the input sequence\n\nTokenization\n\nBPE / Wordpiece tokenizer"
  },
  {
    "objectID": "jfsky-05-encoder.html#evaluation",
    "href": "jfsky-05-encoder.html#evaluation",
    "title": "38  Encoder-Decoder Models",
    "section": "38.3 Evaluation",
    "text": "38.3 Evaluation\n\nHuman Evaluation\n\nAdequacy: How accurate is the meaning\nFluency: Grammatical correctness\n\nAutomatic Evaluation\n\nchrF Score: Character F-Score\nBLEU: Bilingual Evaluation Understudy\n\nn-gram precision: Compares n-gram of source with n-gram of output\nAdd a brevity penalty for best match length\n\nBERTScore\n\nPass the sequences to BERT\nCompute embeddings for each token\nCompute cosine similarity for ech pair of tokens\nMatch the tokens greedily and compute precision and recall ## Attention\n\n\nFinal hidden state acts as the bottleneck\nAttention mechanism helps the decoder to acess all the intermediate hidden states and not just the last one\nGenerate the context vector using weighted sum of all encoder states\nReplces the static context vector with one dynmically derived from encoder hidden states\nConsine similarity between decoder hidden state at time t wrt encoder hidden states"
  },
  {
    "objectID": "jfsky-05-encoder.html#decoding",
    "href": "jfsky-05-encoder.html#decoding",
    "title": "38  Encoder-Decoder Models",
    "section": "38.4 Decoding",
    "text": "38.4 Decoding\n\nPlain vanilla greedy decoding selects the argmax results over the the vocab to generate the output\n\nSelect the highest probability token\n\nThe overall results may be suboptimal. A token that looks good now may turn out to be wrong later\nUse search trees.\n\nThe most probable sequence may not be composed of argmax tokens at each step\nExhaustive search is too slow\n\nBeam Search\n\nSelect top-k possible tokens at each time step (beam width) (BFS approach)\nEach of the “k” hypothesis is passed incrementally to distinct decoders\nThe process continues until  token is sampled\nSeach continues untill ll the beams converge\nLonger sequences are penalized. Normalization is required\nUsually k is between 5 and 10\n\nTop-K Sampling\n\nTop-K tokens are sleected and the probability mas is redistributed among them\n\nTop-P Sampling\n\nInstead of selecting the top-k tokens, select the set of tokens whos eprobability mass exceeds threshold"
  },
  {
    "objectID": "jfsky-06-transfer.html",
    "href": "jfsky-06-transfer.html",
    "title": "39  Transfer Learning",
    "section": "",
    "text": "Contextual Embeddings: Representation of words in context. Same word can have different embeddings based on the context in which it appears.\nPretraining: Learning contextual embeddings from vast text of data.\nFine-tuning: Taking generic contextual representations and tweaking them to a specific downstream task by using a NN classifier head.\nTransfer Learning: Pretrain-Finetune paradigm is called as transfer learning.\nLanguage Models:\n\nCausal: Left-to-Right transformers\nBidirectional: Model can see both left and right context"
  },
  {
    "objectID": "jfsky-06-transfer.html#pre-training",
    "href": "jfsky-06-transfer.html#pre-training",
    "title": "39  Transfer Learning",
    "section": "40.1 Pre-Training",
    "text": "40.1 Pre-Training\n\nFill-in-the-blank or Cloze task\nPredict the “masked” words (MLM)\nUse CE loss over the vocab to drive training\nSelf-supervised Learning\nMLM\n\nRequires unannotated large text corpus\nRandom sample (15%) of tokens is chosen to masked for learning\n80% replaced with [MASK]\n10% replaced with random word\n10% replaced left unchanged\nPredict original token for each of the masked input\n\nSpan\n\nContiguous sequence of one or more words\nRandomly selected spans from training sequence\nSpan length selected from geometric distribution\nStarting location is slelected from uniforma distribution\nOnce the span is selected, all the words within the span are substituted\nLearning objective: MLM + Span Boundary Objective (SBO)\nPredict words in the span using the starting and ending token and positional embeddings\n\nNSP\n\nNext Sentence prediction\nParaphrase, entailment and discourse coherence\nActual pair of adjacent sentences or not\nDistinguish true paris from random pairs\n\nTraining Data\n\nBooks corpus\nEnglish Wiki"
  },
  {
    "objectID": "jfsky-06-transfer.html#fine-tuning",
    "href": "jfsky-06-transfer.html#fine-tuning",
    "title": "39  Transfer Learning",
    "section": "40.2 Fine-Tuning",
    "text": "40.2 Fine-Tuning\n\nCreation of applications on top of pretrained models leveraging the generalizations from SSL\nLimited mount of labeled data\nFreeze or minimal adjsutments to pretrained weights\nSequence Classification\n\n[CLS] token embedding\nClassifier head\n\nNLI\n\nRecognize contradiciton, entailment and neutral\nCLS token for premise. [CLS] premise [SEP] text [SEP]\n\nSequence Labeling\n\nPrediction for each token\nSoftmax over label classes\nBIO tags (beginning, inside, outside)\nWord Peice Tokenization creates challenge\n\nTraning expand the tags\nScoring use tag assigned to the first subword token\n\n\nSpan based representations\n\nMiddle ground between token level and sequence level classifications\nGenerate possible spans\nAverage the embeddings within the spans\nSpan represenatations: concatenate [start, end and average] embeddings\nUse regression to predict start and end tokens"
  },
  {
    "objectID": "eslr-03-kernel-methods.html",
    "href": "eslr-03-kernel-methods.html",
    "title": "28  Kernel Methods",
    "section": "",
    "text": "A random sample \\([x_0, x_1, ... x_n]\\) is drawn from probability distribution \\(f_X(x)\\)\nParzen Estimate of \\(\\hat f_X(x)\\)\n\n\\(\\hat f_X(x_0) = {1 \\over N \\lambda} \\,\\, \\# x_i \\in N(x_0)\\)\n\\(\\lambda\\) is width of the neighbourhood\nThe esitmate is bumpy\n\nGaussian Kernel of width \\(\\lambda\\) can be a choice of Kernel\n\n\\(\\hat f_X(x_0) = {1 \\over N \\lambda} K_{\\lambda}(x_i, x_0)\\)\n\\(K_{\\lambda}(x_i, x_0) = \\phi(\\|x_i - x_0\\|) / \\lambda\\)\nWeight of point \\(x_i\\) descreases as distance from \\(x_0\\) increases\n\nNew estimate for density is\n\n\\(\\hat f_X(x_0) = {1 \\over N } \\phi_{\\lambda}(x_ - x_0)\\)\nConvolution of Sample empirical distribution with Gaussian Kernel"
  },
  {
    "objectID": "eslr-03-kernel-methods.html#kernel-desnity-classification",
    "href": "eslr-03-kernel-methods.html#kernel-desnity-classification",
    "title": "28  Kernel Methods",
    "section": "28.2 Kernel Desnity Classification",
    "text": "28.2 Kernel Desnity Classification\n\nBayes’ Theorem\n\\(P(G=j | X=x_0) \\propto \\hat \\pi_j \\hat f_j(x_0)\\)\n\\(\\hat \\pi_j\\) is the sample proportion of the class j\n\\(\\hat f_j(x_0)\\) is the Kernel density estimate for class j\nLearning separate class densities may be misleading\n\nDense vs Non-dense regions in feature space\nDensity estimates are critical only near the decision boundary"
  },
  {
    "objectID": "eslr-03-kernel-methods.html#naive-bayes-classifier",
    "href": "eslr-03-kernel-methods.html#naive-bayes-classifier",
    "title": "28  Kernel Methods",
    "section": "28.3 Naive Bayes Classifier",
    "text": "28.3 Naive Bayes Classifier\n\nApplicable when dimension of feature space is high\nAssumption: For a given class, the features are independent\n\n\\(f_j(x) = \\prod_p f_{jp}(x_p)\\)\nRarely holds true in real world dataset\n\n\\(\\log \\frac{P(G=i|X=X)}{P(G=j|X=X)} = \\log \\frac{\\pi_i \\prod f_{ip}(x_p)}{\\pi_j \\prod f_{jp}(x_p)}\\)\n\n\\(\\log \\frac{\\pi_i}{\\pi_j} + \\sum \\log \\frac{f_{ip}(x_p)}{f_{jp}(x_p)}\\)"
  },
  {
    "objectID": "eslr-03-kernel-methods.html#radial-basis-functions",
    "href": "eslr-03-kernel-methods.html#radial-basis-functions",
    "title": "28  Kernel Methods",
    "section": "28.4 Radial Basis Functions",
    "text": "28.4 Radial Basis Functions\n\nBasis Functions \\(f(x) = \\sum \\beta h(x)\\)\nTransform lower dimension features to high dimensions\nData which is not linearly separable in lower dimension may become linearly separable in higher dimensions\nRBF treats gaussian kernel functions as basis functions\n\nTaylor series expansion of \\(\\exp(x)\\)\nPolynomial basis function with infinite dimensions\n\n\\(f(x) = \\sum_j K_{\\lambda_j}(\\xi_j, x) \\beta_j\\)\n\\(f(x) = \\sum_j D({\\|x_i - \\xi_j\\| \\over \\lambda _j}) \\beta_j\\)\n\nD is the standard normal gaussian density function\n\nFor least square regression, SSE can be optimized wrt to \\(\\beta, \\xi, \\lambda\\)\n\nNon-Linear Optimization\nUse greedy approaches like SGD\n\nSimplify the calculations by assuming \\(\\xi, \\lambda\\) to be hyperparameters\n\nUse unsupervised learning to estimate them\nAssuming constant variance simplifies calculations\n\nIt can create “holes” where none of the kernels have high density estimate"
  },
  {
    "objectID": "eslr-03-kernel-methods.html#mixture-models",
    "href": "eslr-03-kernel-methods.html#mixture-models",
    "title": "28  Kernel Methods",
    "section": "28.5 Mixture Models",
    "text": "28.5 Mixture Models\n\nExtension of RBF\n\\(f(x) = \\sum_j \\alpha_j \\phi(x, \\mu_j, \\Sigma_j)\\)\n\\(\\sum \\alpha_j = 1\\), are the mixing proporitons\nGaussian mixture models use Gaussian kernel in place of \\(\\phi\\)\nParameters are fit using Maximum Likelihood\nIf the covariance martix is restricted to a diagonal matrix \\(\\Sigma = \\sigma^2 I\\), then it reduces to radial basis expansion\nClassification can be done via Bayes Theorem\n\nSeparate density estimation for each class\nProbability is \\(\\propto \\hat \\pi_i f_j(x)\\)"
  }
]