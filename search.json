[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DS/ML Notes",
    "section": "",
    "text": "This is a collection of notes on data science topics.\n\nGeneral DS topics from Statquest etc.\nKevin Murphy’s Probabilistic Machine Learning\nHastie’s Elements of Statistical Learning\nJurafsky’s Speech and Language Processing"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "See Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "jfsky-00.html",
    "href": "jfsky-00.html",
    "title": "SLP Notes",
    "section": "",
    "text": "Resource: Speech and Language Processing by Jurafsky\nTopics Covered\n\nChapter 2: Regex\nChapter 3: N-Grams\nChapter 6: Vector Semantics\nChapter 9: Sequence Models\nChapter 10: Encoder Decoder Models\nChapter 11: Transfer Learning"
  },
  {
    "objectID": "jfsky-01-regex.html",
    "href": "jfsky-01-regex.html",
    "title": "34  Regex",
    "section": "",
    "text": "Language for specifying text search strings\nAlgebraic notation for characterizing a set of strings\nBasic regular expression\n\nMatch the “word” /word/\nMatch the “word” or “Word” /[wW]ord/\nMatch single digit /[1234567890]/\n\nRanges\n\nCapital Letters /[A-Z]/\nLower Case Letters /[a-z]/\nSingle Digit /[0-9]/\n\nCaret\n\nExclusions\nNot an upper case letter /[^A-Z]/\nNot a period /[^.]/\nIf caret is not the first character, it’s treated as any other character\n\nQuestion Mark\n\nPreceding character or nothing\n“word” or “words” /words?/\n“colour” or “color” /colou?r/\n\nKleene *\n\nZero or more occurances\nZero or more “a” /a*/\nZero or more “a”s or “b”s /[ab]*/\n\nKleene +\n\nOne or more occurances\nOne or more digits /[0-9]+/\n\nWildcard\n\nMatch any single expression\nAny character between “beg” and “n” /beg.n/\n\nAnchors\n\nStart of the line ^\nLines starting with “the” /^The/\nEnd of the line $\nLines ending with period /\\.$/\nWord boundary \n\nGrouping\n\nDisjunction “|”\n\nMatch either cat or dog /cat|dog/\n\nParanthesis ()\n\nMatch “guppy” or “guppies” /gupp(y|ies)/\n\n\nExample\n\n/(ˆ|[ˆa-zA-Z])[tT]he([ˆa-zA-Z]|$)/\n\nAt the start or a non-alphabetic character\nAt the end or non-alphabetic character\nLook for “the” or “The”\n\n\nOperators\n\nAny digit \nAny non-digit \nWhitespace \nNon-whitespace \nAny alphanumeric \nNon Alpha-numeric \n\nRange\n\nZero or more *\nOne or more +\nExactly zero or one ?\nN Occurances {n}\nN-to-M Occurances {n,m}\nAtleast N Occurances {n,}\nUpto M Occurances {,m}"
  },
  {
    "objectID": "jfsky-02-tokenization.html",
    "href": "jfsky-02-tokenization.html",
    "title": "35  Tokenization",
    "section": "",
    "text": "Language Models assign probabilities to sequence of words\n\n\\(P(w_1, w_2, ..., w_n)\\)\n\nSimplify the calculation using chain rule\n\n\\(P(w_1, w_2, ..., w_n) = P(w_1) \\times P(w_2 | w_1)..... \\times P(w_n | w_1 w_2 .. w_{n-1})\\)\n\\(P(w_1, w_2, ..., w_n) = \\prod P(w_i | w_{1:i-1})\\)\n\nJoint probability can be expressed as a product of conditional probabilities\n\nProbability of a word given historical context\n\\(P(w_n | h)\\)\n\nN-gram refers to a sequence of n words\nN-gram model makes the Markov assumption\n\n\\(P(w | h)\\) can be approximated using just the last n-1 words\n\nFor example in case of a bigram model\n\n\\(P(w_n | h) \\approx P(w_n | w_{n-1})\\)\n\nEstimate the probabilities using Maximum Likelihood\n\nRelative frequency\n\\(P(w_n | w_{n-1}) = {P(w_{n-1}, w_n) \\over \\sum_k P(w_{n-1}, w_k)}\\)\n\\(P(w_n | w_{n-1}) = P(w_{n-1}, w_n) / P(w_{n-1})\\)\n\nBOS and EOS tokens to handle the edge cases\nN-gram models apt at capturing syntactic features (noun-verb-adj etc)\nTo avoid numerical underflow, overflow problems use log probabilities\n\n\\(p_1 \\times p_2 = \\exp(\\log p_1 + \\log p_2)\\)"
  },
  {
    "objectID": "jfsky-03-vectors.html",
    "href": "jfsky-03-vectors.html",
    "title": "3  Vectors",
    "section": "",
    "text": "Issues that make it harder for syntactic models to scale well\nLemmas and word forms (sing vs sang vs sung are infinitive forms of sing)\nWord Sense Disambiguation (mouse animal vs mouse hardware)\nSynonyms with same propositional meaning (couch vs sofa)\nWord Relatedness (coffee vs cup)\nSemantic Frames (A buy from B vs B sell to A)\nConnotation (affective meaning)"
  },
  {
    "objectID": "jfsky-04-sequence.html",
    "href": "jfsky-04-sequence.html",
    "title": "4  Sequence Architectures",
    "section": "",
    "text": "FFNNs cant be used because of limited context window\n\nLanguages can have longer dependencies over arbitrary context length\n\nLanguage Models assign conditional probability to the next word\n\n\\(P(W_{1:n}) = \\prod P(W_i | W_{1:i-1})\\)\n\n\nQuality of a language model is assessed by perplexity\n\n\\(PP = P(W_{1:n})^{-1/n}\\)\nInverse probability that the model assigns to the test sequence nomarlied by the length"
  },
  {
    "objectID": "jfsky-05-encoder.html",
    "href": "jfsky-05-encoder.html",
    "title": "5  Encoder-Decoder Models",
    "section": "",
    "text": "Encoder-Decoder or Seq2Seq architecture\nCan be implemented using Transformers or RNNs\nOutput sequence is a complex transformation of the entire input sequence\nIn MT, the sequence order may not always agree.\n\nWord order topology changes from language to language (subject-verb-object)\nSame vocab maynot exists. Words map to phrases.\n\nEncoder block takes an input sequence and created a contextualized vector representation\nDecoder block uses this representation to generate the output sequence\nArchitecture\n\nEncoder: Input is sequence and output is contextualized hidden states\nContext Vector: A transformation of the contextualized hidden states\nDecoder: Uses context vector to geenrate arbitratry length sequences"
  },
  {
    "objectID": "jfsky-06-transfer.html",
    "href": "jfsky-06-transfer.html",
    "title": "6  Transfer Learning",
    "section": "",
    "text": "Contextual Embeddings: Representation of words in context. Same word can have different embeddings based on the context in which it appears.\nPretraining: Learning contextual embeddings from vast text of data.\nFine-tuning: Taking generic contextual representations and tweaking them to a specific downstream task by using a NN classifier head.\nTransfer Learning: Pretrain-Finetune paradigm is called as transfer learning.\nLanguage Models:\n\nCausal: Left-to-Right transformers\nBidirectional: Model can see both left and right context"
  },
  {
    "objectID": "jfsky-02-tokenization.html#perplexity",
    "href": "jfsky-02-tokenization.html#perplexity",
    "title": "35  Tokenization",
    "section": "35.2 Perplexity",
    "text": "35.2 Perplexity\n\nInverse probability normalized by the length of sequence\n\\(PP(W) = P(w_1 w_2 ... w_n)^{ - {1 \\over n}}\\)\n\\(PP(W) = \\sqrt[n]{\\prod 1 / P(w_i | w_{1:i-1})}\\)\nHigher the conditional probability, lower is the perplexity\nWeighted average branching factor\n\nBranching factor refers to the number of possible words that can follow a particluar word\n\nPerplxity of LMs comparable only if they use same vocabulary"
  },
  {
    "objectID": "jfsky-02-tokenization.html#perplexity-and-entropy",
    "href": "jfsky-02-tokenization.html#perplexity-and-entropy",
    "title": "35  Tokenization",
    "section": "35.3 Perplexity and Entropy",
    "text": "35.3 Perplexity and Entropy\n\nEntropy is a measure of information\n\nNumber of bits it takes to encode information (log base 2)\n\\(H(x) = - \\sum p \\log (p)\\)\n\nEntropy Rate: Entropy // Seq Length\nLMs can potentially consider infinite sequence length\n\n\\(H(W) = - \\lim_{n \\to \\infty} {1 \\over n} \\sum p(w_{1:n}) \\log(p_{1:n})\\)\n\\(H(W) \\approx - {1 \\over n} \\log p(w_{1:n})\\)\n\n\\(P(W) = 2^{H(W)}\\)"
  },
  {
    "objectID": "jfsky-02-tokenization.html#unknown-words",
    "href": "jfsky-02-tokenization.html#unknown-words",
    "title": "35  Tokenization",
    "section": "35.4 Unknown Words",
    "text": "35.4 Unknown Words\n\nIf probability of a word is zero, the perplexity is not defined.\nUnknown words or OOV words (out of vocab)\nHandle via pre-processing  token\n\nReplace rare words with this token in training corpus\n\nLMs can achieve lower perplexity by selecting smaller vocab size"
  },
  {
    "objectID": "jfsky-02-tokenization.html#smoothing",
    "href": "jfsky-02-tokenization.html#smoothing",
    "title": "35  Tokenization",
    "section": "35.5 Smoothing",
    "text": "35.5 Smoothing\n\nAvoid assigning zero probabilities to unseen sequences\nLaplace Smoothing\n\nAdd smoothing constants while calculating relative frequencies\n1 to numerator\nV to denominator, V is the vocab size to ensure that probabilities sum up to 1\n\\(P(w_i) = (w_i + 1) / (N + V)\\)\n\\(P(w_i | w_n) = \\text{count}(w_i, w_n) + 1 / \\text{count}(w_n) + V\\)\nDiscount some probability mass from seen phrases and save it for unseen phrases\nGeneralization to “Add - k” smoothing\n\nBack-off\n\nUse shorter sequences in case not enough support for full context\nTrigram if evidence is sufficent, otherwise use bigram\n\nInterpolation\n\nMix the probability estimates from all n-grams\n\\(P(w_1 | w_2 w_3) = \\lambda_1 P(w_1) + \\lambda_2 P(w_1 | w_2) + + \\lambda_3 P(w_1 | w_2 w_3)\\)\n\nKneser-Ney Smoothing\n\nAbsolute discounting\n\\(P(w_1 | w_2) = C(w_1 w_2) - d / \\sum c(w_2) + \\lambda P(w_1)\\)"
  },
  {
    "objectID": "jfsky-02-tokenization.html#efficiency",
    "href": "jfsky-02-tokenization.html#efficiency",
    "title": "35  Tokenization",
    "section": "35.6 Efficiency",
    "text": "35.6 Efficiency\n\nReduce memory footprint\nQuantization for probabilities\nReverse Tries for N-grams\nString Hashing\nBloom filters\nStupid Backoff"
  },
  {
    "objectID": "jfsky-01-regex.html#words",
    "href": "jfsky-01-regex.html#words",
    "title": "34  Regex",
    "section": "34.2 Words",
    "text": "34.2 Words\n\nUtterance is the spoken correlate of a sentence\nDisfluency\n\nFragments: broken off words\nFillers or Filled Pauses “um”\n\nLemma: Lexical form of the same word (cats vs cat)\nTypes (V): Number of distinct words\nTokens (N): Number of running words\nHeap’s Law: \\(V = K N^\\beta\\)"
  },
  {
    "objectID": "jfsky-01-regex.html#text-normalization",
    "href": "jfsky-01-regex.html#text-normalization",
    "title": "34  Regex",
    "section": "34.3 Text Normalization",
    "text": "34.3 Text Normalization\n\nInvolves three steps\n\nTokenzing Words\nNormalizing word formats\nSegmenting sentences\n\nTokenization\n\nBreaing up a an utterance into tokens\nPenn Treebank Tokenization\nNLTK Regex Tokenization\nByte Pair Encoding\n\nEmperically determine the tokens using data\nUseful in dealing with unseen words\nUse subwords tokens which are arbitrary substrings\nToken Learner: Creates vocabulary out of corpus\nToken Segementor: Applies token learner on raw test data\nBPE Token Learner\n\nStarts with individual characters as vocab\nMerges the most frequently occuring pairs and adds them to back vocab\nRepeats the count and merge process to create longer substrings\nContinues until vocab size is reached\nNo merging across word boundries\n\nBPE Token Parser\n\nRun the token learner on test data\nSame order in which tokens were created\nFirst split into individual characters\nMerge the characters based on BPE vocab"
  },
  {
    "objectID": "jfsky-01-regex.html#word-normalization",
    "href": "jfsky-01-regex.html#word-normalization",
    "title": "34  Regex",
    "section": "34.4 Word Normalization",
    "text": "34.4 Word Normalization\n\nPutting words and tokens in a standard format\nCase Folding: Convert everything to lowercase\nLemmatization: Reduce words to roots\n\nStemming (ing, ed etc.)\nPorter Stemming"
  },
  {
    "objectID": "jfsky-01-regex.html#edit-distance",
    "href": "jfsky-01-regex.html#edit-distance",
    "title": "34  Regex",
    "section": "34.5 Edit Distance",
    "text": "34.5 Edit Distance\n\nSimilarity between two strings\nMinimum number of editing operations needed to transform one string into another\n\nInsertion, Deletion and Substitution\n\nLevenstien Distance: All three operations ahve the same cost\nDynamic Programming\n\nViterbi Algorithm"
  },
  {
    "objectID": "eslr-01-regression.html",
    "href": "eslr-01-regression.html",
    "title": "7  Regression",
    "section": "",
    "text": "\\(y = X\\beta + \\epsilon\\)\n\n\\(\\epsilon \\sim N(0, \\sigma^2)\\)\n\nLinear Function Approximation\n\n\\(E(Y|X) = f(X) = \\beta_0 + \\sum \\beta_j x_j\\)\nModel is linear in parameters\n\nMinimize Residual Sum of Squares\n\n\\(RSS = \\sum (y_i - f(x_i))^2 = (y - X\\beta)^T(y - X\\beta)\\)\n\nOptimal value of beta:\n\n\\({\\delta RSS \\over \\delta \\beta} = 0\\)\n\\(\\hat \\beta = (X^T X)^{-1}(X^Ty)\\)\n\\(\\hat y = X \\hat \\beta = (X(X^T X)^{-1}X^T)y = H y\\)\n\nH is the projection or Hat matrix"
  },
  {
    "objectID": "jfsky-03-vectors.html#vector-semantics",
    "href": "jfsky-03-vectors.html#vector-semantics",
    "title": "3  Vectors",
    "section": "3.2 Vector Semantics",
    "text": "3.2 Vector Semantics\n\nRepresent words using vectors called “embeddings”\nDerived from co-occurance matrix\nDocument Vectors\n\nTerm-Document Matrix\n|V| X |D| Dimension\nCount of times a word shows up in a document\nVector of the document in |V| dimension space\nUsed for informational retrieval\nVector Space Model\n\nWord Vectors\n\nTerm-Term Matrix\n\n|V| x |V| dimension\nNumber of times a word and context word show up in the same document\nWord-Word co-occurance matrix\nSparsity is a challenge\n\nCosine Distance\n\nNormalized Dot Product\nNormalized by the l2-norm, to control for vector size\n\\(\\cos \\theta = a.b / |a||b|\\)\n1 if vectors are in the same direction\n-1 if vectors are in opposite direction\n0 if vectors are perpendicular\nFor nomrlized vectors, it’s directly related to euclidean distance\n\\(|a - b|^2 = |a|^2 + |b|^2 - 2|a||b|\\cos\\theta = 2(1 - \\cos \\theta)\\)"
  },
  {
    "objectID": "jfsky-03-vectors.html#tf-idf",
    "href": "jfsky-03-vectors.html#tf-idf",
    "title": "3  Vectors",
    "section": "3.3 TF-IDF",
    "text": "3.3 TF-IDF\n\nTerm Frequency\n\nFrequency of word t in document d\n\\(tf_{t,d} = \\text{count}(t,d)\\)\nSmooth TF\n\\(tf_{t,d} = \\log(1 + \\text{count}(t,d))\\)\n\nDocument Frequency\n\nNumber of documents in which term t appears\n\\(df_t\\)\n\nInverse Document Frequency\n\n\\(idf_t = \\log(N / df_t)\\)\n\nTF-IDF\n\n\\(w_{t,d} = tf_{t,d} \\times idf_t\\)"
  },
  {
    "objectID": "jfsky-03-vectors.html#pmi",
    "href": "jfsky-03-vectors.html#pmi",
    "title": "3  Vectors",
    "section": "3.4 PMI",
    "text": "3.4 PMI\n\nRatio\n\nHow often to x and y actually co-occur?\nHow often will x and y co-occur if they were independent?\n\n\\(I(x,y) = \\log ({P(x,y) \\over P(x)P(y)})\\)\nRanges from negative to positive infinity\nPositive PMI max(0, PMI)"
  },
  {
    "objectID": "jfsky-03-vectors.html#vector-representation",
    "href": "jfsky-03-vectors.html#vector-representation",
    "title": "3  Vectors",
    "section": "3.5 Vector Representation",
    "text": "3.5 Vector Representation\n\nFor a given word T\n\nTerm-Document Matrix\nEach word vector has |D| dimensions\nEach cell is weighted using TF-IDF logic\n\nDocument Vector\n\nAverage of all word vecotrs appearing in the document\nSimilarity is calculated by cosine distance"
  },
  {
    "objectID": "jfsky-03-vectors.html#word2vec",
    "href": "jfsky-03-vectors.html#word2vec",
    "title": "3  Vectors",
    "section": "3.6 Word2Vec",
    "text": "3.6 Word2Vec\n\nTf-IDF and PMI generate sparse vectors\nNeed for dense and more efficient representation of words\nStatic EMbeddings\n\nSkipgram with Negative Sampling\n\nContextual Embeddings\n\nDynamic embedding for each word\nChanges with context (ex - positional embedding)\n\nSelf Supervised Learning"
  },
  {
    "objectID": "jfsky-03-vectors.html#skipgram",
    "href": "jfsky-03-vectors.html#skipgram",
    "title": "3  Vectors",
    "section": "3.7 Skipgram",
    "text": "3.7 Skipgram\n\nAlgorithm\n\nTreat tatget workd and neighbouring context word as positive samples (Window)\nRandomly sample other words from vocab as negative samples\nUse FFNN / Logistic Regression train a classifier\nUse the learned weights as embeddings\n\nPositive Examples\n\nContext Window of Size 2\nAll words +-2 from the given word\n\nNegative Examples\n\nUnigram frequency\nDownweighted to avoid sampling stop words frequently\n\\(P_{ij} \\propto f_{ij}^{0.75}\\)\n\nClassifier\n\nMaximize the similarity to positive samples\nMinimize the similarity to negative samples\n\\(L_{CE} = \\log P(+ | w,C_+) - \\sum \\log P(- | w,C_-)\\)\n\\(L_{CE} = \\log \\sigma(w . C_+) - \\sum \\log \\sigma(-w . C_-)\\)\nUse SGD to update w\n\nEach word has two separate embeddings\n\ntarget (when is shows up as w)\ncontext (when it shows up as c)\nFinal embedding is the sum of the two"
  },
  {
    "objectID": "jfsky-03-vectors.html#enhancements",
    "href": "jfsky-03-vectors.html#enhancements",
    "title": "3  Vectors",
    "section": "3.8 Enhancements",
    "text": "3.8 Enhancements\n\nUnknown / OOV words\n\nUse subwords models like FastText\nn-grams on characters\n\nGloVe\n\nGlobal vectors\nRatios of probabilities form word-word co-occurance matrix\n\nSimilarity\n\n\\(a:b :: a':b'\\)\n\\(b' = \\arg \\min \\text{distance}(x, b - a + a')\\)\n\nBias\n\nAllocation Harm\n\nUnfair to different groups\nfather-doctor, mother - housewife\n\nRepresentational Harm\n\nWrong association for marginal groups\nAfrican-american names to negative sentiment words"
  },
  {
    "objectID": "jfsky-04-sequence.html#recurrent-neural-networks",
    "href": "jfsky-04-sequence.html#recurrent-neural-networks",
    "title": "4  Sequence Architectures",
    "section": "4.2 Recurrent Neural Networks",
    "text": "4.2 Recurrent Neural Networks\n\nNN architecture that contains a cycle in its network connections\nThe hidden layer output from previous step is linked to the current hidden layer output\nPredict using current intput and previous hidden state\nRemoves the fixed context dependency arising in FFNNs\nThe temporal hidden output can be persisited for infinite steps\nInference\n\n\\(h_t = g(U h_{t-1} + W x_t)\\)\n\\(y_t = V (h_t)\\)\n\nTraining\n\nChain rule for backpropagation\nOutput dependens on hidden state and hiddent state depends on previous time step\nBPTT: backpropagation through time\nIn terms of computational graph, the network is “unrolled” for the entire sequence\nFor very long sequences, use truncated BPTT\n\nRNNs and Language Models\n\nPredict next word using current word and previous hidden state\n\nRemoves the limited context problem\nUse word embeddings to enhance the model’s generalization ability\n$e_t = E x_t $\n\\(h_t = g(U h_{t-1} + W e_t)\\)\n\\(y_t = V (h_t)\\)\nOutput the probability distribution over the entire vocabulary\nLoss function: Cross entropy, difference between predictied probability and true distribution\nMinimize the error in predicting the next word\nTeacher forcing for training\n\nIn training phase, ignore the model output for predicting the next word.\nUse the actual word instead\n\nWeight tying\n\nInput embedding lookup and output probbaility matrix have same dimensions |V|\nAvoid using two different matrices, use the same one instead\n\n\nRNN Tasks\n\nSequence Labeling\n\nNER tasks, POS tagging\nAt each step predict the current tag rather than the next word\nUse softmax over tagset with CE loss function\n\nSequence Classification\n\nClassify entire sequences rather than the tokens\nUse hidden state from the last step and pass to FFNN\nBackprop will be used to update the RNN cycle links\nUse pooling to enhance performance\n\nElement-wise Mean, Max of all intermediate hidden states\n\n\nSequence Generation\n\nEncoder-decoder architecture\n\nAutoregressive generation\nUse  as the first token (BOS) and hidden state from encoder\nSample form RNN, using output softmax\nUse the embedding from the generated token as next input\nKeep sampling till  (EOS) token is sampled\n\n\nRNN Architectures\n\nStacked RNNs\n\nMultiple RNNs “stacked together”\nOutput from one layer serves as input to another layer\nDifferening levels of abstraction across layers\n\nBidirectional RNNs\n\nMany applications have full access to input sequence\nProcess the sequence from left-to-right and right-to-left\nConcatenate the output from forward and reversed passes"
  },
  {
    "objectID": "jfsky-04-sequence.html#lstm",
    "href": "jfsky-04-sequence.html#lstm",
    "title": "4  Sequence Architectures",
    "section": "4.3 LSTM",
    "text": "4.3 LSTM\n\nRNNs are hard to train\nHidden state tends to be fairly local in practice, limited long term dependencies\n\nVanishing gradients\nRepeated multiplications in backpropagation step\nSignoid derivatives between (0-0.25) and tanh derivatives between (0-1)\nDrives the gradients to zero over long sequence lengths\nInfinite memeory of hidden states\n\nLSTMs introduce context management\n\nEnable network to learn to forget information no longer needed\nPersist information for likely needed for deicisions yet to come\nUse gating mechanism (through additional weights) to control the flow of information\n\nArchitecture\n\nFeedforward layer\nSigmoid activation\nPoint-wise multiplication with the layer being gated (binary mask)\n\nInput Gate\n\nActual information\n\\(g_t = \\sigma(U h_{t-1} + W x_t)\\)\n\nAdd Gate\n\nSelect the information to keep from current context\n\\(i_t = \\sigma(U h_{t-1} + W x_t)\\)\n\\(j_t = i_t \\odot g_t\\)\n\nForget gate\n\nDelete information from context no longer needed\nWeighted sum of previous hidden state and current input\n\\(f_t = \\sigma(U h_{t-1} + W x_t)\\)\n\\(k_t = f_t \\odot c_{t-1}\\)\n\nContext\n\nSum of add and forget\n\\(c_t = j_t + k_t\\)\n\nOutput Gate\n\n\\(o_t = \\sigma(U h_{t-1} + W x_t)\\)\n\\(h_t = o_t \\odot \\tanh(c_t)\\)\n\nIn addition to hidden state, LSTMs also persist the context"
  },
  {
    "objectID": "jfsky-04-sequence.html#self-attention",
    "href": "jfsky-04-sequence.html#self-attention",
    "title": "4  Sequence Architectures",
    "section": "4.4 Self Attention",
    "text": "4.4 Self Attention\n\nLSTMs difficult to parallelize\nStill not effective for very long dependencies. Bahdanau attention etc. hacks needed.\nTransformers - Replace recurrent layers with self attention layers\nSelf Attention Mechanism\n\nMap input to output of same length\nAt step t, model has access to all inputs upto step t\n\nHelps with auto-regressive generation\n\nComputation for step t is independent of all other steps\n\nEasy parallelization\n\nCompare current input to the collection which reveals its relevance in the given context\n\\(y_3\\) is generated by comparing \\(x_3\\) to \\(x_1, x_2, x_3\\)\n\nCore of Attention Approach\n\nComparison is done using dot product operations (large value, more similar)\n\n\\(\\text{score}(x_i, x_j) = x_i . x_j\\)\n\nCompute attention weights\n\n\\(\\alpha_{ij} = \\text{softmax}(\\text{score}(x_i, x_j))\\)\n\nCompute output\n\n\\(y_i = \\sum \\alpha_{ij} x_j\\)\n\n\nSophistication wrt Transformers\n\nEach input can play three different roles\n\nQuery: When it’s being compared to other inputs (Current focus)\nKey: When it’s acting as context (previous input) fo comparison\nValue: When it’s being used to compute the output\n\nFor each role, there exists a separate embedding matrix\n\n\\(\\text{score}(x_i, x_j) = q_i . k_j / \\sqrt d\\)\n\\(y_i = \\sum \\alpha_{ij} v_j\\)\nNormalization to avoid overflow in softmax layer\n\nSince calculations are independent, use matrix multiplications\nUse masking to avoid peeking into the future\n\nTransformer Block\n\nAttention layer followed by FFNN with residual connections and layer norm\n\\(z = \\text{Layer Norm}(x + \\text{Self Attention}(x))\\)\n\\(y = \\text{Layer Norm}(z + \\text{FFNN}(z))\\)\nLayer Norm dies normalization across the hidden dimension\n\nMulti-Head Attention\n\nWords can exhibit different interrelationships (syntactic, semantic etc.)\nParallel layers to capture each of the underlying relationships\nConcatenate the output from each of the heads\n\nPositional Embeddings\n\nShuffling input order should matter\nSelf-attention logic (unlike RNNs) doesn’t respect sequence\nPositional embeddings modify the input embedddings based on the position in the sequence\nSize and Cosine functions\n\nBERT Architecture\n\nBase Model - 12 heads, 12 layers, 64 diemnsions, 768 size (12 * 64)\nLarge Model - 16 heads, 24 layers, 64 dimensions, 1024 size (16 * 64)"
  },
  {
    "objectID": "jfsky-05-encoder.html#sequence-models",
    "href": "jfsky-05-encoder.html#sequence-models",
    "title": "5  Encoder-Decoder Models",
    "section": "5.2 Sequence Models",
    "text": "5.2 Sequence Models\n\nModels are autoregressive by nature\nAdd a BOS token  for conditional generation\nKeep sampling till EOS token \nRNN / LSTM\n\nEncoder\n\nProcess the input sequence token by token\n\nContext vector\n\nUse the final hidden state of LSTM as the context vector\n\nDecoder\n\nUse the context vector for initialization\nUse BOS token for generation\n\nDrawback: Influence of context vector wanes as longer sequences are generated\n\nSolution is to make context vecotr available for each timestep of the decoder\n\nTraining happens via teacher forcing\n\nTransformers\n\nUses Cross-Attention for decoding\nKeys and values come from encoder but query comes from decoder\nAllows decoder to attend to each token of the input sequence\n\nTokenization\n\nBPE / Wordpiece tokenizer"
  },
  {
    "objectID": "jfsky-05-encoder.html#evaluation",
    "href": "jfsky-05-encoder.html#evaluation",
    "title": "5  Encoder-Decoder Models",
    "section": "5.3 Evaluation",
    "text": "5.3 Evaluation\n\nHuman Evaluation\n\nAdequacy: How accurate is the meaning\nFluency: Grammatical correctness\n\nAutomatic Evaluation\n\nchrF Score: Character F-Score\nBLEU: Bilingual Evaluation Understudy\n\nn-gram precision: Compares n-gram of source with n-gram of output\nAdd a brevity penalty for best match length\n\nBERTScore\n\nPass the sequences to BERT\nCompute embeddings for each token\nCompute cosine similarity for ech pair of tokens\nMatch the tokens greedily and compute precision and recall ## Attention\n\n\nFinal hidden state acts as the bottleneck\nAttention mechanism helps the decoder to acess all the intermediate hidden states and not just the last one\nGenerate the context vector using weighted sum of all encoder states\nReplces the static context vector with one dynmically derived from encoder hidden states\nConsine similarity between decoder hidden state at time t wrt encoder hidden states"
  },
  {
    "objectID": "jfsky-05-encoder.html#decoding",
    "href": "jfsky-05-encoder.html#decoding",
    "title": "5  Encoder-Decoder Models",
    "section": "5.4 Decoding",
    "text": "5.4 Decoding\n\nPlain vanilla greedy decoding selects the argmax results over the the vocab to generate the output\n\nSelect the highest probability token\n\nThe overall results may be suboptimal. A token that looks good now may turn out to be wrong later\nUse search trees.\n\nThe most probable sequence may not be composed of argmax tokens at each step\nExhaustive search is too slow\n\nBeam Search\n\nSelect top-k possible tokens at each time step (beam width) (BFS approach)\nEach of the “k” hypothesis is passed incrementally to distinct decoders\nThe process continues until  token is sampled\nSeach continues untill ll the beams converge\nLonger sequences are penalized. Normalization is required\nUsually k is between 5 and 10\n\nTop-K Sampling\n\nTop-K tokens are sleected and the probability mas is redistributed among them\n\nTop-P Sampling\n\nInstead of selecting the top-k tokens, select the set of tokens whos eprobability mass exceeds threshold"
  },
  {
    "objectID": "jfsky-06-transfer.html#pre-training",
    "href": "jfsky-06-transfer.html#pre-training",
    "title": "6  Transfer Learning",
    "section": "7.1 Pre-Training",
    "text": "7.1 Pre-Training\n\nFill-in-the-blank or Cloze task\nPredict the “masked” words (MLM)\nUse CE loss over the vocab to drive training\nSelf-supervised Learning\nMLM\n\nRequires unannotated large text corpus\nRandom sample (15%) of tokens is chosen to masked for learning\n80% replaced with [MASK]\n10% replaced with random word\n10% replaced left unchanged\nPredict original token for each of the masked input\n\nSpan\n\nContiguous sequence of one or more words\nRandomly selected spans from training sequence\nSpan length selected from geometric distribution\nStarting location is slelected from uniforma distribution\nOnce the span is selected, all the words within the span are substituted\nLearning objective: MLM + Span Boundary Objective (SBO)\nPredict words in the span using the starting and ending token and positional embeddings\n\nNSP\n\nNext Sentence prediction\nParaphrase, entailment and discourse coherence\nActual pair of adjacent sentences or not\nDistinguish true paris from random pairs\n\nTraining Data\n\nBooks corpus\nEnglish Wiki"
  },
  {
    "objectID": "jfsky-06-transfer.html#fine-tuning",
    "href": "jfsky-06-transfer.html#fine-tuning",
    "title": "6  Transfer Learning",
    "section": "7.2 Fine-Tuning",
    "text": "7.2 Fine-Tuning\n\nCreation of applications on top of pretrained models leveraging the generalizations from SSL\nLimited mount of labeled data\nFreeze or minimal adjsutments to pretrained weights\nSequence Classification\n\n[CLS] token embedding\nClassifier head\n\nNLI\n\nRecognize contradiciton, entailment and neutral\nCLS token for premise. [CLS] premise [SEP] text [SEP]\n\nSequence Labeling\n\nPrediction for each token\nSoftmax over label classes\nBIO tags (beginning, inside, outside)\nWord Peice Tokenization creates challenge\n\nTraning expand the tags\nScoring use tag assigned to the first subword token\n\n\nSpan based representations\n\nMiddle ground between token level and sequence level classifications\nGenerate possible spans\nAverage the embeddings within the spans\nSpan represenatations: concatenate [start, end and average] embeddings\nUse regression to predict start and end tokens"
  },
  {
    "objectID": "eslr-00.html",
    "href": "eslr-00.html",
    "title": "ESLR Notes",
    "section": "",
    "text": "Resource: Elements of Statistical Learning\nTopics Covered:\n\nChapter 3: Linear Regression\nChapter 4: Linear Classification\nChapter 6: Kernel Methods\nChapter 7: Model Assessment\nChapter 8: Model Inference\nChapter 9: Additive Models\nChapter 10: Boosting\nChapter 15: Random Forests"
  },
  {
    "objectID": "eslr-02-classification.html",
    "href": "eslr-02-classification.html",
    "title": "8  Classification",
    "section": "",
    "text": "Classificaiton approach is to learn a discriminant function \\(\\delta_k(x)\\) for each class\n\nClassify x to the class with largest discriminant value\n\nThe decision boundary is linear if\n\n\\(\\delta_k(x)\\) is linear\nPosterior prbability is linear \\(P(G=k|X=x)\\)\nTheir monotonic transformation is linear\n\nLinear Decision Boundary: \\(f_k(x) = \\beta_{0k} + \\beta_k x\\)\nDecision Boundary between two classes (k, l) is the set of points where \\(f_k(x) = f_l(x)\\)\n\n\\(\\{x : (\\beta_{0k} - \\beta_{0l}) + (\\beta_k - \\beta_l) x = 0\\}\\)\nAffine set or a hyperplane\n\nExample: Binary Logistic Regression\n\n\\(P({G=1 \\over X=x}) = \\frac{\\exp(\\beta x)}{1 + \\exp(\\beta x)}\\)\n\\(P({G=0\\over X=x}) = \\frac{1}{1 + \\exp(\\beta x)}\\)\n\\(\\log({P(G=1 | X=x) \\over p(G=0 | X=x)}) = x \\beta\\)\nLog-odds transformation gives linear decision boundary\nDecsion boundary is the set of points \\(\\{x| \\beta x = 0\\}\\)"
  },
  {
    "objectID": "eslr-03-kernel-methods.html",
    "href": "eslr-03-kernel-methods.html",
    "title": "9  Kernel Methods",
    "section": "",
    "text": "A random sample \\([x_0, x_1, ... x_n]\\) is drawn from probability distribution \\(f_X(x)\\)\nParzen Estimate of \\(\\hat f_X(x)\\)\n\n\\(\\hat f_X(x_0) = {1 \\over N \\lambda} \\,\\, \\# x_i \\in N(x_0)\\)\n\\(\\lambda\\) is width of the neighbourhood\nThe esitmate is bumpy\n\nGaussian Kernel of width \\(\\lambda\\) can be a choice of Kernel\n\n\\(\\hat f_X(x_0) = {1 \\over N \\lambda} K_{\\lambda}(x_i, x_0)\\)\n\\(K_{\\lambda}(x_i, x_0) = \\phi(\\|x_i - x_0\\|) / \\lambda\\)\nWeight of point \\(x_i\\) descreases as distance from \\(x_0\\) increases\n\nNew estimate for density is\n\n\\(\\hat f_X(x_0) = {1 \\over N } \\phi_{\\lambda}(x_ - x_0)\\)\nConvolution of Sample empirical distribution with Gaussian Kernel"
  },
  {
    "objectID": "eslr-04-model-assessment.html",
    "href": "eslr-04-model-assessment.html",
    "title": "10  Model Selection",
    "section": "",
    "text": "Generalization\n\nPrediction error over an independent test sample\n\n\\(\\text{ERR}_T = E(L(Y, \\hat f(x)) | T)\\)\n\nT refers to the training set used to build the model\nL is the loss function used to evaluate the model performance\n\nRegression: Squared Loss, Absolute Loss\nClassification: 0-1 Loss, Deviance (-2 x LL)\n\n\nAs the model becomes more complex, it adaps to complex underlying structure of the training data\n\nDecrease in bias but increase in variance\nIf the underlying training data changes, the complex fitted model will change to a large extent\n\nIntermediate model complexity that gives minimum expected test error\nTraining error is not a good estimate of test error\n\nConsistently decreases with increasing model complexity\nPoor generalization\n\nModel Selection\n\nEstimating Performance of different models to choose the best one\n\nModel Assessment\n\nHaving selected the model, estimating the generalization error on new, unseen data\n\n\nDivide the dataset\n\nTraining: Fit the models\nValidaiton: Estimate model prediction error for model selection\nTest: Generalizaiton error of the final chosen model"
  },
  {
    "objectID": "eslr-07-model-assessment.html",
    "href": "eslr-07-model-assessment.html",
    "title": "11  Model Selection",
    "section": "",
    "text": "11.0.2 Bias-Variance Decomposition\n\n\\(Y = f(X) + \\epsilon\\)\n\n\\(E(\\epsilon) = 0, V(\\epsilon) = \\sigma^2_{\\epsilon}\\)\n\n\\(\\text{ERR}(x_0) = E((Y - \\hat f(x_0))^2 | x_0)\\)\n\n\\(\\text{ERR}(x_0) = E((f(x_0) + \\epsilon - \\hat f(x_0))^2)\\)\n\\(\\text{ERR}(x_0) = \\sigma^2_{\\epsilon} + [E(\\hat f(x_0) - f(x_0))]^2 + E[\\hat f(x_0) - E(\\hat f(x_0)]^2\\)\nMSE = Irreducible Error + Bias Squared + Variance\n\nBias: Difference between average of estimate and true mean\nVariance: Squared Deviation of model around its mean\nMore Complex Model\n\nLower Bias\nHigher Variance\n\nFor linear Model\n\n\\(\\text{Variance} \\propto p\\)\n\nComplexity of the model is related to the number of parameters\n\n\\(\\text{Bias}^2 = \\text{Model Bias}^2 + \\text{Estimation Bias}^2\\)\n\nModel Bias: Best fitting linar model and True function\nEstimation Bias: Estimated Model and Best fitting linar model\n\nFor OLS: Estimation Bias is 0, BLUE\nFor Ridge: Estimation Bias is positive\n\nTrade-off with reduction in variance\n\n\n\n\n\n11.0.3 Optimism of Training Error\n\n\\(\\text{ERR} = E(\\text{ERR}_T)\\)\nTraining error is less than test error\n\nSame data is being used to train and evaluate the model\n\nOptimistic estimate of generalization error\n\n\\(\\text{ERR}_{in}\\):\n\nError between sample and populaiton regression function estimates on training data\n\n\\(\\bar{\\text{err}}\\)\n\nAverage sample regression error over training data\n\n\nOptimisim in training error estimate\n\n\\(\\text{op} = \\text{ERR}_{in} - \\bar{\\text{err}}\\)\nRelated to \\(\\text{cov}(y, \\hat y)\\)\nHow strongly a label value affects its own prediction\n\nOptimism increases with number of inputs\nOptimism decreases with number of training samples\n\n\n\n11.0.4 In-sample Prediciton Error\n\n\\(\\text{ERR}_{in} = \\bar{\\text{err}} + \\text{op}\\)\nCp Statistic\n\n\\(C_p = \\bar{\\text{err}} + 2{p \\over N} \\sigma^2_{\\epsilon}\\)\np is the effective number of parameters\n\nAIC\n\n\\(\\text{AIC} = {-2 \\over N} LL + 2{p \\over N}\\)\np is the effective number of parameters\nFor model selection, choose the one with lowest AIC\n\nEffective Number of Parameters\n\nLinear Regression: \\(\\hat y = ((X'X)^{-1}X')y\\)\nRidge Regression: \\(\\hat y = (((X'X)^{-1} + \\lambda I)X')y\\)\nGeneralized Form: \\(\\hat y = S y\\)\np is the trace of the S Matrix\n\nBIC\n\n\\(\\text{BIC} = {-2 \\over N} LL + \\log N \\times p\\)\nPenalizes complex models more heavily compared to AIC\nBayesian Approach\n\n\\(P(M |D) \\propto P(D |M) P(M)\\)\n\nLaplace Approximation\n\n\\(\\log P(D |M) = \\log P(D |M, \\theta) - p \\log N\\)\n\\(\\log P(D |M, \\theta)\\) is the MLE objective function\n\nCompare two models\n\n\\(P(M1 |D) / P(M2 |D) = P(M1) / P(M2) + P(D | M1) / P(D | M2)\\)\nThe first term is constant (non-informative priors)\nThe second term the Bayes Factor\n\n\n\n\n\n11.0.5 VC Dimension\n\nAIC, C-p statistic need the information on model complexity\n\nEffective number of parameters\n\nDifficult to estimate for non-linear models\nVC Dimension is Generalized Model Complexity of a class of functions\n\nHow “wiggly” can the memeber of this class be?\n\nShattering\n\nPoints that can be perfectly separated by a class of functions, no matter how the binary labels are assigned\n\nVC Dimension: Largest number of points that can be shattered by members of class of functions\n3 points in case of linear classifier in a plane\n\n4 points can lead to XOR\n\n\n\n\n11.0.6 Cross Validation\n\nEstimation for \\(\\text{ERR}_T\\)\nData scarce situation\nDivide data into K equal parts\n\nIndexing function: \\(\\kappa : \\{1,2,....N\\} \\rightarrow \\{1, 2 ... K\\}\\)\n\nFit model on K-1 parts and predict on Kth part\nCross Validaiton Error\n\n\\(CV(f) = {1 \\over N}\\sum L(y_i, \\hat y_i^{f_{-\\kappa}})\\)\n\n5-fold, 10-fold cross validation is recommended\n\n\n\n11.0.7 Boostrap Methods\n\nEstimation for \\(\\text{ERR}\\)\nRandomly draw datasets from training data by sampling with replacement\n\nEach dataset has the same size of original training data\n\nFit the model on each of the bootstrap datasets\n\\(\\text{ERR}_{\\text{boot}} = {1 \\over B}{1 \\over N} \\sum_B \\sum_N L(y, \\hat y)\\)\nBootstrap uses overlapping samples across model fits (unlike cross validation)\n\n\\(P(i \\in B) = 1 - (1 - {1 \\over N})^N \\approx 1 - e^{-1}\\)\n\n\\(\\text{ERR}_{\\text{boot}}\\) isn’t a good estimator becuase of leakage\nUse Out-of-bag error instead\n\nSamples which have been dropped by boostrap"
  },
  {
    "objectID": "eslr-08-model-selection.html",
    "href": "eslr-08-model-selection.html",
    "title": "11  Model Selection",
    "section": "",
    "text": "Maximum Likelihood Inference\n\nParametric Model\n\nRandom variable $ z_i g_(z)$\nUnknown Parameters \\(\\theta = (\\mu, \\sigma^2)\\)\n\nLikelihood Function\n\n\\(L(\\theta, Z) = \\prod g_\\theta(z_i)\\)\nProbability of observed data under the model \\(g_\\theta\\)\n\nMaximize the Likelihood function\n\nSelect the parameters \\(\\theta\\) such that the probability of observed data is maximized under the model\n\nScore Function \\(\\delta L \\over \\delta \\theta\\)\nInformation Matrix \\(I(\\theta) = \\delta^2 L \\over \\delta \\theta^2\\)\nFisher Information \\(i(\\theta) = I(\\theta)_{\\hat \\theta}\\)\n\nSampling Distribution of MLE has limiting normal distribution\n\n\\(\\theta \\rightarrow N(\\hat \\theta, I(\\hat \\theta)^{-1})\\)\n\nOLS estimates are equivalent to MLE estimates for Linear Regression\n\n\\(\\text{VAR}(\\hat \\beta) = \\sigma^2 / S_{xx}\\)\n\\(\\text{VAR}(\\hat y_i) = \\sigma^2 X^* (X'X^-1) X^*\\)"
  },
  {
    "objectID": "eslr-09-additive-models.html",
    "href": "eslr-09-additive-models.html",
    "title": "12  Additive Models",
    "section": "",
    "text": "Linear models fail to capture non-linear trends\nAdditive models an alternative\n\n\\(g[\\mu(X)] = \\alpha + f(X_1) + f(X_2)....\\)\n\\(f(x)\\) are non-parametric smoothing functions (say cubic splines)\n\\(\\mu(x)\\) is the conditional mean\n\\(g(x)\\) is the link functions\n\nidentity, logit, log-linear etc.\n\n\n\nEstimation using Penalized Sum Squares (PRSS)\nThe coefficients of the regression are replaced with a flexible function (say spline)\n\nAllows for modeling non-linear relationships"
  },
  {
    "objectID": "eslr-15-random-forest.html",
    "href": "eslr-15-random-forest.html",
    "title": "14  Random Forests",
    "section": "",
    "text": "Decision trees have low bias and high variance\nBagging or bootstrap aggreagation aims to reduce the variance of these classifiers\n\nAverage many noisy unbiased models\n\nVariance of mean of B random variables (say prediciton form trees)\n\nIndividual Variance: \\(\\sigma^2\\)\nPairwise Correlation: \\(\\rho\\)\n\\(\\rho \\sigma^2 + {(1 - \\rho) \\over B} \\sigma^2\\)\nIncrease in B, cannot cause the forest to overfit.\n\nFor gains, reduce the correlation between trees\n\nRandom feature subset selection\nBoostrap Sampling\n\nOOB Error\n\nErrors on observations not selected in Bootstrap Sampling\nIdentical to CV error"
  },
  {
    "objectID": "eslr-01-regression.html#sampling-distribution-of-beta",
    "href": "eslr-01-regression.html#sampling-distribution-of-beta",
    "title": "7  Regression",
    "section": "7.2 Sampling Distribution of \\(\\beta\\)",
    "text": "7.2 Sampling Distribution of \\(\\beta\\)\n\nDeviations around the conditional mean are Gaussian\n\\(Var(\\hat \\beta) = (X^T X)^{-1} \\sigma^2\\)\nEstimate of Sigma can be done by looking at sample variance\n\\(\\hat \\sigma^2 = {1 \\over N-p} \\sum (y_i - \\hat y_i)^2\\)\n\\(\\hat \\beta \\sim N(\\beta, (X^T X)^{-1} \\sigma^2)\\)"
  },
  {
    "objectID": "eslr-01-regression.html#statistical-significance",
    "href": "eslr-01-regression.html#statistical-significance",
    "title": "7  Regression",
    "section": "7.3 Statistical Significance",
    "text": "7.3 Statistical Significance\n\n\\(Z_i = {\\beta_i \\over SE_i} = \\frac{\\beta_i}{\\hat \\sigma \\sqrt v_i}\\)\n\nv is the diagnoal element of \\((X^T X)^{-1}\\)\n\nTesting significance for a group of parameters\n\nSay categorical variables with all k variables\n\\(F = (RSS_0 - RSS_1) / (RSS_1 / N - p)\\)\nChange is RSS of the bigger model normalized by the estimate of variance"
  },
  {
    "objectID": "eslr-01-regression.html#gauss-markov-theorem",
    "href": "eslr-01-regression.html#gauss-markov-theorem",
    "title": "7  Regression",
    "section": "7.4 Gauss-Markov Theorem",
    "text": "7.4 Gauss-Markov Theorem\n\nAmong all the unbiased estimators, the least square estimates have lowest variance\n\\(E(Y_0 - \\hat Y_0))^2 = \\sigma^2 + MSE(\\hat f(X_0))\\)"
  },
  {
    "objectID": "eslr-01-regression.html#subset-selection",
    "href": "eslr-01-regression.html#subset-selection",
    "title": "7  Regression",
    "section": "7.5 Subset Selection",
    "text": "7.5 Subset Selection\n\nSelect only a few variables for better interpretability\nBest subset selection os size K is the one that yields minimum RSS\nForward Selection\n\nSequentially add one variable that most improves the fit\nQR decomposition / successive orthogonalization to look at correlation\n\nBackward Selection\n\nSequentially delete the variable that has least impact on the fit\nZ Score\n\nHybrid Stepwise Selection\n\nConsider both forward and backward moves at each step\nAIC for weighting the choices\n\nForward Stagewise Selection\n\nAdd the variable most correlated with current residual\nDon’t re-adjust the coefficients of the existing variables"
  },
  {
    "objectID": "eslr-01-regression.html#shrinkage-methods",
    "href": "eslr-01-regression.html#shrinkage-methods",
    "title": "7  Regression",
    "section": "7.6 Shrinkage Methods",
    "text": "7.6 Shrinkage Methods\n\nShinkage methods result in biased estimators but a large reduction in variance\nMore continuous and dont suffer from high variability\nRidge Regression\n\nImpose a penalty the size of the coefficicents\n\\(\\hat \\beta^{\\text{ridge}} = \\arg \\min (y - X \\beta)^T(y - X \\beta) + \\lambda \\sum \\beta^2\\)\n\\(\\hat \\beta^{\\text{ridge}} = \\arg \\min (y - X \\beta)^T(y - X \\beta) \\; \\text{subject to} \\sum \\beta^2 \\le t\\)\nt is the budget\nIn case of correlated variables, coefficcients are poorly determined\nA large positive coefficient of a variable is canceled by a large negative coefficient of the correlated variable\nSolution not invariant to scaling. Standardize the inputs and don’t impose penalty on intercept\n\\(\\hat \\beta^{\\text{ridge}} = (X^T X + \\lambda I)^{-1}(X^Ty)\\)\nIn case of correlated predictors, the original \\((X^T X)\\) wasn’t full rank. But by adding noise to diagonal elements, the matrix can now be inverted.\nIn case of orthonormal inputs (PCA), the ridge coefficicents are scaled versions of the original least-square estimates.\n\\(\\lambda\\) controls the degrees of freedom. A large value results in effectively dropping the variables.\n\nLasso Regression\n\n\\(\\hat \\beta^{\\text{ridge}} = \\arg \\min (y - X \\beta)^T(y - X \\beta) + \\lambda \\sum |\\beta|\\)\nNon-linear optimization\nA heavy restriction on budget makes some coefficients exactly zero\nContinuous subset selection\nComparison between Ridge and Laso\n\nRidge represents a disk \\(\\beta_1^2 + \\beta_2^2 <= t\\)\nLasso represents a rhombus \\(|\\beta_1| + |\\beta_2| <= t\\)\nAt optimal value, the estimated parameters can be exactly zero (corner solutions)\nBayesian MAP estimates with different priors\n\nLasso has Laplace Prior\nRidge has Gaussian Prior\n\n\nElastic Net\n\n\\(\\lambda \\sum \\alpha \\beta^2 + (1 - \\alpha) |\\beta|\\)\nVariable selection like Lasso\nShinking coefficients like Ridge"
  },
  {
    "objectID": "eslr-01-regression.html#partial-least-squares",
    "href": "eslr-01-regression.html#partial-least-squares",
    "title": "7  Regression",
    "section": "7.7 Partial Least Squares",
    "text": "7.7 Partial Least Squares\n\nAlternative approach to PCA deal with correlated features\nSupervised transformation\nPrincipal component regression seeks directions that have high variance\nPartial Least Square seeks direction with high variance and high correlation with response\nDerive new features by linear combination of raw variables re-weighted by the correlation"
  },
  {
    "objectID": "eslr-02-classification.html#linear-probability-model",
    "href": "eslr-02-classification.html#linear-probability-model",
    "title": "8  Classification",
    "section": "8.2 Linear Probability Model",
    "text": "8.2 Linear Probability Model\n\nEncode each of the k classes with an indicator function \\(Y_{N \\times K}\\)\nFit a regression model to each of the classes simulatneously\n\n\\(\\hat \\beta = (X'X)^{-1}(X'Y)\\)\n\\(\\hat Y = X \\hat \\beta\\)\n\nDrawbacks\n\nPredictions can be outside range (0,1)\nClasses can be masked by others\n\nLarge number of classes with small number of features\nPossible that one of the classes (say 2) gets dominated thoughout by the other classes (1,3)\nThe model will never predict for class 2"
  },
  {
    "objectID": "eslr-02-classification.html#linear-and-quadratic-discriminant-analysis",
    "href": "eslr-02-classification.html#linear-and-quadratic-discriminant-analysis",
    "title": "8  Classification",
    "section": "8.3 Linear and Quadratic Discriminant Analysis",
    "text": "8.3 Linear and Quadratic Discriminant Analysis\n\nBayes theorem\n\nposterior \\(\\propto\\) prior x likelihood\n\\(P(G = k | X= x) = \\frac{f_k(x) \\times \\pi_k}{\\sum_k f_k(x) \\times \\pi_k}\\)\n\\(f_k(x)\\) is the discriminant function\n\\(\\pi_k\\) is the prior estimate\n\nNaive Bayes assumes each of the class densities are product of marginal densities.\n\nInputs are conditionally independent of each class\n\nLDA (and QDA) assumes the discriminant function to have MVN probability density function\nLDA makes the assumption that the covariance martix (for MVN) is common for all the classes\nDiscrimination Function\n\n\\(f_k(x) = \\frac{1}{(2\\pi)^{p/2} \\Sigma^{1/2}} \\exp\\{(X - \\mu)^T \\Sigma^{-1} (X - \\mu)\\}\\)\n\nDecision Boundary\n\n\\(\\log(\\frac{P(G=k | X=x)}{P(G=l | X=x)}) = C + X^T \\Sigma^{-1}(\\mu_k - \\mu_l)\\)\nLinear in X\nThe constant terms can be grouped together because of common covariance matrix\n\nEstimation\n\n\\(\\pi_k = N_k / N\\)\n\\(\\mu_k = \\sum_{i \\in K} x_i / N_k\\)\n\\(\\Sigma = \\sum_k \\sum_{i \\in K} (x_i - \\mu_k)^T(x_i - \\mu_k) / N_k\\)\n\nQDA relaxes the assumtion of contant covariance matrix\n\nIt assumes a class specific covariance matrix\nDiscrimination function becomes quadratic in x\nThe number of parameters to be estimated grows considerably\n\nRegularization\n\nCompromise between LDA and QDA\nShrink the individual covariances of QDA towards LDA\n\\(\\alpha \\Sigma_k + (1 - \\alpha) \\Sigma\\)\n\nComputation\n\nSimplify the calculation by using eigen decomposition of the covariance matrix \\(\\Sigma\\)"
  },
  {
    "objectID": "eslr-02-classification.html#logistic-regression",
    "href": "eslr-02-classification.html#logistic-regression",
    "title": "8  Classification",
    "section": "8.4 Logistic Regression",
    "text": "8.4 Logistic Regression\n\nModel posterior probabilities via separate functions while ensuring the output remains in the range [0,1]\n\n\\(P({G=1 \\over X=x}) = \\frac{\\exp(\\beta x)}{1 + \\exp(\\beta x)}\\)\n\\(P({G=0\\over X=x}) = \\frac{1}{1 + \\exp(\\beta x)}\\)\n\nEstimation is done by maximizing conditional log-likelihood\n\n\\(LL(\\beta) = \\sum y_i(\\log(p(x_i, \\beta)) + (1 - y_i) (1 - \\log(p(x_i, \\beta))\\)\n\\(LL(\\beta) = \\sum y (x \\beta) + \\log(1 + \\exp x \\beta)\\)\nNormal Equation\n\n\\(\\frac{\\delta LL}{\\delta \\beta} = \\sum x_i (y_i - p(x_i, \\beta)) = 0\\)\n\n\nOptimization\n\nNon-linear function of parameters\nUse Newton-Raphson method\nSeond Order Derivative or Hessian\n\n\\(\\frac{\\delta^2 LL}{\\delta \\beta^2} = \\sum x_i x_i^T p(x_i, \\beta) (1 - p(x_i, \\beta))\\)\n\nThe second order derivative is positive, hence it’s a convex optimization problem\nIRLS (Iteratively Weighted Least Squares) algorithm\n\nGoodness of Fit\n\nDeviance = \\(-2 (\\log L_M - \\log L_S)\\)\nL(M): LL of Current Model\nL(S) LL of Saturated Model\n\nModel that perfectly fits the data, Constant for a given dataset\n\nCompare two different models by looking at change in deivance\n\nRegularization\n\nLasso penalties can be added to the objective function\nIntercept term isn’t penalized"
  },
  {
    "objectID": "eslr-02-classification.html#comparison-between-lda-and-logistic-regression",
    "href": "eslr-02-classification.html#comparison-between-lda-and-logistic-regression",
    "title": "8  Classification",
    "section": "8.5 Comparison between LDA and Logistic Regression",
    "text": "8.5 Comparison between LDA and Logistic Regression\n\nBoth Logistic and LDA return linear decision boundaries\nDifference lies in the way coefficients are estimated\nLogistic Regression makes less stringent assumptions\n\nLR maximizes conditional log-likelihood\nLDA maximizes full log-likelihood (i.e. joint desnity)\n\nLDA makes more restrictive assumptions about the distributions\n\nEfficiency is estimation\nLess robust to outliers"
  },
  {
    "objectID": "eslr-02-classification.html#percepton-learning-algorithm",
    "href": "eslr-02-classification.html#percepton-learning-algorithm",
    "title": "8  Classification",
    "section": "8.6 Percepton Learning Algorithm",
    "text": "8.6 Percepton Learning Algorithm\n\nMinimize the distance of missclassified points to the separating hyperplane\n\\(D(\\beta) = - \\sum y (x^T \\beta)\\)\nUse SGD to estimate the parameters\nWhen the data is separable, there are many solutions that exist. The final convergence depends on the initialization.\nWhen data isn’t separable, there is no convergence."
  },
  {
    "objectID": "eslr-02-classification.html#maximum-margin-classifiers",
    "href": "eslr-02-classification.html#maximum-margin-classifiers",
    "title": "8  Classification",
    "section": "8.7 Maximum Margin Classifiers",
    "text": "8.7 Maximum Margin Classifiers\n\nMaximize the distance of of points from either class to the hyperplane.\n\\(L = \\max_{\\beta, ||\\beta|| = 1} M \\, \\, \\text{subject to} \\, y_i \\times x_i \\beta >= M \\, \\forall \\, i \\in N\\)\nThe final parameters can be arbitrarily scaled.\n\\(L = \\max {1 \\over 2}||\\beta||^2 \\, \\, \\text{subject to} \\, y_i \\times x_i \\beta >= 1 \\, \\forall \\, i \\in N\\)\nLagrangian Multiplier\n\\(L = \\max {1 \\over 2}||\\beta||^2 - \\sum \\alpha_i (y_i \\times x_i \\beta) - 1)\\)\nTaking derivative wrt to \\(\\beta\\)\n\n\\(\\beta = \\sum \\alpha_i y_i x_i\\)\nParameter is a linear combination of points where the constraints are active \\(\\alpha_i > 0\\)"
  },
  {
    "objectID": "eslr-03-kernel-methods.html#kernel-desnity-classification",
    "href": "eslr-03-kernel-methods.html#kernel-desnity-classification",
    "title": "9  Kernel Methods",
    "section": "9.2 Kernel Desnity Classification",
    "text": "9.2 Kernel Desnity Classification\n\nBayes’ Theorem\n\\(P(G=j | X=x_0) \\propto \\hat \\pi_j \\hat f_j(x_0)\\)\n\\(\\hat \\pi_j\\) is the sample proportion of the class j\n\\(\\hat f_j(x_0)\\) is the Kernel density estimate for class j\nLearning separate class densities may be misleading\n\nDense vs Non-dense regions in feature space\nDensity estimates are critical only near the decision boundary"
  },
  {
    "objectID": "eslr-03-kernel-methods.html#naive-bayes-classifier",
    "href": "eslr-03-kernel-methods.html#naive-bayes-classifier",
    "title": "9  Kernel Methods",
    "section": "9.3 Naive Bayes Classifier",
    "text": "9.3 Naive Bayes Classifier\n\nApplicable when dimension of feature space is high\nAssumption: For a given class, the features are independent\n\n\\(f_j(x) = \\prod_p f_{jp}(x_p)\\)\nRarely holds true in real world dataset\n\n\\(\\log \\frac{P(G=i|X=X)}{P(G=j|X=X)} = \\log \\frac{\\pi_i \\prod f_{ip}(x_p)}{\\pi_j \\prod f_{jp}(x_p)}\\)\n\n\\(\\log \\frac{\\pi_i}{\\pi_j} + \\sum \\log \\frac{f_{ip}(x_p)}{f_{jp}(x_p)}\\)"
  },
  {
    "objectID": "eslr-03-kernel-methods.html#radial-basis-functions",
    "href": "eslr-03-kernel-methods.html#radial-basis-functions",
    "title": "9  Kernel Methods",
    "section": "9.4 Radial Basis Functions",
    "text": "9.4 Radial Basis Functions\n\nBasis Functions \\(f(x) = \\sum \\beta h(x)\\)\nTransform lower dimension features to high dimensions\nData which is not linearly separable in lower dimension may become linearly separable in higher dimensions\nRBF treats gaussian kernel functions as basis functions\n\nTaylor series expansion of \\(\\exp(x)\\)\nPolynomial basis function with infinite dimensions\n\n\\(f(x) = \\sum_j K_{\\lambda_j}(\\xi_j, x) \\beta_j\\)\n\\(f(x) = \\sum_j D({\\|x_i - \\xi_j\\| \\over \\lambda _j}) \\beta_j\\)\n\nD is the standard normal gaussian density function\n\nFor least square regression, SSE can be optimized wrt to \\(\\beta, \\xi, \\lambda\\)\n\nNon-Linear Optimization\nUse greedy approaches like SGD\n\nSimplify the calculations by assuming \\(\\xi, \\lambda\\) to be hyperparameters\n\nUse unsupervised learning to estimate them\nAssuming constant variance simplifies calculations\n\nIt can create “holes” where none of the kernels have high density estimate"
  },
  {
    "objectID": "eslr-03-kernel-methods.html#mixture-models",
    "href": "eslr-03-kernel-methods.html#mixture-models",
    "title": "9  Kernel Methods",
    "section": "9.5 Mixture Models",
    "text": "9.5 Mixture Models\n\nExtension of RBF\n\\(f(x) = \\sum_j \\alpha_j \\phi(x, \\mu_j, \\Sigma_j)\\)\n\\(\\sum \\alpha_j = 1\\), are the mixing proporitons\nGaussian mixture models use Gaussian kernel in place of \\(\\phi\\)\nParameters are fit using Maximum Likelihood\nIf the covariance martix is restricted to a diagonal matrix \\(\\Sigma = \\sigma^2 I\\), then it reduces to radial basis expansion\nClassification can be done via Bayes Theorem\n\nSeparate density estimation for each class\nProbability is \\(\\propto \\hat \\pi_i f_j(x)\\)"
  },
  {
    "objectID": "eslr-04-model-assessment.html#bias-variance-decomposition",
    "href": "eslr-04-model-assessment.html#bias-variance-decomposition",
    "title": "10  Model Selection",
    "section": "10.2 Bias-Variance Decomposition",
    "text": "10.2 Bias-Variance Decomposition\n\n\\(Y = f(X) + \\epsilon\\)\n\n\\(E(\\epsilon) = 0, V(\\epsilon) = \\sigma^2_{\\epsilon}\\)\n\n\\(\\text{ERR}(x_0) = E((Y - \\hat f(x_0))^2 | x_0)\\)\n\n\\(\\text{ERR}(x_0) = E((f(x_0) + \\epsilon - \\hat f(x_0))^2)\\)\n\\(\\text{ERR}(x_0) = \\sigma^2_{\\epsilon} + [E(\\hat f(x_0) - f(x_0))]^2 + E[\\hat f(x_0) - E(\\hat f(x_0)]^2\\)\nMSE = Irreducible Error + Bias Squared + Variance\n\nBias: Difference between average of estimate and true mean\nVariance: Squared Deviation of model around its mean\nMore Complex Model\n\nLower Bias\nHigher Variance\n\nFor linear Model\n\n\\(\\text{Variance} \\propto p\\)\n\nComplexity of the model is related to the number of parameters\n\n\\(\\text{Bias}^2 = \\text{Model Bias}^2 + \\text{Estimation Bias}^2\\)\n\nModel Bias: Best fitting linar model and True function\nEstimation Bias: Estimated Model and Best fitting linar model\n\nFor OLS: Estimation Bias is 0, BLUE\nFor Ridge: Estimation Bias is positive\n\nTrade-off with reduction in variance"
  },
  {
    "objectID": "eslr-04-model-assessment.html#optimism-of-training-error",
    "href": "eslr-04-model-assessment.html#optimism-of-training-error",
    "title": "10  Model Selection",
    "section": "10.3 Optimism of Training Error",
    "text": "10.3 Optimism of Training Error\n\n\\(\\text{ERR} = E(\\text{ERR}_T)\\)\nTraining error is less than test error\n\nSame data is being used to train and evaluate the model\n\nOptimistic estimate of generalization error\n\n\\(\\text{ERR}_{in}\\):\n\nError between sample and populaiton regression function estimates on training data\n\n\\(\\bar{\\text{err}}\\)\n\nAverage sample regression error over training data\n\n\nOptimisim in training error estimate\n\n\\(\\text{op} = \\text{ERR}_{in} - \\bar{\\text{err}}\\)\nRelated to \\(\\text{cov}(y, \\hat y)\\)\nHow strongly a label value affects its own prediction\n\nOptimism increases with number of inputs\nOptimism decreases with number of training samples"
  },
  {
    "objectID": "eslr-04-model-assessment.html#in-sample-prediciton-error",
    "href": "eslr-04-model-assessment.html#in-sample-prediciton-error",
    "title": "10  Model Selection",
    "section": "10.4 In-sample Prediciton Error",
    "text": "10.4 In-sample Prediciton Error\n\n\\(\\text{ERR}_{in} = \\bar{\\text{err}} + \\text{op}\\)\nCp Statistic\n\n\\(C_p = \\bar{\\text{err}} + 2{p \\over N} \\sigma^2_{\\epsilon}\\)\np is the effective number of parameters\n\nAIC\n\n\\(\\text{AIC} = {-2 \\over N} LL + 2{p \\over N}\\)\np is the effective number of parameters\nFor model selection, choose the one with lowest AIC\n\nEffective Number of Parameters\n\nLinear Regression: \\(\\hat y = ((X'X)^{-1}X')y\\)\nRidge Regression: \\(\\hat y = (((X'X)^{-1} + \\lambda I)X')y\\)\nGeneralized Form: \\(\\hat y = S y\\)\np is the trace of the S Matrix\n\nBIC\n\n\\(\\text{BIC} = {-2 \\over N} LL + \\log N \\times p\\)\nPenalizes complex models more heavily compared to AIC\nBayesian Approach\n\n\\(P(M |D) \\propto P(D |M) P(M)\\)\n\nLaplace Approximation\n\n\\(\\log P(D |M) = \\log P(D |M, \\theta) - p \\log N\\)\n\\(\\log P(D |M, \\theta)\\) is the MLE objective function\n\nCompare two models\n\n\\(P(M1 |D) / P(M2 |D) = P(M1) / P(M2) + P(D | M1) / P(D | M2)\\)\nThe first term is constant (non-informative priors)\nThe second term the Bayes Factor"
  },
  {
    "objectID": "eslr-04-model-assessment.html#vc-dimension",
    "href": "eslr-04-model-assessment.html#vc-dimension",
    "title": "10  Model Selection",
    "section": "10.5 VC Dimension",
    "text": "10.5 VC Dimension\n\nAIC, C-p statistic need the information on model complexity\n\nEffective number of parameters\n\nDifficult to estimate for non-linear models\nVC Dimension is Generalized Model Complexity of a class of functions\n\nHow “wiggly” can the memeber of this class be?\n\nShattering\n\nPoints that can be perfectly separated by a class of functions, no matter how the binary labels are assigned\n\nVC Dimension: Largest number of points that can be shattered by members of class of functions\n3 points in case of linear classifier in a plane\n\n4 points can lead to XOR"
  },
  {
    "objectID": "eslr-04-model-assessment.html#cross-validation",
    "href": "eslr-04-model-assessment.html#cross-validation",
    "title": "10  Model Selection",
    "section": "10.6 Cross Validation",
    "text": "10.6 Cross Validation\n\nEstimation for \\(\\text{ERR}_T\\)\nData scarce situation\nDivide data into K equal parts\n\nIndexing function: \\(\\kappa : \\{1,2,....N\\} \\rightarrow \\{1, 2 ... K\\}\\)\n\nFit model on K-1 parts and predict on Kth part\nCross Validaiton Error\n\n\\(CV(f) = {1 \\over N}\\sum L(y_i, \\hat y_i^{f_{-\\kappa}})\\)\n\n5-fold, 10-fold cross validation is recommended"
  },
  {
    "objectID": "eslr-04-model-assessment.html#boostrap-methods",
    "href": "eslr-04-model-assessment.html#boostrap-methods",
    "title": "10  Model Selection",
    "section": "10.7 Boostrap Methods",
    "text": "10.7 Boostrap Methods\n\nEstimation for \\(\\text{ERR}\\)\nRandomly draw datasets from training data by sampling with replacement\n\nEach dataset has the same size of original training data\n\nFit the model on each of the bootstrap datasets\n\\(\\text{ERR}_{\\text{boot}} = {1 \\over B}{1 \\over N} \\sum_B \\sum_N L(y, \\hat y)\\)\nBootstrap uses overlapping samples across model fits (unlike cross validation)\n\n\\(P(i \\in B) = 1 - (1 - {1 \\over N})^N \\approx 1 - e^{-1}\\)\n\n\\(\\text{ERR}_{\\text{boot}}\\) isn’t a good estimator becuase of leakage\nUse Out-of-bag error instead\n\nSamples which have been dropped by boostrap"
  },
  {
    "objectID": "eslr-08-model-selection.html#bootstrap",
    "href": "eslr-08-model-selection.html#bootstrap",
    "title": "11  Model Selection",
    "section": "11.2 Bootstrap",
    "text": "11.2 Bootstrap\n\nBootstrap assesses uncertainty by sampling from training data\n\nEstimate different models using bootstrap datasets\nCalculate the variance of estimates for ith observation from these models\n\nNon-Parametric Booststrap\n\nUses raw data for sampling, model free\n\nParametric Bootstrap\n\nSimulate new target variable by adding gaussian noise to predicted values from model\nPredictions estimated from this sampling will follow Gaussian distribution\n\nComputational alternative to MLE\n\nNo formulae are available\n\nBoostrap mean is equivalent to posterior average in Bayesian inference\nBagging averages predictions over collection of bootstrap samples\n\nReduces variance of estimates\nBagging often descreases mean-squared error"
  },
  {
    "objectID": "eslr-08-model-selection.html#bayesian-methods",
    "href": "eslr-08-model-selection.html#bayesian-methods",
    "title": "11  Model Selection",
    "section": "11.3 Bayesian Methods",
    "text": "11.3 Bayesian Methods\n\nAssume a prior distribution over unknown parameters\n\n\\(P(\\theta)\\)\n\nSampling Distribution of data given the parameters\n\n\\(P(Z | \\theta)\\)\n\nPosterior Distribution\n\nUpdated knowledge of parameters after seeing the data\n\\(P(\\theta | Z) \\propto P(Z | \\theta) \\times P(\\theta)\\)\n\nPredictive Distribution\n\nPredicting values of new unseen observations\n\\(P(z | Z) = \\int P(z | \\theta) P(\\theta | Z) d\\theta\\)\n\nMAP Estimate\n\nMaximum a Posterior, point estimate of unknown parameters\nSelec the parameters that maximze posterior density function\n\\(\\hat \\theta = \\arg \\max P(\\theta | Z)\\)\n\nMAP differs from frequentist approaches (like MLE) in its use of prior distrbution\n\nPrior Distribution acts as regularization\nMAP for linear regression for Gaussian priors yields Ridge Regression"
  },
  {
    "objectID": "eslr-08-model-selection.html#em-algorithm",
    "href": "eslr-08-model-selection.html#em-algorithm",
    "title": "11  Model Selection",
    "section": "11.4 EM Algorithm",
    "text": "11.4 EM Algorithm\n\nSimplify difficult MLE problems\nBimodal Data Distribution\n\n\\(Y_1 = \\sim N(\\mu_1, \\sigma^2_1)\\)\n\\(Y_2 = \\sim N(\\mu_2, \\sigma^2_2)\\)\n\\(Y = \\Delta Y_1 + (1 - \\Delta) Y_2\\)\n\n\\(\\Delta \\in \\{0,1\\}\\)\n\\(p(\\Delta = 1) = \\pi\\)\n\nDensity function of Y\n\n\\(g_Y(y) = (1 - \\pi) \\phi_1(y) + \\pi \\phi_2(y)\\)\n\n\nDirect maximization of likelihood difficult\n\nSum operation inside log\n\nResponsibility\n\n\\(\\Delta_i\\) is latent for a given observation\n\\(\\gamma_i(\\Delta | Z, \\theta) = P(\\Delta = 1 | Z, \\theta)\\)\nSoft Assignments\n\nEM Algorithm\n\nTake Initial Guesses for paramters\n\nSample Mean, Sample Variances, Proportion\n\nExpentation Step: Compute the responsibility\n\n\\(\\hat \\gamma_i = \\hat \\pi \\phi_2(y_i) / (1 - \\hat \\pi \\phi_1(y_i) + \\hat \\pi \\phi_2(y_i)\\)\n\nMaximization Step: Compute the weighted means and variances, and mixing probability\n\n\\(\\mu_1 = \\sum (1 - \\hat \\gamma_i) y_i / \\sum 1 - \\hat \\gamma_i\\)\n\\(\\mu_2 = \\sum \\hat \\gamma_i y_i / \\sum \\hat \\gamma_i\\)\n\\(\\hat \\pi = \\sum \\gamma_i / N\\)"
  },
  {
    "objectID": "eslr-08-model-selection.html#mcmc",
    "href": "eslr-08-model-selection.html#mcmc",
    "title": "11  Model Selection",
    "section": "11.5 MCMC",
    "text": "11.5 MCMC\n\nGiven a set of random variables \\(U_1, U_2, U_3...\\)\n\nSampling from joint distribution is difficult\nSampling from conditional distribution is easy\nFor example bayesian inference\n\nJoint distribution \\(P(Z, \\theta)\\)\nConditional Distribution \\(P(Z | \\theta)\\)\n\n\nGibbs Sampling\n\nTake Some initial values of RVs \\(U^0_k\\)\nDraw from conditional Distribution\n\n\\(P(U^0_1 | U^0_1, U^0_2...., U^0_K)\\)\n\nContinue until the joint distribution doesn’t change\nMarkov Chain whose stationary distribution is the true joint distribution\nMarkov Chain Monte Carlo\n\nGibbs Sampling is related to EM algorithm\n\nGenerate \\(\\Delta_i \\in {0,1}\\) using \\(p(\\Delta_i = 1) = \\gamma_i (\\theta)\\)\nCalculate the means and variances\n\n\\(\\mu_1 = \\sum (1 - \\Delta_i) y_i / \\sum 1 - \\Delta_i\\)\n\\(\\mu_2 = \\sum \\Delta_i y_i / \\sum \\Delta_i\\)\n\nKeep repeating until the join distribution doesn’t change"
  },
  {
    "objectID": "eslr-09-additive-models.html#tree-based-methods",
    "href": "eslr-09-additive-models.html#tree-based-methods",
    "title": "12  Additive Models",
    "section": "12.2 Tree-based Methods",
    "text": "12.2 Tree-based Methods\n\nPartition the feature space into rectangels and fit a simple model in each partition\nRegression Setting\n\\(f(X) = \\sum c_i I\\{X \\in R_i\\}\\)\n\\(c_m = ave(y_i | X_i \\in R_m)\\)\nGreedy Algorithms to find best splits\n\n\\(R_1 = \\{X | X_j \\le s\\}; \\; R_2 = \\{X | X_j > s\\}\\)\n\n\\(\\min_{j,s} \\min \\sum (y_i - c_i)^2 I\\{X \\in R_i\\}\\)\n\nTree size is a hyperparameter\nPruning\n\nOption-1\n\nSplit only if delta is greater than some threshold\nShort Sighted, the node may lead to a better split down the line\n\n\nOption 2\n\nGrow the tree to full saize (say depth 5)\n\\(N_m\\) # of observations in m’th node\n\\(C_m = \\sum y_i / N_m\\)\n\\(Q_m = {1 \\over N_m }\\sum (y_i - C_m)^2\\)\nCost-Complexity Pruning\n\\(C = \\sum_T N_m Q_m(T) + \\alpha |T|\\)\n\\(\\alpha\\) governs the trade-off, large value leads to smaller trees\n\n\nClassification Setting\n\\(p_{mk} = {1 \\over N_m}\\sum_{R_m} I\\{y_i = k\\}\\)\nSplitting Criteria\n\nMiss-classification Error: \\(1 - \\hat p_{mk}\\)\nGini Index: \\(\\sum_K p_{mk}(1 - \\hat p_{mk})\\)\n\nProbability of miscalssification\nVariance of Binomial Distribution\n\nCross-Entropy: \\(- \\sum_K p_{mk} \\log (p_{mk})\\)\nGini Index and Cross Entropy more sensitive to node probabilities\n\nSplitting categorical variable\n\nN levels, \\(2^{N-1} - 1\\) possible paritions\nOrder the categories by proportion\nTreat the variable as continuous\n\nMissing Values\n\nCreate a new level within the original corresponding to missing observations\nCreate a surrogate variable for missing values\n\nSplit by non-missing values\nLeverage the correlation between predictors and surrogates to minimize loss of information\n\n\nEvaluation\n\n\\(L_{xy} =\\) Loss for predicting class x obkect as k\n\\(L_{00}, L_{11} = 0\\)\n\\(L_{10} =\\) False Negative\n\\(L_{01} =\\) False Positive\nSentitivity:\n\nPrediciting disease as disease (Recall)\nTP / TP + FN\n\\(L_{11} / (L_{11} + L_{10})\\)\n\nSpecificity:\n\nPredicting non-disease as non-disease\n\nTN / TN + FP\n\\(L_{00} / (L_{00} + L_{01})\\)\n\n\n\nAUC-ROC\n\nHow Sentitivity (y) and Specificty (x) vary with thresholds\nArea under ROC Curve is the C-statistic\nEquivalent to Mann-Whitney U Test, Wilcoxin rank-sum test\nMedian Difference in prediction scores for two groups\n\n\nMARS\n\nHigh dimension regression\nPiece-wise Linear basis Functions\nAnalogous to deision tree splits\nCan handle interactions"
  },
  {
    "objectID": "eslr-09-additive-models.html#prim",
    "href": "eslr-09-additive-models.html#prim",
    "title": "12  Additive Models",
    "section": "12.3 PRIM",
    "text": "12.3 PRIM\n\nPatient Rule Induction Method\nBoxes with high response rates\nNon-tree partitioning structure\nStart with a large box and\n\nPeeling: compress the side that gives the largest mean\nPasting: expand the bix dimensions that gives the largest mean\n\nCART fragments the data quickly via binary splits\nPRIM is more “patient”"
  },
  {
    "objectID": "eslr-09-additive-models.html#mixture-of-experts",
    "href": "eslr-09-additive-models.html#mixture-of-experts",
    "title": "12  Additive Models",
    "section": "12.4 Mixture of Experts",
    "text": "12.4 Mixture of Experts\n\nTree splits are not hard decisions but soft probabilities\nTerminal nodes are called experts\n\nA linear model is fit in each terminal node\n\nNon-terminal nodes are called gating networks\nThe decision of experts is combined by gating networks\nEstimation via EM Algorithm"
  },
  {
    "objectID": "eslr-10-boosting.html",
    "href": "eslr-10-boosting.html",
    "title": "13  Boosting",
    "section": "",
    "text": "Sequentially apply the weak classifers on modified versions of the data\nData modifications involve re-weighting the observations\n\nErrors from previous round are given more weight\n\n\\[Y \\in \\{-1,+1\\}\\]\n\\[G(x_i) = sign(\\sum \\alpha_m G_m(x_i))\\]\n\nG(.) is weak classifier with an accuracy slightly better than random\n\\[\\alpha\\] is calcualted by the boosting algorithm\n\nAlgorithm\n\nInitial weights \\[w_i = 1/N\\]\nFor m rounds\n\nFit classifier \\[G_i(x)\\] using \\[w_i\\]\nCompute Error \\[\\bar{err} = {1 \\over \\sum w_i}\\sum I\\{y_i \\ne G(x_i)\\}\\]\nCompute \\[alpha_m = \\log((1 - err_m) / err_m)\\]\nCompute new weights \\[w_i = w_i \\exp \\alpha_m I\\{y_i \\ne G(x_i)\\}\\]"
  },
  {
    "objectID": "eslr-10-boosting.html#additive-models",
    "href": "eslr-10-boosting.html#additive-models",
    "title": "13  Boosting",
    "section": "13.2 Additive Models",
    "text": "13.2 Additive Models\n\nBoosting fits a forward stagewise additive model\n\\[f(x) = \\sum \\beta_m b(x, \\gamma_m)\\]\n\\[\\min L(y_i, \\beta_m b(x, \\gamma_m))\\]\n\nOptimal values of beta and gama can be found iteratively\n\\[\\min L(y_i, f_{m-1}(x) + \\beta b(x, \\gamma))\\]\n\nL2 Loss Funciton\n\n\\[\\min L(y_i, f_{m-1}(x) + \\beta b(x, \\gamma))\\]\n\\[\\sum (y_i - f_{m-1}(x) - \\beta b(x, \\gamma))^2\\]\n\\[\\sum (r_{im} - \\beta b(x, \\gamma))^2\\]\nFit on residuals from previous rounds\nRobust Loss Functions for regression\n\nHuber Loss\nL2 penalty for large errors\nL1 penalty for small errors\n\n\nExponential Loss\n\n\\[L(y_i, f(x)) = \\exp (-y f(x))\\]\nEquivalent to using deviance\nThe optimal f that minimizes this loss is 1/2 log-odds\nHence justified to use the sign of f(x) for prediction\n\\[\\min \\sum \\exp (-y f_{m-1}(x) + -y \\beta G(x))\\]\n\\[\\min \\sum w_i^m \\exp (-y \\beta G(x))\\]\n\\[\\min \\exp -\\beta \\sum_{\\text{correct}} w_i^m + \\exp \\beta \\sum_{\\text{incorrect}} w_i^m\\]\n\\[\\beta = {1 \\over 2}\\log({1- err \\over err})\\]"
  },
  {
    "objectID": "eslr-10-boosting.html#gradient-boosting",
    "href": "eslr-10-boosting.html#gradient-boosting",
    "title": "13  Boosting",
    "section": "13.3 Gradient Boosting",
    "text": "13.3 Gradient Boosting\n\nGradient Descent in function space\nMinimize \\[L(f) = \\sum L(y_i, f(x_i))\\]\n\\[\\arg \\min L(\\mathbf f); \\; \\mathbf f = \\{f(x_1), f(x_2)....\\}\\]\nAdditive Models \\[ \\mathbf f = \\sum_m h_m\\]\nSteepest Descent \\[h_m = \\rho_m g_m\\]\n\n\\[g_{im} = \\delta L(y_i, f(x_i)) / \\delta f(x_i)\\]\n\nLine Search for optimal step size\n\n\\[\\rho_m = \\arg \\min L(f_{m-1} - \\rho g_m)\\]\n\nGradients\n\nL2 Loss: Residual\nL1 Loss: Sign of Residual\nClassification / Deviance: Error\nHuber"
  },
  {
    "objectID": "eslr-15-random-forest.html#variable-importance",
    "href": "eslr-15-random-forest.html#variable-importance",
    "title": "14  Random Forests",
    "section": "14.2 Variable Importance",
    "text": "14.2 Variable Importance\n\nAt each split:\n\nCalculate the improvement in the criterion\nAttribute it to the splitting variable\nAccumatlate over all the trees and splits\n\nUsing OOB Sample, Permutation accuracy\n\nPass down OBB samples for a tree, calculate accuracy\nShuffle the variable j\nRe-calculate accuracy\nAverage the difference over all the trees"
  },
  {
    "objectID": "eslr-15-random-forest.html#proximity-plots",
    "href": "eslr-15-random-forest.html#proximity-plots",
    "title": "14  Random Forests",
    "section": "14.3 Proximity Plots",
    "text": "14.3 Proximity Plots\n\nNxN Proximity matrix\nCount of pairs of OOB observations that share the terminal nodes\nMulti-dimensional scaling (optional)"
  },
  {
    "objectID": "probml-00.html",
    "href": "probml-00.html",
    "title": "ProbML Notes",
    "section": "",
    "text": "Resource: Probabilistic Machine Learning\nTopics Covered:\n\nChapter 1: Introduction\nChapter 2: Probability\nChapter 3: Probability\nChapter 4: Statistics\nChapter 5: Decision Theory\nChapter 6: Information Theory\nChapter 8: Optimization\nChapter 9: Discriminant Analysis\nChapter 10: Logistic Regression\nChapter 11: Linear Regression\nChapter 13: FFNN\nChapter 14: CNN\nChapter 15: RNN\nChapter 16: KNN\nChapter 18: Trees\nChapter 19: SSL\nChapter 21: Rec Sys"
  },
  {
    "objectID": "probml-01-introduction.html",
    "href": "probml-01-introduction.html",
    "title": "15  Introduction",
    "section": "",
    "text": "Supervised Learning\n\nTask is to learn a mapping function from inputs to outputs\nInputs are vectors called features, covariates, predictors\nOutput is called as label, target, response\n\n\n\n\nClassification\n\nOutput is a set of unordered, mutually exclusive labels\nGoal is to design a model with minimum misclassification rate\n\\(L(\\theta) = {1 \\over N} \\sum I\\{y \\ne f(x, \\theta)\\}\\)\nEmpirical Risk is the average loss on training set\nModel fitting minimizes empirical risk (ERM)\nOverall goal is generalization\n\n\n\n\n\nUncertainty\n\nModel cannot map inputs to outputs with 100% certainty\nModel Uncertainty due to lack of knowledge between input and output\nData Uncertainty due to stochasticity in labels\nGood model assigns high probability to true output for each input\nIntuition for minimizing NLL (negative log-likelihood)\n\\(NLL(\\theta) = {1 \\over N} \\sum p(y | f(x, \\theta))\\)\nOptimal parameters give the MLE estimate\n\n\n\n\n\nRegression\n\nOutput is a real valued quantity\nModel fitting often involves minimizing the quadratic loss or MSE\n\\(L(\\theta) = {1 \\over N} \\sum (y - f(x, \\theta))^2\\)\nData uncertainty. For example: if the output distribution is Gaussian\n\\(p(y | x, \\theta) = \\mathcal N(y | f(x, \\theta), \\ \\sigma^2)\\)\n\\(NLL(\\theta) \\propto MSE(\\theta)\\)\nLinear Regression is an affine transformation between inputs and outputs\nPolynomial Regression improves the fit by considering higher order interactions\nFNNs do feature extraction by nesting the functions\n\n\n\n\n\nOverfitting and Generalization\n\nA model that perfectly fits training data but is too complex suffers from overfitting\nPopulation risk the theoretical expected loss on the true data generating process\nGeneralization gap is the difference between empirical risk and population risk\nHigh generalization gap is a sign of overfitting\nPopulation risk is hard to estimate. Approximate using test risk. Expected error on unseen data points.\nTest error has U-shaped curve wrt model’s degree of freedom\n\n\n\n\n\nNo Free Lunch Theorem: No single best model that works optimally for all kinds of problems\n\nUnsupervised Learning\n\nLearn an unconditional model of the data \\(p(x)\\) rather than \\(p(y | x)\\)\nIn clustering, the goal is to partition the input space into regions with homogenous points.\nDimensionality reduction projects input data from high dimension to lower dimension subspace.\nSelf-Supervised Learning involves creating proxy supervised tasks from unlabled data\nEvaluation is done by measuring the probability assigned by the model to unseen data\nThis treats the problem as one of density estimation\nUnsupervised learning also aims to increase sample efficiency in downstream supervised learning tasks\n\nReinforcement Learning\n\nA system or an agent learns how to interact with its environment\nGoal is learn a policy that specifies optimal action given a state\nUnlike, Supervised Learning, the reward signal is occasional and delayed.\nLearning with critic vs learning with teacher.\n\nData Preprocessing\n\nText Data\n\nBag of Words transforms a document to a term frequency matrix\nFrequent words have undue influence (pareto distribution)\nLog of counts assuage some of the problems\nInverse Document Frequency: \\(\\text{IDF}_i = \\log {N \\over 1 + \\text{DF}_i}\\)\nN is the total number of documents and DF is the documents with term i\n\\(\\text{TF-IDF} = \\log(1 + TF) \\times IDF\\)\nWord embeddings map sparse vector representation of word to lower dimension dense vector\nUNK token can help capture OOV words\nSub-word units or word pieces created using byte-pair encoding perform better than UNK token and help in reducing the vocabulary.\n\n\n\n\n\nMissing Data\n\nMCAR: missing completely at random\nMAR: missing at random\nNMAR: not missing at random\nHandling of the missing values depends to the type"
  },
  {
    "objectID": "probml-02-probability.html",
    "href": "probml-02-probability.html",
    "title": "16  Probability",
    "section": "",
    "text": "Frequentist View: Long run frequencies of events that can happen multiple times\nBayesian View: Quantify the uncertainty\n\nModel Uncertainty: Ignorance of underlying process\nData Uncertainty: Stochasticity\nData uncertainty can’t be reduced with more data\n\nEvent: Some state of the world (A) that either holds or doesn’t hold.\n\n\\(0 \\le P(A) \\le 1\\)\n\\(P(A) + P(\\bar A) = 1\\)\n\nJoint Probability: If two events happen simultaneously\n\n\\(P(A,B)\\)\nIf A and B are independent: \\(P(A,B) = P(A)P(B)\\)\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\n\nConditional Probability: Event B happens given A has already happened\n\n\\(P(A | B) = P(A \\cap B) | P(A)\\)\n\nA random variable represents unknown quantity of interest whose value cannot be determined.\nSample space denotes the set of possible values of a random variable.\nEvent is a set of outcomes from a given sample space.\n\nIf the sample is finite or countably finite, it’s discrete random variable\nIf the sample space is real valued, it’s continuous random variable\n\nProbability Mass Function computes the probability of events of a given random variable\n\n\\(0 \\le p(x) \\le 1\\)\n\\(\\sum_x p(x) = 1\\)\n\nCumulative Distribution Function are monotonically non-decreasing functions.\n\n\\(\\text{CDF}(x) = P(X \\le x)\\)\n\\(P(A \\le X \\le B) = \\text{CDF}(B) - \\text{CDF}(A)\\)\n\nProbability Density Function is the derivative of CDF\nInverse CDF or Quantile Function\n\n\\(P^{-1}(0.5)\\) is the median\n\\(P^{-1}(0.25); P^{-1}(0.75)\\) are lower and upper quartiles\n\nMarginal Distribution of an random variable\n\n\\(p(X=x) = \\sum_y p(X=x, Y=y)\\)\n\nConditional Distribution of a Random Variable\n\n\\(p(Y=y | X=x) = {p(Y=y, X=x) \\over p(X=x)}\\)\n\nProduct Rule\n\n\\(p(x,y) = p(y|x)p(x) = p(x|y) p(y)\\)\n\nChain Rule\n\n\\(p(x1,x2,x3) = p(x1) p(x2 | x1) p(x3 | x1, x2)\\)\n\nX and Y are independent\n\n\\(X \\perp Y \\Rightarrow p(X,Y) = p(X) p(Y)\\)\n\nX and Y are conditionally independent of Z\n\n\\(X \\perp Y | Z \\Rightarrow p(X,Y | Z) = p(X|Z) p(Y | Z)\\)\n\n\nMoments of a Distribution\n\nMean or Expected Value\n\nFirst moment around origin\n\\(\\mathbf E(X) = \\sum xp(x) \\; \\text{OR} \\; \\int_x xp(x) dx\\)\nLinearity of Expectation: \\(\\mathbf E(aX + b) = a \\mathbf E(X) + b\\)\n\n\n\n\n\nVariance of a distribution\n\nSecond moment around mean\n\\(\\mathbf E(X-\\mu)^2 = \\sigma^2\\)\n\\(\\text{Var}(aX + b) = a^2 Var(X)\\)\n\n\n\n\n\nMode of a distribution\n\nValue with highest probability mass or probability density\n\n\n\n\n\nLaw of Total / Iterated Expectation\n\n\\(E(X) = E(E(X|Y))\\)\n\n\n\n\n\nLaw of Total Variance\n\n\\(V(X) = E(V(X | Y)) + V(E(X | Y))\\)\n\n\nBayes’ Rule\n\nCompute probability distribution over some unknown quantity H given observed data Y\n\\(P(H | Y) = {P(Y |H) P(H) \\over P(Y)}\\)\nFollows from product rule\np(H) is the prior distribution\np(Y | H) is the observation distribution\np(Y=y | H=h) is the likelihood\nBayesian Inference: \\(\\text{posterior} \\propto \\text{prior} \\times \\text{likelihood}\\)\n\nDistributions\n\nBernoulli and Binomial Distribution\n\nDescribes a binary outcome\n\\(Y \\sim Ber(\\theta)\\)\n\\(Y = \\theta^y (1 - \\theta)^{1-y}\\)\nBinomial distribution is N repeatitions of Bernoulli trials\n\\(Bin(p | N,\\theta) = {N \\choose p} \\theta^p (1 - \\theta)^{1-p}\\)\n\n\n\n\n\nLogistic Distribution\n\nIf we model a binary outcome using ML model, the range of f(X) is [0,1]\nTo avoid this constraint, use logistic function: \\(\\sigma(a) = {1 \\over 1 + e^{-a}}\\)\nThe quantity a is log-odds: log(p | 1-p)\nLogistic function maps log-odds to probability\n\\(p(y=1|x, \\theta) = \\sigma(f(x, \\theta))\\)\n\\(p(y=0|x, \\theta) = \\sigma( - f(x, \\theta))\\)\nBinary Logistic Regression: \\(p(y|x, \\theta) = \\sigma(wX +b)\\)\nDecision boundary: \\(p(y|x, \\theta) = 0.5\\)\nAs we move away from decision boundary, model becomes more confident about the label\n\n\n\n\n\nCategorical Distribution\n\nGeneralizes Bernoulli to more than two classes\n\\(\\text{Cat}(y | \\theta) = \\prod \\theta_c ^ {I(y=C)} \\Rightarrow p(y = c | \\theta) = \\theta_c\\)\nCategorical distribution is a special case of multinomial distribution. It drops the multinomial coefficient.\nThe categorical distribution needs to satisfy\n\n\\(0 \\le f(X, \\theta) \\le 1\\)\n\\(\\sum f(X, \\theta) = 1\\)\n\nTo avoid these constraints, its common to pass the raw logit values to a softmax function\n\n\\({e^x_1 \\over \\sum e^x_i} , {e^x_2 \\over \\sum e^x_i}....\\)\n\nSoftmax function is “soft-argmax”\n\nDivide the raw logits by a constant T (temperature)\nIf T → 0 all the mass is concentrated at the most probable state, winner takes all\n\nIf we use categorical distribution for binary case, the model is over-parameterized.\n\n\\(p(y = 0 | x) = {e^{a_0} \\over e^{a_0} + e^{a_1}} = \\sigma(a_0 - a_1)\\)\n\n\n\n\n\n\nLog-Sum-Exp Trick\n\nIf the raw logit values grow large, the denominator of softmax can enounter numerical overflow.\nTo avoid this:\n\n\\(\\log \\sum \\exp(a_c) = m + \\log \\sum \\exp(a_c - m)\\)\nif m is arg max over a, then we wont encounter overflow.\n\nLSE trick is used in stable cross-entropy calculation by transforming the sigmoid function to LSE(0,-a).\n\n\n\n\n\nGaussian Distribution\n\nCDF of Gaussian is defined as\n\n\\(\\Phi(y; \\mu, \\sigma^2) = {1 \\over 2} [ 1 + \\text{erf}({z \\over \\sqrt(2)})]\\)\nerf is the error function\n\nThe inverse of the CDF is called the probit function.\nThe derivative of the CFD gives the pdf of normal distribution\nMean, Median and Mode of gaussian is \\(\\mu\\)\nVariance of Gaussian is \\(\\sigma\\)\nLinear Regression uses conditional gaussian distribution\n\n\\(p(y | x, \\theta) = \\mathcal N(y | f_\\mu(x, \\theta); f_\\sigma(x, \\theta))\\)\nif variance does not depend on x, the model is homoscedastic.\n\nGaussian Distribution is widely used because:\n\nparameters are easy to interpret\nmakes least number of assumption, has maximum entropy\ncentral limit theorem: sum of independent random variables are approximately gaussian\n\nDirac Delta function puts all the mass at the mean. As variance approaches 0, gaussian turns into dirac delta.\nGaussian distribution is sensitive to outliers. A robust alternative is t-distribution.\n\nPDF decays as polynomial function of distance from mean.\nIt has heavy tails i.e. more mass\nMean and mode is same as gaussian.\nVariance is \\(\\nu \\sigma^2 \\over \\nu -2\\)\nAs degrees of freedom increase, the distribution approaches gaussian.\n\n\n\n\n\n\nExponential distribution describes times between events in Poisson process.\nChi-Squared Distribution is sum-squares of Gaussian Random Variables.\n\nTransformations\n\nAssume we have a deterministic mapping y = f(x)\nIn discrete case, we can derive the PMF of y by summing over all x\nIn continuous case:\n\n\\(P_y(y) = P(Y \\le y) = P(f(X) \\le y) = P(X \\le f^{-1}(y)) = P_x(f^{-1}(y))\\)\nTaking derivatives of the equation above gives the result.\n\\(p_y(y) = p_x(x)|{dy \\over dx}|\\)\nIn multivariate case, the derivative is replaced by Jacobian.\n\n\n\n\n\nConvolution Theorem\n\ny = x1 + x2\n\\(P(y \\le y^*) = \\int_{-\\infty}^{\\infty}p_{x_1}(x_1) dx_1 \\int_{-\\infty}^{y^* - x1}p_{x_2}(x_2)dx_2\\)\nDifferentiating under integral sign gives the convolution operator\n\\(p(y) = \\int p_1(x_1) p_2(y - x_1) dx_1\\)\nIn case x1 and x2 are gaussian, the resulting pdf from convolution operator is also gaussian. → sum of gaussians results in gaussian (reproducibility)\n\n\n\n\n\nCentral Limit Theorem\n\nSuppose there are N random variables that are independently identically distributed.\nAs N increases, the distribution of this sum approaches Gaussian with:\n\nMean as Sample Mean\nVariance as Sample Variance\n\n\n\n\n\n\nMonte-Carlo Approximation\n\nIt’s often difficult ti compute the pdf of transformation y = f(x).\nAlternative:\n\nDraw a large number of samples from x\nUse the samples to approximate y"
  },
  {
    "objectID": "probml-03-probability.html",
    "href": "probml-03-probability.html",
    "title": "17  Probability",
    "section": "",
    "text": "Simpson’s Paradox\n\nStatistical Trend that appears in groups of data can disappear or reverse when the groups are combined.\n\n\n\n\n\nMixture Models\n\nConvex combination of simple distributions\n\\(p(y|\\theta) = \\sum \\pi_k p_k(y)\\)\nFirst sample a component and then sample points from the component\nGMM: \\(p(y) = \\sum_K \\pi_k \\mathcal N(y | \\mu_k, \\sigma_k)\\)\nGMMs can be used for unsupervised soft clustering.\nK Means clustering is a special case of GMMs\n\nUniform priors over components\nSpherical Gaussians with identity matrix variance\n\n\n\n\n\n\nMarkov Chains\n\nChain Rule of probability\n\\(p(x1,x2,x3) = p(x1) p(x2 | x1) p(x3 | x1, x2)\\)\nFirst-order Markov Chain: Future only depends on the current state.\ny(t+1:T) is independent of y(1:t)\n\\(p(x1,x2,x3) = p(x1) p(x2 | x1) p(x3 | x2)\\)\nThe p(y | y-1) function gives the state transition matrix\nRelaxing these conditions gives bigram and trigram models."
  },
  {
    "objectID": "probml-04-statistics.html",
    "href": "probml-04-statistics.html",
    "title": "18  Statistics",
    "section": "",
    "text": "Maximum Likelihood Estimation\n\nPick parameters that assign highest probability to training data\n\n\\(\\theta_{MLE} = \\arg \\max p(D | \\theta) = \\prod p(y | x, \\theta)\\)\n\nMLE can be factorized because of IID assumption\nMaximizing MLE is equivalent to minimizing NLL\n\n\\(\\text{NLL}(\\theta) = -\\log p(D | \\theta)\\)\n\nFor unsupervised learning MLE is unconditional.\n\n\\(\\theta_{MLE} = \\arg\\max p( x | \\theta)\\)\n\nJustification for MLE\n\nBayesian MAP estimate with uninformative uniform prior\n\n\\(\\theta_{MAP} = \\arg\\max p(\\theta | D) = \\arg \\max p(D | \\theta) + p(\\theta)\\)\n\nKL Divergence: MLE brings predicted distribution close to empirical ditribution\n\n\\(KL(p||q) = H(p) - H(p,q)\\)\nCross-entropy term in KL-Divergence corresponds to KL-Divergence\n\n\nSufficient Statistics of the data summarize all the information needed.\n\nN0 (negative # samples) and N1 (positive # samples) in case of Bernoulli Distribution\n\nMLE Examples\n\nBernoulli Distribution\n\n\\(NLL(\\theta) = N_1 \\log(\\theta) - N_0 \\log(1-\\theta)\\)\n\\(\\Delta NLL \\Rightarrow \\theta = N_1 / (N_0 + N_1)\\)\n\nCategorical DIstribution\n\nAdd unity contraint as Lagrangian\n\\(NLL(\\theta) = \\sum N_k \\log(\\theta) + \\lambda (\\sum \\theta_k -1))\\)\n\nGaussian Distribution\n\n\\(NLL(\\theta) = {1 \\over 2\\sigma^2 }\\sum \\log(y - \\mu)^2 + {N \\over 2} log (2\\pi \\sigma^2)\\)\nSample mean and sample variance become sufficient statistics\n\nLinear Regression\n\n\\(p(y | x; \\theta) = \\mathcal N (y | wx +b, \\sigma^2)\\)\n\\(NLL \\propto \\sum (y - wx - b) ^ 2\\)\nQuadratic Loss is a good choice for linear regression\n\n\n\nEmpirical Risk Minimization\n\nEmpirical Risk Minimization is the expected loss where the expectation is taken wrt to empirical distribution\nERM generalizes MLE by replacing log-loss with any loss function\n\n\\(L(\\theta) = {1 \\over N} \\sum l(y, x, \\theta)\\)\nLoss could be miss-classification rate as an example\n\nSurrogate losses devised to make optimization easier.\n\nLog-Loss, Hinge-Loss etc.\n\nMethod of Moments (MoM) compares theoretical moments of a distribution with to the empirical ones.\n\nMoments are quantitative measures related to the shape of the function’s graph\n\nIn batch learning, entire dataset is available before training.\nIn online learning, dataset arrives sequentially.\n\n\\(\\theta_t = f(x_t, \\theta_{t-1})\\)\nRecursive updates are required. For example MA, or EWMA\n\n\\(\\mu_t = \\mu_{t-1} + {1 \\over t}(x_t - \\mu_{t-1})\\)\n\\(\\mu_t = \\beta \\mu_{t-1} + (1 - \\beta) y_t\\)\n\n\n\nRegularization\n\nMLE/ERM picks parameters that minimize loss on training set.\nEmpirical distribution may not be same as true distribution.\nModel may not generalize well. Loss on unseen data points could be high. Overfitting.\nRegularization helps reduce overfitting by adding a penalty on complexity.\n\nIn-built in MAP estimation\n\\(L(\\theta) = NLL + \\lambda \\log p(\\theta)\\)\nAdd-one smoothing in Bernoulli to solve zero count problem is regularization.\nThe extra one term comes from Beta priors.\n\nIn linear regression, assume parameters from standard gaussian.\n\n\\(L(\\theta) = NLL + \\lambda \\log w^2\\)\nL2 Penalty in MAP estimation\n\nRegularization strength is picked by looking at validation dataset\n\nValidation risk is estimate for population risk.\nCross-Validation in case of small size of training dataset\n\nOne Standard Error Rule\n\nSelect the model with loss within one SE of the baseline / simple model\n\nEarly Stopping prevents too many steps away from priors. Model doesn’t memorize too much.\nUsing more suitable informative data samples also prevents overfitting.\n\nBayes’ Error is inherent error due to stochasticity.\nWith more data, learning curve approaches Bayes’ Error.\nIf we start with very few observations, adding more data may increase the error as model uncovers new data patterns.\n\n\nBayesian Statistics\n\nStart with prior distribution\nLikelihood reflects the data for each setting of the prior\nMarginal Likelihood shows the average probability of the data by marginalizing over model parameters\nPosterior Predictive Distribution: is Bayes Model Averaging\n\n\\(p(y | x, D) = \\int p(y | x, \\theta) p(\\theta | D) d\\theta\\)\nMultiple parameter values considered, prevents overfitting\nPlug-in Approximation: Uses dirac delta to pul all the weight on MLE\nThis simplifies the calculations\n\nConjugate Priors\n\nposterior = prior x likelihood\nSelect prior in a form that posterior is closed form and has same family as prior\nBernoulli-Beta\nGaussian-Gaussian\n\n\nFrequentist Statistics\n\nData is a random sample drawn from some underlying distribution\nInduces a distribution over the test statistic calculated from the sample.\nEstimate variation across repeated trials.\nUncertainty is calculated by quantifying how the estimate would change if the data was sampled again.\nSampling Distribution\n\nDistribution of results if the estimator is applied multiple times to different datasets sampled from same distribution\n\nBootstrap\n\nIf the underlying distribution is complex, approximate it by a Monte-Carlo technique\nSample N data points from original dataset of size N with replacement\nBootstrap Sample is 0.633 x N on average\n\nProbability the point is selected atleast once\n\\(1 - (1 - {1 \\over N})^N \\approx 1 - {1 \\over e}\\)\n\n\n100 (1 - a) % CI is the probability that the true value of the parameter lies in the range.\n\nBias-Variance Tradeoff\n\nBias of an estimator\n\n\\(bias(\\hat \\theta) = E[\\hat \\theta] - \\theta^*\\)\n\nMeasures how much the estimate will differ from true value\nSample variance is not an unbiased estimator for variance\n\n\\(\\mathbf V[\\hat \\theta] = E[\\hat \\theta ^ 2] - E[\\hat \\theta]^2\\)\n\nMeasures how much will the estimate vary is data is resampled\n\nMean Squared Error\n\n\\(E[(\\hat \\theta - \\theta^*)^2] = \\text{bias}^2 + \\text{variance}\\)\nIt’s okay to use a biased estimator if the bias is offset by decrease in variance."
  },
  {
    "objectID": "probml-05-decision_theory.html",
    "href": "probml-05-decision_theory.html",
    "title": "19  Decision Theory",
    "section": "",
    "text": "ROC Curves\n\nSummarize performance across various thresholds\nConfusion Matrix\n\nGive a threshold \\(\\tau\\)\nConfusion Matrix\n\nPositive, Negative: Model Prediction\nTrue, False: Actual Labels\nTP, TN: Correct Predictions\nFP: Model predicts 1, Ground Truth is 0\nFN: Model predicts 0, Ground Truth is 1\n\nRatios from Confusion Matrix\n\nTPR, Sensitivity, Recall\n\nTP / (TP + FN)\nAccuracy in positive predictions\n\nFPR, Type 1 Error rate\n\nFP / (FP + TN)\nError in Negative Predictions\n\n\nROC Curve is a plot between FPR (x-axis) and TPR (y-axis) across various thresholds\nAUC is a numerical summary of ROC\nEqual Error Rate is where ROC crosses -45 degree line.\nROC Curve is insensitive to class imbalance\n\nFPR consists of TN in denominator\nIf TN >> TP, metric becomes insensitive to FPR\n\nPrecision-Recall Curves\n\nThe negatives are not model specific but system specific\nFor a search query, retrieve 50 vs 500 items. (or tiles vs list)\nPrecision\n\nTP / TP + FP\n\nRecall\n\nTP / TP + FN\n\nThere is no dependency on TN\nPrecision curve has distortions. Smooth it out by interpolation.\nTo summarize the performance by a scalar\n\nPrecision @ K\nAverage Precision: Area under interpolated precision curve\nmAP or Mean Average Precision is mean of AP across different PR curves (say different queries)\n\nF-Score\n\nWeighted harmonic mean between precision and recall\n\\({1 \\over F} = {1 \\over 1 + \\beta^2} {1 \\over P} + {\\beta^2 \\over 1 + \\beta^2} {1 \\over R}\\)\nHarmonic mean imposes more penalty if either precision or recall fall to a very low level\n\n\nClass Imbalance\n\nROC curves are not sensitive to class imbalance. Does not matter which class is defined as 1 or 0.\nPR curves are sensitive to class imbalance. Switching classes impacts performance.\n\n\\(P = {TP \\over TP + FP}\\)\n\\(P = {TPR \\over TPR + r^{-1} FPR}\\)\nr = positive / negative\n\nF-Score is also affected by class imbalance.\n\n\n\nRegression Metrics\n\nL2 Loss\n\n\\(l(h,a) = (h-a)^2\\)\nRisk Estimate\n\\(R(a|x) = E[(h-a)^2| x] = E[h^2|x] -2aE[h|x] + a^2\\)\nTo minimize risk, set the derivative of risk to zero.\n\\(\\pi(x) \\Rightarrow E[h|X] = a\\)\nOptimal action is to set the prediction to posterior conditional mean.\n\nL1 Loss\n\nL2 Loss is sensitive to outliers.\nL1 is more robust to alternatives\n\\(l(h,a) = |h-a|\\)\n\nHuber Loss\n\nMiddle ground between L1 and L2 loss\nSet a threshold \\(\\delta\\)\n\nIf error exceeds thresholds → L1 loss\nIf error below threshold → L2 loss\n\n\n\nProbabilistic Metrics\n\nEstimate probabilistic distribution over labels\nKL Divergence\n\n\\(KL(p||q) = \\sum p log(p|q)\\)\n\\(KL(p||q) = H(p,q) - H(p)\\)\nAlways >= 0. Equality holds when p == q\nH(p) is the entropy.\nH(p,q) is the cross entropy.\nCross entropy measures the bits required to encode data coming from p encoded via q.\nKL divergence measures the extra bits needed to compress information using wrong distribution q instead of p.\nH(p) is independent of q. Hence, minimizing KL divergence is equivalent to minimizing the cross-entropy.\nExtending cross-entropy to multiple labels leads to log-loss.\nKL divergence is sensitive to errors at low probability events.\n\n\nA/B Testing\n\nTest and Roll approach to business decisions\nRandomly assign different actions to different populations\nIncurs opportunity cost. Exploration-Exploitation tradeoff.\nBayesian Approach\nBandits\nMarginal Log-Likelihood\n\n\nInformation Criteria\n\nMarginal Likelihood difficult to compute.\nICs incorporate model complexity penalty without the use of validation set.\nICs are of the form deviance + some form of complexity.\n\n\\(\\text{deviance} = -2 \\sum \\log p + C\\)\n\nBayesian Information Criterion\n\n\\(C = \\log(N) \\times \\text{dof}\\)\ndof is degrees of freedom or number of free parameters\nlog of marginal likelihood of the gaussian approximation to the posterior\n\nAkaike Information Criterion\n\nPenalizes model less heavily compared to BIC\n\\(C = 2 \\times \\text{dof}\\)\n\n\nFrequentist Decision Theory\n\nRisk of an estimator is the expected loss when applying the estimator to data sampled from likelihood function \\(p( y,x | \\theta)\\)\nBayes Risk\n\nTrue generating function unknown\nAssume a prior and then average it out\n\nMaximum Risk\n\nMinimize the maximum risk\n\nConsistent Estimator\n\nRecovers true parameter in the limit of infinite data\n\nEmpirical Risk Minimization\n\nPopulation Risk\n\nExpectation of the loss function w.r.t. true distribution\nTrue distribution is unknown\n\\(R(f, \\theta^*) = \\mathbf{E}[l(\\theta^*, \\pi(D))]\\)\n\nEmpirical Risk\n\nApproximate the expectation of loss by using training data samples\n\\(R(f, D) = \\mathbf{E}[l(y, \\pi(x))]\\)\n\nEmpirical Risk Minimizaiton\n\nOptimize empirical risk over hypothesis space of functions\n\\(f_{ERM} = \\arg \\min_H R(f,D)\\)\n\nApproximation Error\n\nRisk that the chosen true parameters don’t lie in the hypothesis space\n\nEstimation Error\n\nError due to having finite training set\nDifference between training error and test error\nGeneralization Gap\n\nRegularized Risk\n\nAdd complexity penalty\n\\(R_\\lambda(f,D) = R(f,D) + \\lambda C(f)\\)\nComplexity term resembles the prior term in MAP estimation\n\nStructural Risk\n\nEmpirical underestimates population risk\nStructural risk minimization is to pick the right level of model complexity by minimizing regularized risk and cross-validation\n\n\n\nStatistical Learning Theory\n\nUpper bound on generalization error with certain probability\nPAC (probably approximately correct) learnable\nHoeffding’s Inequality\n\nUpper bound on generalization error\n\nVC Dimension\n\nMeasures the degrees of freedom of a hypothesis class\n\n\nFrequentist Hypothesis Testing\n\nNull vs Alternate Hypothesis\nLikelihood Ratio Test\n\n\\(p(D| H_0) / p(D| H_1)\\)\n\nNull Hypothesis Significance Testing\n\nType-1 Error\n\nP(Reject H0 | H0 is True)\nSignificance of the test\n\\(\\alpha\\)\n\nType-2 Error\n\nP(Accept H0 | H1 is True)\n\\(\\beta\\)\nPower of the test is \\(1 - \\beta\\)\n\nMost powerful test is the one with highest power given a level of significance\nNeyman-Pearson lemma: Likelihood ratio test is the most powerful test\np-value\n\nProbability, under the null hypothesis, of observing a test statistic larger that that actually observed"
  },
  {
    "objectID": "probml-06-information_theory.html",
    "href": "probml-06-information_theory.html",
    "title": "20  Information Theory",
    "section": "",
    "text": "Cross Entropy\n\n\\(H(p,q) = -\\sum p \\log q\\)\n\n\n\n\n\nJoint Entropy\n\n\\(H(X,Y) = -\\sum p(x,y) \\log p(x,y)\\)\n\n\n\n\n\nConditional Entrpoy\n\n\\(H(Y | X) = \\sum_x p(X) H(p(Y | X=x))\\)\n\\(H(Y|X) = H(X,Y) - H(X)\\)\nReduction in joint uncertainty (X,Y) given we observed X\n\n\n\n\n\nPerplexity\n\nExponentiated cross-entropy\nGeometric mean of inverse probabilities\n\\(\\text{perplexity}(p) = 2^{H(p)} = \\sqrt{ \\prod {1 \\over p}}\\)\nUsed to evaluate the quality of generative language models\nWeighted average of branching factor\n\nNumber of possible words that can follow a given word\nGiven vocab size is K\nIf some words are more frequent, perplexity is lower than K\n\n\n\n\n\n\nKL Divergence\n\nRelative Entropy\nDistance between two distribution\n\\(KL(p||q) = H(p,q) - H(p)\\)\nExtra bits needed when compressing data generated from p using q\nSuppose objective is to minimize KL divergence\nEmpirical distribution puts probability mass on training data and zero mass every where else\n\n\\(p = {1 \\over N} \\sum {\\delta (x - x_n)}\\)\n\nThis reduces KL divergence to cross entropy or negative log likelihood.\n\n\\(KL(p||q) = {1 \\over N} \\sum \\log q(x_n)\\)\n\nData augmentation perturbs data samples to reflect natural variations. This spreads the probability mass over larger space. Prevents overfitting.\n\n\n\n\n\nMutual Information\n\n\\(I(X,Y) = KL(p(x,y) || p(x)p(y))\\)\nKL Divergence between joint and factored marginal distribution\n\\(I(X,Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)\\)\nReduction in uncertainty about X after observing Y\nGeneralized correlation coefficient that can capture non-linear trends.\nCan be normalized to reduce scale effect\nData Processing Inequality: Transformation cannot increase the amount of information\n\n\n\n\n\nFano’s Inequality\n\nFeature selection via high mutual information\nBounds probability of misclassification in terms of mutual information between features"
  },
  {
    "objectID": "probml-08-optimization.html",
    "href": "probml-08-optimization.html",
    "title": "21  Optimization",
    "section": "",
    "text": "First-Order Optimization Methods\n\nLeverage first-order derivatives of the objective function\nIgnore the curvature information\n\\(\\theta_t = \\theta_{t-1} + \\eta_t d_t\\)\nd is the descent direction, \\(\\eta\\) is the step size\nSteepest Descent: direction opposite to the gradient g\nStep Size: controls the amount to move in the descent direction\n\nConstant Step Size\n\nincorrect values can lead to oscillations, slow convergence\n\nLine Search\n\nset as a 1d minimization problem to select the optimal value\n\nLearning rate schedule must respect Robbins-Monro condition\n\n\\({\\sum \\eta^2 \\over \\sum \\eta} \\rightarrow 0 \\, \\text{as} \\, \\eta \\rightarrow 0\\)\n\n\nMomentum\n\nGradient Descent slow across lat regions of the loss landscape\nHeavy Ball or Momentum helps move faster along the directions that were previously good.\n\\(m_t = \\beta m_{t-1} + g_{t-1}\\)\n\\(\\theta_t = \\theta_{t-1} + \\eta_t m_t\\)\nMomentum is essentially EWMA of gradients\n\nNestrov Momentum\n\nMomentum may not slow down enough at the bottom causing oscillation\nNestrov solves for that by adding a lookahead term\n\\(m_{t+1} = \\beta m_t - \\eta_t \\Delta L(\\theta_t + \\beta m_t)\\)\nIt updates the momentum using gradient at the predicted new location\n\n\n\n\n\n\nSecond-Order Optimization Methods\n\nGradients are cheap to compute and store but lack curvature information\nSecond-order methods use Hessian to achieve faster convergence\nNewton’s Method:\n\nSecond-order Taylor series expansion of objective\n\\(L(\\theta) = L(\\theta_t) + g(\\theta - \\theta_t) + {1 \\over 2} H (\\theta - \\theta_t)^2\\)\nDescent Direction: \\(\\theta = \\theta_t - H^{-1} g\\)\n\nBFGS:\n\nQuasi-Newton method\nHessian expensive to compute\nApproximate Hessian by using the gradient vectors\nMemory issues\nL-BFGS is limited memory BFGS\nUses only recent gradients for calculating Hessian\n\n\n\n\n\n\nStochastic Gradient Descent\n\nGoal is to minimize average value of a function with random inputs\n\\(L(\\theta) = \\mathbf E_z[L(\\theta, z)]\\)\nRandom variable Z is independent of parameters theta\nThe gradient descent estimate is therefore unbiased\nEmpirical Risk Minimization (ERM) involves minimizing a finite sum problem\n\n\\(L(\\theta) = {1 \\over N}\\sum l(y, f(x(\\theta))\\)\n\nGradient calculation requires summing over N\nIt can be approximated by summing over minibatch B << N in case of random sampling\nThis will give unbiased approximation and results in faster convergence\n\nVariance Reduction\n\nReduce the variance in gradient estimates by SGD\nStochastic Variance Reduced Gradient (SVRG)\n\nAdjust the stochastic estimates by those calculated on full batch\n\nStochastic Averaged Gradient Accelerated (SAGA)\n\nAggregate the gradients to calculate average values\n\\(g_t = \\Delta L(\\theta) - g_{local} + g_{avg}\\)\n\n\n\n\n\n\nOptimizers\n\nAdaGrad (Adaptive Gradient)\n\nSparse gradients corresponding to features that are rarely present\n\\(\\theta_{t+1} = \\theta_t -\\eta_t {1 \\over \\sqrt{s_t +\\epsilon}} g_t\\)\n\\(s_t = \\sum g^2\\)\nIt results in adaptive learning rate\nAs the denominator grows, the effective learning rate drops\n\nRMSProp\n\nUses EWMA instead of sum in AdaGrad\n\\(s_t = \\beta s_{t-1} + (1-\\beta)g^2_t\\)\nPrevents from s to grow infinitely large\n\nAdaDelta\n\nLike RMSProp, uses EWMA on previous gradients\nBut also uses EWMA on updates\n\\(\\delta_t = \\beta \\delta_{t-1} + (1 - \\beta) (\\Delta \\theta^2)\\)\n\\(\\theta_{t+1} = \\theta_t -\\eta_t {\\sqrt{\\delta_t +\\epsilon} \\over \\sqrt{s_t +\\epsilon}} g_t\\)\n\nAdam\n\nAdaptive Moment Estimation\nCombines RMSProp with momentum\n\\(m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\\)\n\\(s_t = \\beta_1 s_{t-1} + (1 - \\beta_1) g_t^2\\)\n\\(\\Delta \\theta = \\eta {1 \\over \\sqrt s_t + e} m_t\\)\n\n\n\n\n\n\nConstrained Optimization\n\nLagrange Multipliers\n\nConvert constrained optimization problem (with equality constraints) to an unconstrained optimization problem\nAssume the constraint is \\(h(\\theta) = 0\\)\n\\(\\nabla h(\\theta)\\) is orthogonal to the plane \\(h(\\theta) = 0\\)\n\nFirst order Taylor expansion\n\nAlso, \\(\\nabla L(\\theta)\\) is orthogonal to the plane \\(h(\\theta) = 0\\) at the optimum\n\nOtherwise, moving along the constraint can improve the objective value\n\nHence, at the optimal solution: \\(\\nabla L(\\theta) = \\lambda \\nabla h(\\theta)\\)\n\n\\(\\lambda\\) is the Lagrangian multiplier\n\nConvert the above identity to an objective\n\n\\(L(\\theta, \\lambda) = L(\\theta) - \\lambda h(\\theta)\\)\n\n\n\n\n\n\n\nKKT Conditions\n\nGeneralize the concept of Lagrange multiplier to inequality constraints\nAssume the inequality constraint: \\(g(\\theta) < 0\\)\n\\(L(\\theta, \\mu) = L(\\theta) + \\mu g(\\theta)\\)\n\\(\\min L(\\theta) \\rightarrow \\min_{\\theta} \\max_{\\mu \\ge 0} L(\\theta, \\mu)\\)\n\nCompeting objectives\n\\(\\mu\\) is the penalty for violating the constraint.\nIf \\(g(\\theta) > 0\\), then the objective becomes \\(\\infty\\)\n\nComplementary Slackness\n\nIf the constraint is active, \\(g(\\theta) = 0, \\mu > 0\\)\nIf the constraint is inactive, \\(g(\\theta) < 0, \\mu = 0\\)\n\\(\\mu * g = 0\\)\n\n\n\n\n\n\nLinear Programming\n\nFeasible set is a convex polytope\nSimplex algorithm moves from vertex to vertex of the polytope seeking the edge that improves the objective the most.\n\n\n\n\n\nProximal Gradient Descent\n\nComposite objective with smooth and rough parts\nProximal Gradient Descent calculates the gradients of the smooth part and projects the update into a space the respects the rough part\nL1 Regularization is sparsity inducing. Can be optimized using proximal gradient descent. (0,1) is preferred vs \\(1 \\over \\sqrt 2\\), \\(1 \\over \\sqrt 2\\). L2 is agnostic between the two.\n\n\n\n\n\nExpectation Maximization Algorithm\n\nCompute MLE / MAP in cases where there is missing data or hidden variables.\nE Step: Estimates hidden variables / missing values\nM Step: Uses observed data to calculate MLE / MAP\n\\(LL(\\theta) = \\sum \\log p( y | \\theta) = \\sum \\log \\sum p(y, z | \\theta)\\)\nz is the hidden / latent variable\nUsing Jensen’s inequality for convex functions\n\n\\(LL(\\theta) \\ge \\sum \\sum q(z) \\log p (y | \\theta, z)\\)\nq(z) is the prior estimate over hidden variable\nlog(p) is the conditional likelihood\nEvidence lower bound or ELBO method\n\nEMM for GMM\n\nE Step: Compute the responsibility of cluster k for generating the data point\nM Step: Maximize the computed log-likelihood\n\n\n\n\n\n\nSimulated Annealing\n\nStochastic Local Search algorithm that optimizes black-box functions whose gradients are intractable."
  },
  {
    "objectID": "probml-09-discriminant_analysis.html",
    "href": "probml-09-discriminant_analysis.html",
    "title": "22  Discriminant Analysis",
    "section": "",
    "text": "Gaussian Discriminant Analysis\n\nClass Conditional Densities are multivariate Gaussians\n\\(p(x | y=c, \\theta) = N(\\mu_c, \\Sigma_c)\\)\n\\(\\log p(y = c | x, \\theta )\\) will be quadratic in \\(\\mu_c\\), \\(\\Sigma_c\\) (QDA)\nIf the covariance matrices are shared across class labels, the decision boundary will become linear in \\(\\mu_c\\) (LDA)\nLDA can be refactored to be similar to logistic regression\nModels are fitted via MLE.\n\n\\(\\Sigma_c\\) estimates often lead to overfitting\nTied covariances i.e. LDA solve this problem\nMAP estimation can introduce some regularization\n\nClass assignment is based on nearest centroid based on the estimates of \\(\\mu_c\\)\n\n\n\n\n\nNaive Bayes Classifiers\n\nWork on Naive Bayes Assumption\nFeatures are conditionally independent given the class label\n\n\\(p(\\mathbf x | y = c) = \\prod p(x_d | y = c)\\)\n\nThe assumption is naive since it will rarely hold true\n\\(p(y = c | x, \\theta) \\propto \\pi(y = c) \\prod p(x_d | y = c, \\theta_{dc})\\)\nModel has very few parameters and is easy to estimate\nThe distribution of \\(p(x_d | y = c)\\) is\n\nBernoulli for binary\nCategorical for categorical\nGaussian for continuous\n\n\n\n\n\n\nGenerative Classifiers are better at handing missing data or unlabeled data\nDiscriminative Models give more robust estimates for posterior probabilities"
  },
  {
    "objectID": "probml-10-logistic_regression.html",
    "href": "probml-10-logistic_regression.html",
    "title": "23  Logistic Regression",
    "section": "",
    "text": "Binary Logisitc Regression\n\ny is binary {0,1}\n\\(p(y | x, \\theta) = Ber(y | \\sigma(w^Tx + b))\\)\n\\(\\sigma\\) is the sigmoid function\n\\(p(y = 1 | x, \\theta) = \\sigma(w^Tx +b)\\)\nAlternative equivalent notation y is {-1, +1}\nCompact notation:\n\n\\(p(\\tilde y | x, \\theta) = \\sigma(\\tilde y \\times (w^tx + b))\\)\n\nIf the misclassification cost is same across classes, optimal decision rule\n\npredict y = 1 if class 1 is more likely\n\\(p(y = 1 | x) > p (y = 0 | x)\\)\n\\(\\log {p(y = 1 |x) \\over p(y = 0 | x)} > 1\\)\n\\(w^Tx + b > 0\\)\n\\(w^Tx + b\\) is the linear decision boundary of the classifier\n\nMaximum Likelihood Estimation\n\nMinimize the NLL\n\\(\\text{NLL} = - \\log \\prod \\text{Ber}(y| \\sigma(w^Tx +b))\\)\n\\(\\text{NLL} = -\\sum y \\log(\\hat y) = H(y, \\hat y)\\) i.e. binary cross-entropy\nIf compact notation is used\n\n\\(\\text{NLL} = \\sum \\log \\sigma (\\tilde y (w^Tx+b))\\)\n\\(\\text{NLL} = \\sum \\log ( 1 + \\exp (\\tilde y (w^Tx +b))))\\)\n\nOptimization:\n\n\\(\\Delta \\text{NLL} =0\\) is the first order condition\n\\(\\Delta \\sigma(x) = \\sigma(x) \\times (1 - \\sigma(x))\\)\n\\(\\Delta NLL = \\sum (\\hat y - y) x\\)\nSum of residuals weighted by inputs\nHessian is positive definite making the optimization convex\nMinimization of NLL can be achieved by first order methods like SGD\nSecond order methods like Newton’s method can result in faster convergence\n\nIRLS (iteratively weighted least squares) is the equivalent\n\n\n\nMAP Estimation\n\nMLE estimation leads to overfitting\nUse zero mean Gaussian priors over w\n\\(p(w) = N(0, \\lambda ^{-1} I)\\)\n\\(L(w) = NLL(w) + \\lambda ||w||^2\\)\n\\(\\lambda\\) is the L2 regularization which penalizes the weights from growing large\nGiven the gaussian priors assume zero mean in MAP, it’s important to standardize the input features to make sure they are on the same scale\n\n\n\n\n\n\nMultinomial Logistic Regression assumes categorical distribution instead of Bernoulli\n\n\\(p(y=c|x, \\theta) = { \\exp^{a_c} \\over \\sum \\exp ^a}\\)\nIf features are made class dependent, the model is called maximum entropy classifier\n\nCommonly used in NLP\n\\(p(y=c|w, x) \\propto exp(w^T\\phi(x,c))\\)\n\n\n\n\n\n\nHierarchical Classification\n\nLabels follow a taxonomy\nLabel Smearing: Label is propagated to all the parent nodes\nSet up as multi-label classification problem\n\n\n\n\n\nHandling Many Classes\n\nHierarchical Softmax\n\nFaster computation of normalization constant in softmax\nPlace the output nodes in tree structure with frequent classes sitting on top\n\nClass Imbalance\n\nLong-tail has little effect on loss and model may ignore these classes\nUse sampling startegies\n\\(p_c = N_c^q / \\sum N_c^q\\)\nInstance based sampling: q = 1\nClass balanced sampling: q = 0\nSquare root sampling: q = 0.5\n\n\n\n\n\n\nRobust Logistic Regression\n\nRobust to outliers\nMixture Model\n\nSmoothen the likelihood with uniform Bernoulli prior\n\\(p(y | x) = \\pi Ber(0.5) + (1 - \\pi) Ber(y |x, \\theta)\\)\n\nBi-tempered Logistic Loss\n\nTempered Cross-Entropy\n\nHandles mislabeled outliers away from the decision boundary\n\nTempered Softmax\n\nHandles mislabeled points near the decision boundary\n\n\nProbit Approximtion\n\nSigmoid function is similar in shape to Gaussian CDF\nUsing it gives the probit approximation"
  },
  {
    "objectID": "probml-11-linear_regression.html",
    "href": "probml-11-linear_regression.html",
    "title": "24  Linear Regression",
    "section": "",
    "text": "Least Square Estimate\n\nMinimize the negative log likelihood (NLL)\n\\(\\text{NLL}(w, \\sigma^2) = {1 \\over 2\\sigma^2} \\sum (y - \\hat y)^2 + {N \\over 2} \\log(2\\pi\\sigma^2)\\)\nFirst term is referred as Residual Sum Squares (RSS)\nOrdinary Least Squares\n\n\\(\\Delta_w RSS = 0\\)\n\\(X^TXw = X^Ty\\)\nNormal Equation because \\(Xw - y\\) is orthogonal to \\(X\\)\n\\(w = (X^TX)^{-1}X^Ty\\)\nHessian is \\(X^TX\\) i.e. positive definite if X is full rank\n\nInverting \\(X^TX\\) is not easy for numerical reasons as it may be ill-conditioned or singular\nA better approach is to compute pseudo-inverse using SVD\nIf the variance is heteroskedastic, the model becomes weighted least squares.\n\n\\(p(y|x, \\theta) = N(y| wx +b, \\sigma^2(x))\\)\n\nIn case of if simple linear regression:\n\n\\(w = C_{xy} / C_{xx}\\), i.e. ratio of covariances\n\\(b = \\bar y - w \\bar x\\)\n\nIn case of two inputs with no correlation:\n\n\\(W_{X1} = R_{YX2.X1}\\)\n\\(W_{X2} = R_{YX1.X2}\\)\nPartial Regression Coefficients Y on X1 keeping X2 constant\n\nThe estimate of variance from NLL is MSE of residuals\n\n\\(\\hat \\sigma^2 = {1 \\over N}\\sum (y - \\hat y)^2\\)\n\n\n\n\n\n\nGoodness of Fit\n\nResidual Plots: Check if the residuals are normally distributed with zero mean\nPrediction Accuracy: RMSE \\(\\sqrt{ {1\\over N} RSS}\\) measures prediction error\nCoefficient of Determination: \\(R^2 = 1 - {RSS \\over TSS}\\)\n\nTSS: Prediction from baseline model: average of Y\nTSS - RSS: Reduction in variance / betterment in fit\n\n\n\n\n\n\nRidge Regression\n\nMLE / OLS estimates can result in overfitting\nMAP estimation with zero mean Gaussian Prior\n\n\\(p(w) = N(0, \\lambda^{-1}\\sigma^2)\\)\n\\(L(w) = RSS + \\lambda ||w||^2\\)\n\n\\(\\lambda\\) is the L2 regularization or weight decay\nRidge Regression is connected to PCA\n\nThe eigenvectors, eigenvalues of \\(X^TX\\) matrix\nRidge regression shrinks the eigenvectors corresponding to smaller eigenvalues.\n\\(\\lambda\\) is sometimes referred as shrinkage parameter\nAlternate way is to run PCA on X and then run regression\nRidge is a superior approach\n\n\n\n\n\n\nRobust Linear Regression\n\nMLE/MAP is sensitive to outliers\nSolutions\n\nReplace Gaussian with Student-t distribution which has heavy tails\n\nThe model does not get obsessed with outliers\nTails have more mass which gets factored in while maximizing MLE\n\nCompute MLE using EM\n\nRepresent Student-t distribution as Gaussian scale mixture\n\nUsing Laplace Distribution which is robust to outliers\nUsing Huber Loss\n\nL2 loss for small errors\nL1 loss for large erros\nLoss function is differentiable\n\nRANSAC\n\nRandom Sample Concensus\nIdentify outliers from fitted models\n\n\n\n\n\n\n\nLasso Regression\n\nLeast absolute shrinkage and selection operator\nCase where we want the parameters to be zero i.e. sparse models\nUsed for feature selection\nMAP formulation with Laplace priors\nL1 regularization\nRationale for sparsity\n\nConsider Lagrange Formulation with constraint\nL1 formulation: \\(||w|| \\le B\\)\nL2 formulation: \\(||w||^2 \\le B\\)\nL1 constraint is a rhombus\nL2 constraint is a sphere\nThe objective is more likely to intersect L1 constraint at an point corner\nAt the corners the parameters for some dimensions are 0\n\nRegularization Path\n\nStart with very high value of regularization\nGradually decrease the regularization strength\nThe set of parameters that get swept out is known as regularization path\nPerforms variable selection\n\n\n\n\n\n\nElastic Net\n\nCombination of Ridge and Lasso\nHelpful in dealing with correlated variables\nEstimates of highly correlated variables tend be equal\n\n\n\n\n\nCoordinate Descent\n\nBasis for glmnet library\nSolve for jth coefficient while all others are fixed\nCycle through the coordinates"
  },
  {
    "objectID": "probml-13-ffnn.html",
    "href": "probml-13-ffnn.html",
    "title": "25  Feed Forward NN",
    "section": "",
    "text": "Backpropagation Algorithm\n\nCompute gradient of a loss function wrt parameters in each layer\nEquivalent to repeated application of chain rule\nAutodiff: Automatic Differentiation on Computation Graph\nSuppose \\(f = f_1 \\circ f_2 \\circ f_3 \\circ f_4\\)\n\nJacobain \\(\\mathbf J_f\\) needs to be calculated for backprop\nRow Form: \\(\\triangledown f_i(\\mathbf x)\\) is the ith row of jacobian\n\nCalculated efficiently using forward mode\n\nColumn Form: \\(\\delta \\mathbf f \\over \\delta x_i\\) is the ith column of jacobian\n\nCalculated efficiently using the backward mode\n\n\n\n\n\n\n\nDerivatives\n\nCross-Entropy Layer\n\n\\(z = \\text{CrossEntropyWithLogitsLoss(y,x)}\\)\n\\(z = -\\sum_c y_c \\log(p_c)\\)\n\\(p_c = {\\exp x_c \\over \\sum_c \\exp x_c}\\)\n\\({\\delta z \\over \\delta x_c} = \\sum_c {\\delta z \\over \\delta p_i} \\times {\\delta p_i \\over \\delta x_c}\\)\nWhen i = c\n\n\\({\\delta z \\over \\delta x_c} = {-y_c \\over p_c} \\times p_c (1 - p_c) = - y_c ( 1 - p_c)\\)\n\nWhen i <> c\n\n\\({\\delta z \\over \\delta x_c} = {-y_c \\over p_c} \\times - p_i p_c = -y_c p_c\\)\n\nAdding up\n\n\\(-y_c(1-p_c) + \\sum_{i \\ne c} y_c p_i = p_c \\sum_c y_c - y_c = p_c - y_c\\)\n\n\nReLU\n\n\\(\\phi(x) = \\max(x,0)\\)\n\\(\\phi'(x,a) = I\\{x > 0\\}\\)\n\nAdjoint\n\n\\({\\delta o \\over \\delta x_j} = \\sum_{children} {\\delta o \\over \\delta x_i} \\times {\\delta x_i \\over \\delta x_j}\\)\n\n\n\n\n\n\nTraining Neural Networks\n\nMaximize the likelihood: \\(\\min L(\\theta) = -\\log p(D|\\theta)\\)\nCalculate gradients using backprop and use an optimizer to tune the parameters\nObjective function is not convex and there is no guarantee to find a global minimum\nVanishing Gradients\n\nGradients become very small\nStacked layers diminish the error signals\nDifficult to solve\nModify activation functions that don’t saturate\nSwitch to architectures with additive operations\nLayer Normalization\n\nExploding Gradients\n\nGradients become very large\nStacked layers amplify the error signals\nControlled via gradient clipping\n\nExploding / Vanishing gradients are related to the eigenvalues of the Jacobian matrix\n\nChain Rule\n\\({\\delta L \\over \\delta z_l} = {\\delta L \\over \\delta z_{l+1}} \\times {\\delta z_{l+1} \\over \\delta z_{l}}\\)\n\\({\\delta L \\over \\delta z_l} = J_l \\times g_{l+1}\\)\n\n\n\n\n\n\nNon-Saturating Activations\n\nSigmoid\n\n\\(f(x) = 1 / (1 + \\exp^{-x}) = z\\)\n\\(f'(x) = z (1 - z)\\)\nIf z is close to 0 or 1, the derivative vanishes\n\nReLU\n\n\\(f(x) = \\max(0, x)\\)\n\\(f'(x) = I \\{x > 0\\}\\)\nDerivative will exist as long as the input is positive\nCan still encounter dead ReLU problem when weights are large negative/positive\n\nLeaky ReLU\n\n\\(f(x,\\alpha) = max(\\alpha x, x); \\,\\,\\, 0< \\alpha < 1\\)\nSlope is 1 for for positive inputs\nSlope is alpha for negative inputs\nIf alpha is learnable, then we get parametric ReLU\n\nELU, SELU are smooth versions of ReLU\nSwish Activation\n\n\\(f(x) = x \\sigma(x)\\)\n\\(f'(x) = f(x) + \\sigma(x) (1 - f(x))\\)\nThe slope has additive operations\n\n\n\n\n\n\nResidual Connections\n\nIt’s easier to learn small perturbations to inputs than to learn new output\n\\(F_l(x) = x + \\text{activation}_l(x)\\)\nDoesn’t add more parameters\n\\(z_L = z_l + \\sum_{i=l}^L F_i(z_i)\\)\n\\({\\delta L \\over \\delta \\theta_l} = {\\delta L \\over \\delta z_l} \\times {\\delta z_l \\over \\delta \\theta_l}\\)\n\\({\\delta L \\over \\delta \\theta_l} = {\\delta z_l \\over \\delta \\theta_l} \\times {\\delta L \\over \\delta z_L} (1 + \\sum f'(z_i))\\)\nThe derivative of the layer l has a term that is independent of the network\n\nInitialization\n\nSampling parameters from standard normal distribution with fixed variance can result in exploding gradients\nSuppose we have linear activations sampled from standard Normal Distribution\n\n\\(o = \\sum w_j x_ij\\)\n\\(E(o) = \\sum E(w_j)E(x_{ij}) = 0\\)\n\\(V(o) \\propto n_{in} \\sigma^2\\)\n\nSimilarly for gradients:\n\n\\(V(o') \\propto n_{out} \\sigma^2\\)\n\nTo prevent the expected variance from blowing up\n\n\\(\\sigma^2 = {1 \\over (n_{in} + n_{out})}\\)\nXavier Initialization, Glorot Initialization\nHe/LeCun Initialization equivalent if n_in = n_out\n\n\n\n\n\n\nRegularization\n\nEarly Stopping\n\nStop training when error on validation set stops reducing\nRestricts optimization algorithm to transfer information from the training examples\n\nWeight Decay\n\nImpose prior on parameters and then use MAP estimation\nEncourages smaller weights\n\nSparse DNNs\n\nModel compression via quantization\n\nDropout\n\nTurnoff outgoing connections with probability p\nPrevents complex co-adaptation\nEach unit should learn to perform well on its own\nAt test time, turning on dropout is equivalent to ensemble of networks (Monte Calo Dropout)"
  },
  {
    "objectID": "probml-14-cnn.html",
    "href": "probml-14-cnn.html",
    "title": "26  Convolution NN",
    "section": "",
    "text": "Convolution Operators\n\nConvolution between two functions\n\n\\([f \\star g](z) = \\int f(u) g(z-u) du\\)\n\nSimilar to cross-correlation operator\n\n\\([w \\star x](i) = \\sum_u^{L-1} w_ux_{i+u}\\)\n\nConvolution in 2D\n\n\\([W \\star X](i,j) = \\sum_{u=0}^{H-1}\\sum_{v=0}^{W-1} w_{u,v}x_{i+u,j+v}\\)\n2D convolution is template matching, feature detection\nThe output is called feature map\n\nConvolution is matrix multiplication\n\nThe corresponding weight matrix is Toeplitz like\n\\(y = Cx\\)\n\\(C = [[w_1, w_2,0|w_3, w_4, 0|0,0,0],[0, w_1, w_2 | 0, w_3, w_4 | 0,0,0],....]\\)\nWeight matrix is sparse in a typical MLP setting\n\nValid Convolution\n\nFilter Size: \\((f_h, f_w)\\)\nImage Size: \\((x_h, x_w)\\)\nOutput Size : \\((x_h - f_w + 1, x_w - f_w + 1)\\)\n\nPadding\n\nFilter Size: \\((f_h, f_w)\\)\nImage Size: \\((x_h, x_w)\\)\nPadding Size: \\((p_h, p_w)\\)\nOutput Size : \\((x_h + 2p_h - f_w + 1, x_w + 2p_w - f_w + 1)\\)\nIf 2p = f - 1, then output size is equal to input size\n\nStrided Convolution\n\nSkip every sth input to reduce redundancy\nFilter Size: \\((f_h, f_w)\\)\nImage Size: \\((x_h, x_w)\\)\nPadding Size: \\((p_h, p_w)\\)\nStride Size: \\((s_h, s_w)\\)\nOutput Size: \\(\\lbrack {x_h + 2p_h -f_h +s_h \\over s_h}, {x_w + 2p_w -f_w + s_w \\over s_w} \\rbrack\\)\n\nMutiple channels\n\nInput images have 3 channels\nDefine a kernel for each input channel\nWeight is a 3D matrix\n\\(z_{i,j} = \\sum_H \\sum_W \\sum_C x_{si + u, sj+v, c} w_{u,v,c}\\)\n\nIn order to detect multiple features, extend the dimension of weight matrix\n\nWeight is a 4D matrix\n\\(z_{i,j,d} = \\sum_H \\sum_W \\sum_C x_{si + u, sj+v, c} w_{u,v,c,d}\\)\nOutput is a hyper column formed by concatenation of feature maps\n\nSpecial Case: (1x1) point wise convolution\n\nFilter is of size 1x1.\nOnly the number of channels change from input to output\n\\(z_{i,j,d} = \\sum x_{i,j,c}w_{0,0,c,d}\\)\n\nPooling Layers\n\nConvolution preserves information about location of input features i.e. equivariance\nTo achieve translational invariance, use pooling operation\nMax Pooling\n\nMaximum over incoming values\n\nAverage Pooling\n\nAverage over incoming values\n\nGlobal Average Pooling\n\nConvert the (H,W,D) feature maps into (1,1,D) output layer\nUsually to compute features before passing to fully connected layer\n\n\nDilated Convolution\n\nConvolution with holes\nTakes every rth input (r is the dilation rate)\nThe filters have 0s\nIncreases the receptive field\n\nTransposed Convolution\n\nProduce larger output form smaller input\nPad the input with zeros and then run the filter\n\nDepthwise\n\n\n\n\n\nNormalization\n\nVanishing / Exploding gradient issues in deeper models\nAdd extra layers to standardize the statistics of hidden units\nBatch Normalization\n\nZero mean and unit variance across the samples in a minibatch\n\\(\\hat z_n = {z_n - \\mu_b \\over \\sqrt{\\sigma^2_b +\\epsilon}}\\)\n\\(\\tilde z_n = \\gamma \\hat z_n + \\beta\\)\n\\(\\gamma, \\beta\\) are learnable parameters\nWhen applied to input layer, BN is close to unsual standardization process\nFor other layers, as model trains, the mean and variance change\n\nInternal Covariate Shift\n\nAt test time, the inference may run on streaming i.e. one example at a time\n\nSolution: After training, re-compute the mean and variance across entire training batch and then freeze the parameters\nSometimes, after recomputing, the BN parameters are fused to the hidden layer. This results in fused BN layer\n\nBN struggles when batch size is small\n\nLayer Normalization\n\nPool over channel, height and width\nMatch on batch index\n\nInstance Normalization\n\nPool over height and width\nMatch over batch index\n\nNormalization Free Networks\n\nAdaptive gradient clipping\n\n\n\nCommon Architectures\n\nResNet\n\nUses residula blocks to learn small perturbation in inputs\nResidual Block: conv:BN:ReLU:conv:BN\nUse padding, 1x1 convolution to ensure that additive operation is valid\n\nDenseNet\n\nConcatenate (rather than add) the output with the input\n\\(x \\rightarrow [x, f_1(x), f_2(x, f_1(x)), f_3(x, f_1(x), f_2(x))]\\)\nComputationally expensive\n\nNeural Architecture Search\n\nEfficeintNetV2\n\n\n\nAdversarial Exmaples\n\nWhite-Box Attacks\n\nGradient Free\nAdd small perturbation to input that changes the prediction from classifier\nTargeted attack\n\nBlack-Box Attack\n\nGradient Free\nDesign fooling images as apposed to adversarial images"
  },
  {
    "objectID": "probml-15-rnn.html",
    "href": "probml-15-rnn.html",
    "title": "27  Recurrent NN",
    "section": "",
    "text": "Vec2Seq (Sequence Generation)\n\nInput is a vector\nOutput is a sequence of arbitrary length\nOutput sequence is generated one token at a time\n\n\\(p(y_{1:T} | x) = \\sum p(y_{1:T}, h_{1:T} | x)\\)\n\\(p(y_{1:T} | x) = \\sum \\prod p(y_t | h_t) \\times p(h_t | h_{t-1} , y_{t-1}, x)\\)\n\\(p(y_t | h_t)\\) can be:\n\nCategorical\nGaussian\n\n\\(h_t = \\phi( W_{xh}[x;y_{t-1}] + W_{hh}h_{t-1} + b_h)\\)\n\nW(x,h) are input to hidden weights\nW(h,h) are hidden to hidden weights\n\n\nRNNs can have unbounded memory unlike Markov models\n\n\n\n\n\nSeq2Vec (Sequence Classification)\n\nInput is a variable length sequence\nOutput is a fixed dimension vector\nFor example: Classification Task\n\n\\(p(y|x_{1:T}) = \\text{Cat}(y|S(WH_T))\\)\n\nResults can be improved if model can depend on both past and future context\n\nApply bidirectional RNN\n\\(h^{\\rightarrow} = \\phi(W_{xh}^{\\rightarrow}x_t + W_{hh}^{\\rightarrow}h_t)\\)\n\\(h^{\\leftarrow} = \\phi(W_{xh}^{\\leftarrow}x_t + W_{hh}^{\\leftarrow}h_t)\\)\nInput to the linear layer is concatenation of the two hidden states\n\n\n\n\n\n\nSeq2Seq (Sequence Translation)\n\nInput is a variable length sequence\nOutput is a variable length sequence\nAligned Case:\n\nIf input and output length are the same\nOne label prediction per step\n\\(p(y_{1:T}|h_{1:T}) = \\sum \\prod p(y_t | h_t) I\\{h_t = f(h_{t-1},x_t)\\}\\)\n\nUnaligned Case\n\nIf input and output length are not the same\nEncoder-Decoder architecture\nEncode the sequence to get the context vector\nGenerate the output sequence using the decoder\nTeacher Forcing\n\nWhile training the model, ground truth is fed to the model and not the labels generated by the model\nTeacher’s values are force fed to the model\nSometimes results in poor test time performance\nScheduled Sampling\n\nStart with teacher forcing\nAt regular intervals feed the samples generated from the model\n\n\n\n\n\n\n\n\nBackpropagation through Time (BPTT)\n\nUnrolling the computation graph along time axis\n\\(h_t = W_{hx}x_t + W_{hh}h_{t-1} = f(x_t, h_{t-1}, w_h)\\)\n\\(o_t = W_{ho}h_t = g(h_t, w_{oh})\\)\n\\(L = {1 \\over T}\\sum l(y_t, o_t)\\)\n\\({\\delta L \\over \\delta w_h} = {1 \\over T} \\sum {\\delta l \\over \\delta w_h}\\)\n\\({\\delta L \\over \\delta w_h} = {1 \\over T} \\sum {\\delta l \\over \\delta o_t} {\\delta o_t \\over \\delta h_t} {\\delta h_t \\over \\delta w_h}\\)\n\\({\\delta h_t \\over \\delta w_h} = {\\delta h_t \\over \\delta w_h} + {\\delta h_t \\over \\delta h_{t-1}} {\\delta h_{t-1} \\over \\delta w_h}\\)\nCommon to truncate the update to length of the longest subsequence in the batch\nAs the sequence goes forward, the hidden state keeps getting multiplied by W(hh)\nGradients can decay or explode as we go backwards in time\nSolution is to use additive rather than multiplicative updates\n\n\n\n\n\nGated Recurrent Units\n\nLearn when to update the hidden state by using a gating unit\nUpdate Gate: Selectively remember important pieces of information\n\n\\(Z_t = \\sigma(W_{xz} X_t + W_{hz} H_{t-1})\\)\n\nReset Gate: Forget things and reset the hidden state when information is no longer useful\n\n\\(R_t = \\sigma(W_{rx} X_t + W_{rh} H_{t-1})\\)\n\nCandidate State\n\nCombine old memories that are not reset\n\\(\\tilde H_t = \\tanh ( W_{xh} X_t + W_{hh} R_t \\times H_{t-1})\\)\nIf reset is close to 1, standard RNN\nIf reset close to 0, standard MLP\nCaptures new short term information\n\nNew State\n\n\\(H_t = Z_t H_{t-1} + (1 - Z_t) \\tilde H_t\\)\nCaptures long term dependecies\nIf Z is close to 1, the hidden state carries as is and new inputs are ignored\n\n\n\n\n\n\nLong Short Term Memory (LSTM)\n\nMore sophisticated version of GRU\nAugment the hidden state with memory cell\nThree gates control this cell\n\nInput: \\(I_t = \\sigma( W_{ix} X_t + W_{ih} H_{t-1})\\), what gets read in\nOutput: \\(O_t = \\sigma(W_{ox} X_t + W_{oh} H_{t-1})\\), what gets read out\nForget: \\(F_t = \\sigma (W_{fx} X_t + W_{fh} H_{t-1})\\), when the cell is reset\n\nCandidate Cell State\n\n\\(\\tilde C_t = \\tanh ( W_{cx} X_t + W_{ch} H_{t-1})\\)\n\nActual Candidate:\n\n\\(C_t = F_{t} \\times C_{t-1} + I_t \\times \\tilde C_{t}\\)\n\nHidden State\n\n\\(H_t = O_t \\times \\tanh(C_t)\\)\nBoth output and hidden state for next time step\nHence, captures short term memory\n\nThe memory cell state captures long term memory\nPeephole Connections\n\nPass cell state as additional input to the gates\n\nHow does LSTM solve vanishing gradients problem?\n\n\n\n\n\nDecoding\n\nOutput is generated one token at a time\nSimple Solution: Greedy Decoding\n\nArgmax over vocab at each step\nKeep sampling unless  token output\n\nMay not be globally optimal path\nAlternative: Beam Search\n\nCompute top-K candidate outputs at each step\nExpand each one in V possible ways\nTotal VK candidates generated\n\nGPT used top-k and top-p sampling\n\nTop-K sampling: Redistribute the probability mass\nTop-P sampling: Sample till the cumulative probability exceeds p\n\n\n\n\n\n\nAttention\n\nIn RNNs, hidden state linearly combines the inputs and then sends them to an activation function\nAttention mechanism allows for more flexibility.\n\nSuppose there are m feature vectors or values\nModel decides which to use based on the input query vector q and its similarity to a set of m keys\nIf query is most similar to key i, then we use value i.\n\nAttention acts as a soft dictionary lookup\n\nCompare query q to each key k(i)\nRetrieve the corresponding value v(i)\nTo make the operation differentiable:\n\nCompute a convex combination\n\n\\(Attn(q,(k_1,v_1),(k_2, v_2)...,(k_m,v_m)) = \\sum_{i=1}^m \\alpha_i (q, \\{k_i\\}) v_i\\)\n\n\\(\\alpha_i (q, \\{k_i\\})\\) are the attention weights\n\nAttention weights are computed from an attention score function \\(a(q,k_i)\\)\n\nComputes the similarity between query and key\n\nOnce the scores are computed, use soft max to impose distribution\nMasking helps in ignoring the index which are invalid while computing soft max\nFor computational efficiency, set the dim of query and key to be same (say d)\n\nThe similarity is given by dot product\nThe weights are randomly initialized\nThe expected variance of dot product will be d.\nScale the dot product by \\(\\sqrt d\\)\nScaled Dot-Product Attention\n\nAttention Weight: \\(a(q,k) = {q^Tk \\over \\sqrt d}\\)\nScaled Dot Product Attention: \\(Attn(Q,K,V) = S({QK^T \\over \\sqrt d})V\\)\n\n\nExample: Seq2Seq with Attention\n\nConsider encoder-decoder architecture\nIn the decoder:\n\n\\(h_t = f(h_{t-1}, c)\\)\nc is the context vector from encoder\nUsually the last hidden state of the encoder\n\nAttention allows the decoder to look at all the input words\n\nBetter alignment between source and target\n\nMake the context dynamic\n\nQuery: previous hidden state of the decoder\nKey: all the hidden states from the encoder\nValue: all the hidden states from the encoder\n\\(c_t = \\sum_{i=1}^T \\alpha_i(h_{t-1}^d, \\{h_i^e\\})h_i^e\\)\n\nIf RNN has multiple hidden layers, usually take the top most layer\nCan be extended to Seq2Vec models\n\n\n\n\n\n\n\nTransformers\n\nTransformers are seq2seq models using attention in both encoder and decoder steps\nEliminate the need for RNNs\nSelf Attention:\n\nModify the encoder such that it attends to itself\nGiven a sequence of input tokens \\([x_1, x_2, x_3...,x_n]\\)\nSequence of output tokens: \\(y_i = Attn(x_i, (x_1,x_1), (x_2, x_2)...,(x_n, x_n))\\)\n\nQuery is xi\nKeys and Values are are x1,x2…xn (all valid inputs)\n\nIn the decoder step:\n\n\\(y_i = Attn(y_{i-1}, (y_1,y_1), (y_2, y_2)...(y_{i-1}, y_{i-1}))\\)\nEach new token generated has access to all the previous output\n\n\nMulti-Head Attention\n\nUse multiple attention matrices to capture different nuances and similarities\n\\(h_i = Attn(W_i^q q_i, (W_i^k k_i, W_i^v v_i))\\)\nStack all the heads together and use a projection matrix to get he output\nSet \\(p_q h = p_k h = p_v h = p_o\\) for parallel computation **How?\n\nPositional Encoding\n\nAttention is permutation invariant\nPositional encodings help overcome this\nSinusoidal Basis\nPositional Embeddings are combined with original input X → X + P\n\nCombining All the Blocks\n\nEncoder\n\nInput: $ Z = LN(MHA(X,X,X) + X$\nEncoder: \\(E = LN(FF(Z) + Z)\\)\n\nFor the first layer:\n\n$ Z = ((X))$\n\n\n\nIn general, model has N copies of the encoder\nDecoder\n\nHas access to both: encoder and previous tokens\nInput: $ Z = LN(MHA(X,X,X) + X$\nInput $ Z = LN(MHA(Z,E,E) + Z$\n\n\n\n\n\n\n\nRepresentation Learning\n\nContextual Word Embeddings\n\nHidden state depends on all previous tokens\nUse the latent representation for classification / other downstream tasks\nPre-train on a large corpus\nFine-tune on small task specific dataset\nTransfer Learning\n\nELMo\n\nEmbeddings from Language Model\nFit two RNN models\n\nLeft to Right\nRight to Left\n\nCombine the hidden state representations to fetch embedding for each word\n\nBERT\n\nBi-Directional Encoder Representations from Transformers\nPre-trained using Cloze task (MLM i.e. Masked Language Modeling)\nAdditional Objective: Next sentence Prediction\n\nGPT\n\nGenerative Pre-training Transformer\nCausal model using Masked Decoder\nTrain it as a language model on web text\n\nT5\n\nText-to-Text Transfer Transformer\nSingle model to perform multiple tasks\nTell the task to perform as part of input sequence"
  },
  {
    "objectID": "probml-16-exemplar.html",
    "href": "probml-16-exemplar.html",
    "title": "28  Exemplar Methods",
    "section": "",
    "text": "Instance-based Learning\n\nModels keep training examples around test time\nDefine similarity between training points and test input\nAssign the label based on the similarity\n\n\n\n\n\nKNN\n\nClassify the ew input based on K closest examples in the training set\n\\(p(y = c | x, D) = {1 \\over K} \\sum I\\{y=c\\}\\)\nThe closest point can be computed using Mahalanobis Distance\n\\(d_M(x,\\mu) = \\sqrt{(x-\\mu)^TM(x-\\mu)}\\)\nM is positive definite matrix\nIf M = I, then distance reduces to Euclidean matrix\n\n\n\n\n\nCurse of Dimensionality\n\nSpace volume grows exponentially with increase in dimension\n\nSuppose inputs are uniformly distributed\nAs we move from square to cube, 10% edge covers less region\n\n\n\n\n\n\nSpeed and Memory Requirements\n\nFinding K nearest neighbors slow\nKD Tree / LSH to speed up approximate neighbor calculation\n\nKD Tree:\n\nK dimensional binary search tree\n\nLSH: Similar objects go to same hash bucket\n\nShingling, Minhash, LSH\n\n\n\n\n\n\n\nOpen Set recognition\n\nNew classes appear at test time\n\nPerson Re-identification\nNovelty Detection\n\n\n\n\n\n\nLearning Distance Matrix\n\nTreat M is the distance matrix as a parameter\nLarge Margin Nearest Neighbors\nFind M such that\n\n\\(M = W^T W\\) (Positive Definite)\nSimilar points have minimum distance\nDissimilar points are at least m units away (margin)\n\n\n\n\n\n\nDeep Metric Learning\n\nReduce the curse of dimensionality\nProject he input from high dimension space to lower dimension via embedding\nNormalize the embedding\nCompute the distance\n\nEuclidean or Cosine, both are related\n\\(|e_1 - e_2|^2 = |e1|^2 + |e_2|^2 - 2e_1 e_2\\)\nEuclidean = 2 ( 1 - Cosine)\n\\(\\cos \\theta = {a \\dot b \\over ||a|| ||b||}\\)\nDerivation via trigonometry\n\n\\(\\ cos \\theta = a^2 + b ^ 2 - c^2 / 2 a b\\)\n\n\nLearn an embedding function such that similar examples are close and dissimar examples are far\nLoss functions:\n\nClassification Losses\n\nOnly learn to push examples on correct side of the decision boundary\n\nPairwise Loss\n\nSimaese Neural Network\nCommon Backbone to embed the inputs\n\\(L(\\theta, x_i, x_j) = I \\{y_i =y_j\\} d(x_i, x_j) + I \\{y_i \\ne y_j\\} [m - d(x_i, x_j)]_+\\)\nIf same class, minimize the distance\nIf different class, maximize the distance with m margin (Hinge Loss)\n\nTriplet Loss\n\nIn Pairwise Loss: positive and negative examples siloed\n\\(L(\\theta, x_i, x^+, x^-) = [m + d(x_i, x_+) - d(x_i, x_-)]_+\\)\nMinimize the distance between anchor and positive\nMaximize the distance between anchor and negative\nm is the safety margin\nNeed to mine hard negative examples that are close to the positive pairs\nComputationally slow\nUse proxies to represent each class and speed up the training\n\n\n\n\n\n\n\nKernel Density Estimation\n\nDensity Kernel\n\nDomain: R\nRange: R+\n\\(\\int K(x)dx = 1\\)\n\\(K(-x) = K(x)\\)\n\\(\\int x K(x-x_n) dx = x_n\\)\n\nGaussian Kernel\n\n\\(K(x) = {1 \\over \\sqrt{2\\pi}} \\exp(-{1\\over2}x^2)\\)\nRBF: Generalization to vector valued inputs\n\nBandwitdth\n\nParameter to control the width of the kernel\n\nDensity Estimation\n\nExtend the concept of Gaussian Mixture models to the extreme\nEach point acts as an individual cluster\n\nMean \\(x_n\\)\nConstant variance\nNo covariance\nVar-Cov matrix is \\(\\sigma^2 I\\)\n\\(p(x|D) = {1 \\over N}\\sum K_h(x - x_n)\\)\nNo model fitting is required\n\n\nKDE vs KNN\n\nKDE and KNN are closely related\nEssentially, in KNN we grow the volume around a point till we encounter K neighbors."
  },
  {
    "objectID": "probml-18-trees.html",
    "href": "probml-18-trees.html",
    "title": "29  Trees",
    "section": "",
    "text": "Model Fitting\n\n\\(L(\\theta) = \\sum_J \\sum_{i \\in R_j} (y_i, w_j)\\)\nThe tree structure is non-differentiable\nGreedy approach to grow the tree\nC4.5, ID3 etc.\nFinding the split\n\n\\(L(\\theta) = {|D_l \\over |D|} c_l + {|D_r \\over |D|} c_r\\)\nFind the split such that the new weighted overall cost after splitting is minimized\nLooks for binary splits because of data fragmentation\n\nDetermining the cost\n\nRegression: Mean Squared Error\nClassification:\n\nGini Index: \\(\\sum \\pi_ic (1 - \\pi_ic)\\)\n\\(\\pi_ic\\) probability that the observation i belongs to class c\n\\(1 - \\pi_ic\\) probability of misclassification\nEntropy: \\(\\sum \\pi_{ic} \\log \\pi_{ic}\\)\n\n\nRegularization\n\nApproach 1: Stop growing the tree according to some heuristic\n\nExample: Tree reaches some maximum depth\n\nApproach 2: Grow the tree to its maximum possible depth and prune it back\n\nHandling missing features\n\nCategorical: Consider missing value as a new category\nContinuous: Surrogate splits\n\nLook for variables that are most correlated to the feature used for split\n\n\nAdvantages of Trees\n\nEasy to interpret\nMinimal data preprocessing is required\nRobust to outliers\n\nDisadvantages of Trees\n\nEasily overfit\nPerform poorly on distributional shifts\n\n\n\n\n\n\nEnsemble Learning\n\nDecision Trees are high variance estimators\nAverage multiple models to reduce variance\n\\(f(y| x) = {1 \\over M} \\sum f_m (y | x)\\)\nIn case of classification, take majority voting\n\n\\(p = Pr(S > M/2) = 1 - \\text{Bin}(M, M/2, \\theta)\\)\nBin(.) if the CDF of the binomial distribution\nIf the errors of the models are uncorrelated, the averaging of classifiers can boost the performance\n\nStacking\n\nStacked Generalization\nWeighted Average of the models\n\\(f(y| x) = {1 \\over M} \\sum w_m f_m (y | x)\\)\nWeights have to be learned on unseen data\nStacking is different from Bayes averaging\n\nWeights need not add up to 1\nOnly a subset of hypothesis space considered in stacking\n\n\n\n\n\n\n\nBagging\n\nBootstrap aggregation\nSampling with replacement\n\nStart with N data points\nSample with replacement till N points are sampled\nProbability that a point is never selected\n\n\\((1 - {1 \\over N})^N\\)\nAs N → \\(\\infty\\), the value is roughly 1/e (37% approx)\n\n\nBuild different estimators of these sampled datasets\nModel doesn’t overly rely on any single data point\nEvaluate the performance on the 37% excluded data points\n\nOOB (out of bag error)\n\nPerformance boost relies on de-correlation between various models\n\nReduce the variance is predictions\nThe bias remains put\n\\(V = \\rho \\sigma ^ 2 + {(1 - \\rho) \\over B} \\sigma ^2\\)\nIf the trees are IID, correlation is 0, and variance is 1/B\n\nRandom Forests\n\nDe-correlate the trees further by randomizing the splits\nA random subset of features chosen for split at each node\nExtra Trees: Further randomization by selecting subset of thresholds\n\n\n\n\n\n\nBoosting\n\nSequentially fitting additive models\n\nIn the first round, use original data\nIn the subsequent rounds, weight data samples based on the errors\n\nMisclassified examples get more weight\n\n\nEven if each single classifier is a weak learner, the above procedure makes the ensemble a strong classifier\nBoosting reduces the bias of the individual weak learners to result in an overall strong classifier\nForward Stage-wise Additive Modeling\n\n\\((\\beta_m, \\theta_m) = \\arg \\min \\sum l(y_i, f_{m-1}(x_i, \\theta_{m-1}) + \\beta_m F_m(x_i, \\theta))\\)\n\\(f_m(x_i, \\theta_m) = f_{m-1}(x_i, \\theta_{m-1}) + \\beta_m F_m(x_i, \\theta_m)\\)\n\nExample: Least Square Regression\n\n\\(l(y_i, f_{m-1}(x_i) + \\beta_m F_m(x_i)) = (y_i - f_{m-1}(x_i) - \\beta_m F_m(x_i))^2\\)\n\\(l(y_i, f_{m-1}(x_i) + \\beta_m F_m(x_i)) = (r_im - \\beta_m F_m(x_i))^2\\)\nSubsequent Trees fit on the residuals from previous rounds\n\nExample: AdaBoost\n\nClassifier that outputs {-1, +1}\nLoss: Exponential Loss\n\n\\(p(y=1|x) = {\\exp F(x) \\over \\exp -F(x) + \\exp F(x)}\\)\n\\(l(y_i, x_i) = \\exp(- \\tilde y F(x_i))\\)\n\n\\(l_m = \\sum \\exp ( - \\tilde y_i f_{m-1} (x_i) - \\tilde y_i \\beta F_m(x_i)) = \\sum w_{im} \\exp (- \\tilde y_i \\beta F_m(x_i))\\)\n\\(l_m = \\exp^{-\\beta} \\sum_{\\tilde y = F(x)} w_{im} + \\exp^\\beta \\sum_{\\tilde y != F(x)} w_{im}\\)\n\\(F_m = \\arg \\min \\sum w_{im} I\\{y_i \\ne F(x)\\}\\)\nMinimize the classification error on re-weighted dataset\nThe weights are exponentially increased for misclassified examples\nLogitBoost an extension of AdaBoost\n\nNewton update on log-loss\n\n\n\n\n\n\n\nGradient Boosting\n\nNo need to derive different algorithms for different loss functions\nPerform gradient descent in the space of functions\nSolve for: $ f = L(f)$\n\nFunctions have infinite dimensions\nRepresent them by their values on the training set\nFuncton: \\(f = (f(x_1), f(x_2)...,f(x_n))\\)\nGradient: \\(g_{im} = [ {\\delta l(y_i, f(x_i)) \\over \\delta f(x_i)}]\\)\nUpdate: \\(f_m = f_{m-1} - \\beta_m g_m\\)\n\nIn the current form, the optimization is limited to the set of training points\nNeed a function that can generalize\nTrain a weak learner that can approximate the negative gradient signal\n\n\\(F_m = \\arg\\min \\sum (-g_m -F(x_i))^2\\)\nUse a shrinkage factor for regularization\n\nStochastic Gradient Boosting\n\nData Subsampling for faster computation and better generalization\n\n\n\n\n\n\nXGBoost\n\nExtreme Gradient Boosting\nAdd regularization to the objective\n\\(L(f) = \\sum l(y_i, f(x_i)) + \\Omega(f)\\)\n\\(\\Omega(f) = \\gamma J + {1 \\over 2} \\lambda \\sum w_j^2\\)\nConsider the forward stage wise additive modeling\n\\(L_m(f) = \\sum l(y_i, f_{m-1}(x_i) + F(x_i)) + \\Omega(f)\\)\nUse Taylor’s approximation on F(x)\n\\(L_m(f) = \\sum l(y_i, f_{m-1}(x_i)) + g_{im} F_m(x_i) + {1 \\over 2} h_{im} F_m(x_i)^2) + \\Omega(f)\\)\n\ng is the gradient and h is the hessian\n\nDropping the constant terms and using a decision tree form of F\n\\(F(x_{ij}) = w_{j}\\)\n\\(L_m = \\sum_j (\\sum_{i \\in I_j} g_{im}w_j) + (\\sum_{i \\in I_j} h_{im} w_j^2) + \\gamma J + {1 \\over 2} \\lambda \\sum w_j^2\\)\nSolution to the Quadratic Equation:\n\n\\(G_{jm} = \\sum_{i \\in I_j} g_{im}\\)\n\\(H_{jm} = \\sum_{i \\in I_j} h_{im}\\)\n\\(w^* = {- G \\over H + \\lambda}\\)\n\\(L(w^*) = - {1 \\over 2} \\sum_J {G^2_{jm} \\over H_{jm} + \\lambda} + \\gamma J\\)\n\nCondition for Splitting the node:\n\n\\(\\text{gain} = [{G^2_L \\over H_L + \\lambda} + {G^2_R \\over H_R + \\lambda} - {G^2_L + G^2_R \\over H_R + H_L + \\lambda}] - \\gamma\\)\nGamma acts as regularization\nTree wont split if the gain from split is less than gamma\n\n\n\n\n\n\nFeature Importance\n\n\\(R_k(T) = \\sum_J G_j I(v_j = k)\\)\nG is the gain in accuracy / reduction in cost\nI(.) returns 1 if node uses the feature\nAverage the value of R over the ensemble of trees\nNormalize the values\nBiased towards features with large number of levels\n\n\n\n\n\nPartial Dependency Plot\n\nAssess the impact of a feature on output\nMarginalize all other features except k"
  },
  {
    "objectID": "probml-19-ssl.html",
    "href": "probml-19-ssl.html",
    "title": "30  SSL",
    "section": "",
    "text": "-Transfer Learning - Some data poor tasks may have structural similarity to other data rich tasks - Transferring information from one dataset to another via shared parameters of a model - Pretrain the model on a large source dataset - Fine tune the model on a small target dataset - Chop-off the the head of the pretrained model and add a new one - The parameters may be frozen during fine-tuning - In case the parameters aren’t frozen, use small learning rates. - Adapters - Modify the model structure to customize feature extraction - For example: Add MLPs after transformer blocks and initialize them for identity mappings - Much less parameters to be learned during fine-tuning\n\nPre-training\n\nCan be supervised or unsupervised.\nSupervised\n\nImagenet is supervised pretraining.\nFor unrelated domains, less helpful.\nMore like speedup trick with a good initialization.\n\nUnsupervised\n\nUse unlabeled dataset\nMinimize reconstruction error\n\nSelf-supervised\n\nLabels are created from ulabeled dataset algorithmically\nCloze Task\n\nFill in the blanks\n\nProxy Tasks\n\nCreate representations\nSiamese Neural Networks\nCapture relationship between inputs\n\nContrastive Tasks\n\nUse data augmentation\nEnsure that similar inputs have closer representations\n\n\nSimCLR\n\nSimple Contrastive Learning for Visual Representations\nGenerate two augmented views (random crops)\nMaximize the similarity of similar views and minimize the similarity of different views\nUse large batch training to ensure diverse set of negatives\nForces the model to learn and focus on local features / views\n\nClip\n\nContrastive Language-Image Pretraining\nImage with matching text\nMaximize the similarity of the image embedding to that of the embedding of the matching text\nWorks well in case of zero-shot learning\nRequires prompt engineering to convert image metadata to labels for text embeddings\n\n\nDomain Adaption\n\nDifferent domains but same output labels\nExample - Product reviews and movie reviews\nDomain adversarial learning\nAuxiliary task: Model has to learn the source of the input\nMinimize the loss on desired classification task\nMaximize the loss on auxiliary task\nGradient reversal trick\n\nSemi-Supervised Learning\n\nLeverage large amount of unlabeled data\nLearn high level structure of data distribution from unlabeled data\nLearn fine-grained details of given task using labeled data\nAvoid labeling all of the dataset\nWorks on clusterassumption.\n\nA good decision boundary will not pass through data dense regions\n\nPseudo Labeling / Self-Training\n\nUse the model itself to infer predictions on unlabeled data\nTreat the predictions as labels for subsequent training\nInferred labels are pseudo-correct in comparison to ground truth\nSuffers from Confirmation Bias\nIf the original predictions are wrong, model becomes progressievely worse\nUse soft labels from model predicitons and scaled using softmax temperture\n\nCo-training\n\nTwo complementary views (say two sets of features) for each data point\nTrain two models independently\nScore the unlabled data point using each of the models\nIf model scores high from one model and low from another - add it to the training dataset for the low scoring model\nRepeat until convergence\n\nLabel Propagation\n\nTransductive learning\nManifold Assumption: Similar data points should share the same label\nConstruct a graph of the dataset\n\nNodes: Data points\nEdges: Similarity between the data points\nModel Labels: Labels of the data points\n\nPropagate the labels in such a way that there is minimal label disagreement between node and it’s neighbours\nLabel guesses for unlabeled data that can be used for superised learning\nDetails:\n\nM labeled points, N unlabeled points\nT: (M+N) x (M+N) transition matrix of normalized edge weights\nY: Label matrix for class distribution of (M+N) x C dimension\nUse transition matrix to propagate labels Y = TY until convergence\n\nSuccess depends on calculating similarity between data points\n\nConsistency Regularization\n\nSmall perturbation to input data point should not change the model predicitons\n\n\nGenerative Models\n\nNatural way of using unlabeled data by learning a model of data generative process.\nVariational Autoencoders\n\nModels joint distribution of data (x) and latent variables (z)\nFirst sample: \\(z \\sim p(z)\\) and then sample \\(x\\sim p(x|z)\\)\nEncoder: Approximate the posterior\nDecoder: Approximate the likelihood\nMaximize evidence lower bound of the data (ELBO) (derived from Jensen’s ineuqlity)\nUse VAEs to learn representations for downstream tasks\n\nGenerative Adversarial Netwworks\n\nGenerator: Maps latent distribution to data space\nDiscriminator: Distinguish between outputs of generator and true distribution\nModify discriminator to predict class labels and fake rather than just fake\n\n\nActive Learning\n\nIdentify true predictive mapping by quering as few data points as possible\nQuery Synthesis: Model asks output for any input\nPool Based: Model selects the data point from a pool of ulabeled data points\nMaximum Entropy Sampling\n\nUncertainty in predicted label\nFails when examples are ambiguous of mislabeled\n\nBayesian Active Learning by Disagreement (BALD)\n\nSelect examples where model makes predictions tht are highly diverese\n\n\nFew-Shot Learning\n\nLearn to predict from very few labeled example\nOne-Shot Learning: Learn to predict from single example\nZero-Shot Lerning: Learn to predict without labeled examples\nModel has to generalize for unseen labels during traning time\nWorks by learning a distance metric\n\nWeak Supervision\n\nExact label not aviabale for data points\nDistribution of labels for each case\nSoft labels / label smoothing"
  },
  {
    "objectID": "probml-21-recsys.html",
    "href": "probml-21-recsys.html",
    "title": "31  Rec Sys",
    "section": "",
    "text": "Feedback\n\nExplicit\n\nRating, Like or Dislike from users\nSparse, Values missing at at random\n\nImplicit\n\nMonitor user actions - click vs no-click, watch vs skip etc.\nSparse positive-only ratings matrix\n\n\nCollaborative Filtering\n\nUsers collaborate on recmmending items\nImpute values by looking at how other similar users have rated the item\n\\(\\hat Y_{ui} = \\sum sim(u_i, u') Y_{u', i}\\)\nTypical approach to calculate similarity is to compare the available ratings from both users\nData Sprsity makes it challenging\nMatrix Factorization\n\nView the problem as that of matrix completion\nPredict all the missing entries of the ratings matrix\nOptimization: \\(L = ||Z - Y||^2\\)\nBreak up Z into low rank matrices: \\(Z = U^TV\\)\nU is the matrix corresponding to users, V is the matrix corresponding to items\nSince Y is incomplete, Z can’t be found using SVD. (unless missing imputation)\nUse ALS (Alternating Least Squares). Estimate U given V and V given U.\nAdd user-specific and item specific biases\n\\(\\hat y_{ui} = \\mu + b_u + c_i + u_u^T v_i\\)\n\\(L = \\sum (y_{ui} - \\hat y_{ui})^2\\)\n\nProbabilistic Matrix Factorization\n\nConvert the ratings to a probabilistic model\n\\(p(Y=y) = N(\\mu + b_u + c_i + u_u^T v_i, \\sigma^2)\\)\n\n\nBayesian Personalized Ranking\n\nImplicit Feedback\nRanking Loss: Model ranks item i (positive set) ahead of j (negative set) for user u\n\n\\(p(y = (u,i,j) | \\theta) = \\sigma(f(u,i;\\theta) - f(u,j;\\theta))\\)\n\nUse hinge-loss to estimate the parameters\n\nFactorization Machines\n\nPredict the rating for each user-item pair using one-hot encodings\n\n\\(x = \\text{concat}[user, item]\\)\n\n\\(f(x) = \\mu + \\sum w_i x_i + \\sum \\sum (v_i v_j) x_i x_j\\)\nThe dot product captures the interactions between users, items and user-items.\nUsing a low-rank matrix it reduces the number of parameters to be estimated.\nAdding contextual information to solve for cold-start is straight forward.\nIf explicit feedback, use MSE Loss.\nIf implicit feedback, use Ranking Loss.\n\nCold-Start Problem\n\nDifficult to generate predictions for new user or new item.\nLeverage side information. Item and user metadata.\n\nExploration-Exploitation Tradeoff\n\nCounterfactual - Is there an item the user would have liked but wasn’t shown?\nRecommend items for which response is uncertain."
  },
  {
    "objectID": "gen-00.html",
    "href": "gen-00.html",
    "title": "General ML Notes",
    "section": "",
    "text": "Resources: Statquest Videos, Cornell (CS4780)\nTopics Covered:\n\nStatistics\nDecision Trees\nGradient Boosting\nXGBoost\nClustering\nSVMs\nDimensionality Reduction\nRegression"
  },
  {
    "objectID": "gen-01-basic-statistics.html",
    "href": "gen-01-basic-statistics.html",
    "title": "1  Basic Statistics",
    "section": "",
    "text": "A characteristic that can be measured for each data point\n\nQuantitative: Numerical\nCategorical: Categories\n\nMeasurement Scales:\n\nQuantitative: Interval Scale\nQualitative:\n\nNominal Scale (Unordered)\nOrdinal Scale (Ordered)\n\n\nStatistic varies from sample to sample drawn from the same distribution\n\nSampling Bias + Sampling Error\n\nSampling Error\n\nError that occurs on account of using a sample to calculate the population statistic\n\nSampling Bias\n\nSelection Bias\nResponse Bias\nNon-Response Bias\n\nSimple Random Sampling\n\nEach data point has equal probability of being selected\n\nStratified Random Sampling\n\nDivide population into strata and select select random samples from each\n\nCluster Sampling\n\nDivide population into clusters and select select random samples from each\n\nMulti-Stage Sampling\n\nCombination of sampling methods"
  },
  {
    "objectID": "gen-02-decision_trees.html",
    "href": "gen-02-decision_trees.html",
    "title": "33  Decision Trees",
    "section": "",
    "text": "Recursively split the input / feature space using stubs i.e. decision rules\n\nSplits are parallel to the axis\n\nMathematical Represenation\n\n\\(R_j = \\{ x : d_1 <= t_1, d_2 >= t_2 ... \\}\\)\n\n\\(\\hat Y_i = \\sum_j w_j I\\{x_i \\in R_j\\}\\)\n\\(w_j = \\frac{\\sum_i y_i I \\{x_i \\in R_j\\}}{\\sum_i I \\{x_i \\in R_j\\}}\\)\n\nTypes of Decision Trees\n\nBinary Splits\n\nClassification and Regression Trees (CART)\nC4.5\n\nMultiple Splits:\n\nCHAID (Chi-Square Automatic Interaction Detection)\nID3"
  },
  {
    "objectID": "gen-03-boosting.html",
    "href": "gen-03-boosting.html",
    "title": "34  Boosting",
    "section": "",
    "text": "Combine multiple rules of thumb to make an accurate and informed decision\n\nBagging: Models are buit in parallel on different data subsets\nBoosting: Models are built in sequence with modified different samples weights\n\n\\(F(x_i) = \\sum_m \\alpha_m f_m(x_i)\\)\n\\(f_m\\) and \\(\\alpha_m\\) are fit jointly\n\n\nPAC Learning Framework\n\nProbably Approximately Correct\nIs the problem learnable?\nModel has error \\(< \\epsilon\\) with probability \\(> (1 -\\delta)\\)\n\nAn algorithm that satisfies the PAC thresholds is a strong learner\nStrong learners are complex models with many parameters and require a lot of training data\nWeak learners are algorithms that perform slightly better than random guessing\nSchapire: Strength of Weak Learnability\n\nIf a problem can be solved by strong learner, it can be solved by a collection of weak learners.\nHypothesis boosting mechanism\nConstruct three hypotheses, trained on different data subsets\n\nH1: Complete Data\nH2: Balanced Sampling of correct and incorrect predictions from H1\nH3: Disagreements between H1 and H2 predictions\nScoring: Majority Voting of H1, H2 and H3\n\nImproved performance but cannot be scaled easily\n\nAdaboost - Adaptive Boosting\n\nAdditive Model\nContruct many hypothesis (more than three)\nThe importance/weight of each new hypotheses added “adapts” or changes\n\n\\(\\alpha_m = \\frac{1}{2}\\log\\lbrack \\frac{1-\\epsilon_m}{\\epsilon_m} \\rbrack\\)\n\\(\\epsilon_m\\) si the weighted classification error\n\nEvery sample has a weight associated while constructing a weak hypothesis\n\nExponential Weighting scheme\nCorrectly Classifier: \\(w_i = w_i \\times \\exp^{\\alpha}\\)\nIncorrectly Classifier: \\(w_i = w_i \\times \\exp^{-\\alpha}\\)\n\nUnderfitting: Not enough hypothesis added to ensemble\nOverfitting: Not using weak learners as hypothesis\n\nGradient Boosting\n\nUses gradients of the loss function to compute the weights\nGradients are a proxy of how poorly a data point is classified\nAdaboost is a special case of gradient boosting"
  },
  {
    "objectID": "gen-04-xgboost.html",
    "href": "gen-04-xgboost.html",
    "title": "35  XGBoost",
    "section": "",
    "text": "Extreme Gradient Boosting\nIntroduces regularization to reduce overfitting"
  },
  {
    "objectID": "gen-05-clustering.html",
    "href": "gen-05-clustering.html",
    "title": "36  Clustering",
    "section": "",
    "text": "Unsupervised learning technique\nAssign similar data points to a single group / cluster"
  },
  {
    "objectID": "gen-06-support_vector_machines.html",
    "href": "gen-06-support_vector_machines.html",
    "title": "37  Support Vector Machines",
    "section": "",
    "text": "Classification setting\nFind the maximum-margin hyperplane that can separate the data\nBest hyperplane is the one that maximizes the margin\n\nMargin the distance of the hyperplane to closest data points from both classes\nHyperplane: \\(H : wx +b = 0\\)\n\nDistance of a point (x) to a hyperplane (h):\n\n\\(d = \\frac{|Wx + b|}{||W||^2}\\)\n\nMargin is defined by the point closest to the hyperplane\n\n\\(\\gamma(W,b) = \\min_{x \\in D} \\frac{|Wx + b|}{||W||^2}\\)\nMargin is scale invariant\n\nSVM wants to maximize this margin\n\nFor margin to be maximized, hyperplane must lie right in the middle of the two classes\nOtherwise it can be moved towards data points of the class that is further away and be further increased\n\nMathematics\n\nBinary Classification\n\n\\(y_i \\in \\{+1,-1\\}\\)\n\nNeed to find a separating hyperplane such that\n\n\\((Wx_i + b) > 0 \\; \\forall \\; y_i = +1\\)\n\\((Wx_i + b) < 0 \\; \\forall \\; y_i = -1\\)\n\\(y_i(Wx_i + b) \\ge 0\\)\n\nSVM posits that the best hyperplane is the one that maximizes the margin\n\nMargin acts as buffer which can lead to better generalization\n\nObjective\n\n\\(\\max_{W,b} \\gamma(W,b) \\; \\text{subject to} \\; y_i(Wx_i + b) > 0\\)\n\\(\\max_{W,b} \\min_{x \\in D} \\frac{|Wx + b|}{||W||^2} \\; \\text{subject to} \\; y_i(Wx_i + b) > 0\\)\nA max-min optimization problem\n\nSimplification\n\nThe best possible hyperplace is scale invariant\nAdd a constraint such that \\(|Wx +b| = 1\\)\n\nUpdated objective\n\n\\(\\max \\frac{1}{||W||^2} \\; \\text{subject to} \\; y_i(Wx_i + b) \\ge 0 \\; ; |Wx +b| = 1\\)\n\\(\\min ||W||^2 \\; \\text{subject to} \\; y_i(Wx_i + b) \\ge 0 \\; ; |Wx +b| = 1\\)\n\nCombining the contraints\n\n\\(y_i(Wx_i + b) \\ge 0\\; ; |Wx +b| = 1 \\implies y_i(Wx_i + b) \\ge 1\\)\nHolds true because the objective is trying to minimize W\n\nFinal objective\n\n\\(\\min ||W||^2 \\; \\text{subject to} \\; y_i(Wx_i + b) \\ge 1\\)\n\n\nQuadratic optimization problem\n\nCan be solved quickly unlike regression which involves inverting a large matrix\nGives a unique solution unlike perceptron\n\nAt the optimal solution, some training points will lie of the margin\n\n\\(y_i(Wx_i + b) = 1\\)\nThese points are called support vectors\n\n\nSoft Constraints\n\nWhat if the optimization problem is infeasible?\n\nNo solution exists\n\nAdd relaxations i.e. allow for some misclassification\n\nOriginal: \\(y_i(Wx_i + b) \\ge 1\\)\nRelaxed: \\(y_i(Wx_i + b) \\ge 1 - \\xi_i \\; ; \\xi_i > 0\\)\n\\(\\xi_i = \\begin{cases} 1 - y_i(Wx_i + b), & \\text{if } y_i(Wx_i + b) < 1\\\\0, & \\text{otherwise} \\end{cases}\\)\nHinge Loss \\(\\xi_i = \\max (1 - y_i(Wx_i + b), 0)\\)\n\nObjective: \\(\\min ||W||^2 + C \\sum_i \\max (1 - y_i(Wx_i + b), 0)\\)\n\nC is the regularization parameter that calculates trade-off\nHigh value of C allows for less torelance on errors\n\n\nDuality\n\nPrimal problem is hard to solve\nConvert the problem to a Dual, which is easier to solve and also provides near-optimal solution to primal\nThe gap is the optimality that arises in this process is the duality gap\nLagrangian multipliers determine if strong suality exists\nConvert the above soft-margin SVM to dual via Lagrangian multipliers\n\\(\\sum \\alpha_i + \\sum\\sum \\alpha_i \\alpha_j y_i y_j x_i^T x_j\\)\n\\(\\alpha\\) is the Lagrangian multiplier\n\nKernelization\n\nSay the points are not separable in lower dimension\n\nTransform them via kernels to project them to a higher dimension\nThe points may be separable the higher dimension\nNon-linear feature transformation\nSolve non-linear problems via Linear SVM\n\nPolynomial Kernel\n\n\\(K(x_i, x_j) = (x_i^T x_j + c)^d\\)\nThe d regers to the degree of the polynomial\nExample: 2 points in 1-D (a and b) transformerd via second order polynomial kernel\n\n\\(K(a,b) = (ab + 1)^2 = 2ab+ a^2b^2 + 1 = (\\sqrt{2a}, a, 1)(\\sqrt{2b}, b, 1)\\)\n\nCalculates similarity between points in higher dimension\n\nRBF Kernel\n\n\\(K(x_i, x_j) = \\exp \\{\\gamma |x_i - x_j|^2\\}\\)\nThe larger the distance between two observations, the less is the similarity\nRadial Kernel determines how much influence each observation has on classifying new data points\n\nTransforms points to an infinite dimension space\n\nTayloy Expansion of exponential term shows how RBF is a polynomial function with inifnite dimensions\n\n2 points in 1-D (a and b) transformerd via RBF\n\n\\(K(a,b) = (1, \\sqrt{\\frac{1}{1!}}a, \\sqrt{\\frac{1}{2!}}a^2...)(1, \\sqrt{\\frac{1}{1!}}b, \\sqrt{\\frac{1}{2!}}b^2...)\\)\n\n\nKernel Trick\n\nTransforming the original dataset via Kernels and training SVM is expensive\nConvert Dot-products of support vectors to dot-products of mapping functions\n\\(x_i^T x_j \\implies \\phi(x_i)^T \\phi(x_j)\\)\nKernels are chosen in a way that this is feasible\n\n\nSVM For Regression\n\nMargins should cover all data points (Hard) or most data points (Soft)\nThe boundary now lies in the middle of the margins\n\nThe regression model to estimate the target values\n\nThe objective is to minimize the the distance of the points to the boundary\nHard SVM is sensitive to outliers"
  },
  {
    "objectID": "gen-07-dimensionality_reduction.html",
    "href": "gen-07-dimensionality_reduction.html",
    "title": "38  Dimensionality Reduction",
    "section": "",
    "text": "Curse of Dimensionality\n\nData has too many features (n << p)\nData volume required for good generalization grows exponentially\nSame edge (say 10) square and cube\n\n1x1 patch covers 1% area in quare\n1x1x1 patch covers 0.1% volume in cube\n\n\nTwo approaches\n\nFeature Selection\n\nUse only a subset of original features\n\n\nLatent Features\n\nRecombine the original features for more efficient representation\nCan be linear or non-linear"
  },
  {
    "objectID": "gen-08-regression.html",
    "href": "gen-08-regression.html",
    "title": "39  Regression",
    "section": "",
    "text": "Fit a straight line to data\n\nHow the conditional mean of response variable changes with change in explanatory variables\nMinimize SSE: \\(\\sum (y - \\hat y)^2\\)\n\\(E(y|x) = \\hat y = a + bx\\)\n\\(a = \\bar y - b \\bar x\\)\n\\(b = S_{xy} / S_{xx} = \\sum (y-\\bar y)(x - \\bar x) / \\sum (x - \\bar x)^2\\)\n\nOutlier: If a point lies far off from the rest of the data\nInfluential: If a point causes large change in slope of the fitted line\nVariance\n\nAssume constant variance (homoscedasticity)\n\\(s = \\sqrt{SSE \\over (n-p)}\\)\n\nCorrelation\n\nStrength of linear association\n\\(r = \\frac{\\sum{(x - \\bar x)(y - \\bar y)}}{\\sqrt{(x - \\bar x)^2} \\sqrt{(y - \\bar y)^2}}\\)\n\\(r = (s_x / s_y) b\\)\n\nIn case of standardized variables, r = b\nRegression towards mean\n\n\\(|r| <= 1\\)\nLine passes thourgh \\((\\bar x, \\bar y)\\)\nA unit increase in x leads to r increase in y\n\n\n\nR-Square\n\nCoefficient of determination\n\\(R^2 = (TSS - SSE) / TSS\\)\n\\(SSE = \\sum(y - \\hat y)^2\\)\n\\(TSS = \\sum(y - \\bar y)^2\\)\nSqaured of correlation coefficient\n\nStatistical Significance\n\n\\(t = b / se\\)\n\\(se = s / \\sqrt{S_{xx}}\\)\n\\(s = \\sqrt{SSE / (n-2)}\\)\n\\(t^2 = F = \\frac{r^2 / (2-1)}{1-r^2 / n - 2}\\)"
  },
  {
    "objectID": "gen-01-basic-statistics.html#descriptive-statistics",
    "href": "gen-01-basic-statistics.html#descriptive-statistics",
    "title": "1  Basic Statistics",
    "section": "1.2 Descriptive Statistics",
    "text": "1.2 Descriptive Statistics\n\nMean, Median, Mode\nShape of Distribution\n\nSymmetric around the cetral value\n\nMean coincides with median\n\n\nLeft Skewed: Left tail is longer\n\nMean > Median\n\nRight Skewed: Right tail is longer\n\nMean < Median\n\nFor skewed distributions, mean lies closer to the long tail\n\nStandard Deviation:\n\nDeviaiton is difference of observation from mean\n\\(s = \\sqrt{\\frac{\\sum deviations ^ 2}{N-1}}\\)\nMeasures variability around mean\n\nIQR: Inter Quartile Range\n\nDifference between 75th and 25th percentile\nOutlier falls beyond 1.5 x IQR\n\nEmpirical Rule:\n\nFor bell-shaped distributions - 68% volume is within 1 sdev and 95% volume within 2 sdev"
  },
  {
    "objectID": "gen-01-basic-statistics.html#probability",
    "href": "gen-01-basic-statistics.html#probability",
    "title": "1  Basic Statistics",
    "section": "1.3 Probability",
    "text": "1.3 Probability\n\n\\(E(X) = \\sum_i x_i \\times p(X=x_i)\\)\n\nFirst moment about origin\n\n\\(V(X) = E(X^2) - (E(X))^2\\)\n\nSecond moment about mean\n\n\\(z = (y - \\mu) / \\sigma\\)\nStandard Normal Distribution \\(\\sim N(0,1)\\)\n\\(Cov(X,Y) = E[(X - \\mu_x)(Y - \\mu_y)]\\)\nCorrelation \\(\\rho = Cov(X,y) / \\sigma_x \\sigma_y = E(z_x z_y)\\)\nSampling Distribution: Probability distribution of the test statistic\nSample Mean\n\nCentral Limit Theorem\n\\(\\sim N(\\mu, \\sigma / \\sqrt N)\\)\nStandard Error \\(\\sigma / \\sqrt N\\)\nStandard Deviation of Sampling Distriution\n\nCase: Exit poll survey\n\n\\(\\sim B(0.5)\\) with sample size 1800\nVariance \\(\\sqrt{p (1-p)}\\) = 0.25\nStandard Error \\(\\sigma / \\sqrt N\\) = 0.01\n99% CI: \\(0.5 \\pm 3 * 0.01 \\approx (0.47, 0.53)\\)\n\nCase: Income Survey\n\n\\(\\sim N(380, 80^2)\\) with sample size 100\n\\(P(\\bar y >= 400)\\)\nStandard Error \\(\\sigma / \\sqrt N\\) = 8\n\\(z = (400 - 380) / 8 = 2.5\\)\n\\(P(Z >= z) < 0.006\\)"
  },
  {
    "objectID": "gen-01-basic-statistics.html#confidence-interval",
    "href": "gen-01-basic-statistics.html#confidence-interval",
    "title": "1  Basic Statistics",
    "section": "1.4 Confidence Interval",
    "text": "1.4 Confidence Interval\n\nPoint Estimate: Single number representing the best guess for the parameter\nUnbiased Estimator:\n\n\\(E(\\bar X) = \\mu\\)\nIn expectation the estimator converges to the true population value\n\nEfficient Estimator:\n\n\\(N \\to \\inf \\implies V(\\bar X) \\to 0\\)\nThe standard error approaches to zero as the sample size increases\n\nInterval Estimate:\n\nConfidence Interval: Range of values that can hold the true parameter value\nConfidence Value: Probability with which true parameter value lies in CI\nPoint Estimate \\(\\pm\\) Margin of Error\n\nCI for Proportion\n\nPoint Estimate \\(\\hat \\pi\\)\nVariance \\(\\hat \\sigma^2 = \\hat \\pi (1 - \\hat \\pi)\\)\nStandard Error: \\(\\hat \\sigma / \\sqrt N = \\sqrt{ \\hat \\pi (1 - \\hat \\pi) / N}\\)\n99% CI = \\(\\hat \\pi \\pm (z_{0.01} \\times se)\\)\n\\((z_{0.01} \\times se)\\) is the margin of error\nConfidence Level increases the CI\nSample Size decreases the CI\nType 1 Error Propability: 1 - confidence level\n\nCI for Mean\n\nPoint Estimate \\(\\hat \\mu = \\bar X\\)\nVariance \\(\\hat \\sigma^2 = \\sum (X_i - \\bar X)^2 / (N-1)\\)\n\nStandard Error: \\(\\hat \\sigma / \\sqrt N\\)\nTrue population variance is unknown\nUsing sample variance as proxy introduces additional error\nConservative: replace z-distribution with t-distribution\n\n\\((t_{n-1,0.01} \\times se)\\) is the margin of error\nAssumptions:\n\nUnderlying distribution is Normal\nRandom Sampling\n\nCI generated from t-distribution are robust wrt normality assumptions violations\n\nSample Size Calculator for Proportions\n\nMargin of error depends on standard error which in turn depends on sample size\nReformulate the CI equation from above\nSample Size : \\(N = \\pi(1-\\pi) \\times (z^2 / M)\\)\n\\(\\pi\\) is the base conversation rate\nZ is the Confidence Level\nM is the margin of error\n\nSample Size Calculator for Mean\n\n\\(N = \\sigma^2 \\times (z^2 / M)\\)\n\nMaximum Likelihood Estimation\n\nPoint estimate the maximizes the probability of observed data\nSampling distributions are approximately normal\nUse them to estimate variance\n\nBootstrap\n\nResampling method\nYield standard errors and confidence intervals for measures\nNo Assumption on underlying distribution"
  },
  {
    "objectID": "gen-01-basic-statistics.html#significance-test",
    "href": "gen-01-basic-statistics.html#significance-test",
    "title": "1  Basic Statistics",
    "section": "1.5 Significance Test",
    "text": "1.5 Significance Test\n\nHypothesis is a statement about the population\nSignificance test uses data to summerize evidence about the hypothesis\nFive Parts:\n\nAssumptions\n\nType of data\nRandomization\nPopulation Distribution\nSample Size\n\nHypothesis\n\nNull\nAlternate\n\nTest Statistic: How far does the parameter value fall from the hypothesis\nP Value: The probability of observing the given (or more extreme value) of the test statistic, assuming the null hypothesis is true\n\nSmaller the p-value, stronger is the evidence for rejecting null hypothesis\n\nConclusion\n\nIf P-value is less than 5%, 95% CI doesn’t contain the hypothesized value of the parameter\n“Reject” or “Fail to Reject” null hypothesis\n\n\nHypothesis testing for Proportions\n\n\\(H_0: \\pi = \\pi_0\\)\n\\(H_1: \\pi \\ne \\pi_0\\)\n\\(z = (\\hat \\pi - \\pi_0) / se\\)\n\\(se = \\sqrt{\\pi (1-\\pi) / N}\\)\n\nHypothesis testing for Mean\n\n\\(H_0: \\mu = \\mu_0\\)\n\\(H_1: \\mu \\ne \\mu_0\\)\n\\(t = (\\bar X - \\mu_0) / se\\)\n\\(se = \\sigma / \\sqrt N\\)\nIn case of small sample sizes, replace the z-test with binomial distribution\n\n\\(P(X=x) = {N\\choose x} p^x (1-p)^{N-x}\\)\n\\(\\mu = np, \\, \\sigma=\\sqrt{np(1-p)}\\)\n\n\nOne-tail Test measure deviation in a particular direction\n\nRisky in case of skewed distributions\nt-test is robust to skewed distributions but one-tailed tests can compound error\n\nErrors\n\nType 1: Reject H0, given H0 is true: (1 - Confidence Level)\nType 2: Fail to reject H0, given H0 is false\nThe smaller P(Type 1 error) is, the larger P(Type 2 error) is.\nProbability of Type 2 error increases as statistic moves closer to H0\nPower of the test = 1 - P(Type 2 error)\n\nSignificance testing doesn’t rely solely on effect size. Small and impractical differences can be statistically significant with large enough sample sizes"
  },
  {
    "objectID": "gen-01-basic-statistics.html#comparison-of-groups",
    "href": "gen-01-basic-statistics.html#comparison-of-groups",
    "title": "1  Basic Statistics",
    "section": "1.6 Comparison of Groups",
    "text": "1.6 Comparison of Groups\n\nDifference in means between two groups\n\n\\(\\mu_1, \\mu_2\\) are the average parameter values for the two groups\nTest for the difference in \\(\\mu_1 - \\mu_2\\)\nEstimate the difference in sample means: \\(\\bar y_1 - \\bar y_2\\)\nAssume \\(\\bar y_1 - \\bar y_2 \\sim N(\\mu_1 - \\mu_2, se)\\)\n\\(E(\\bar y_1 - \\bar y_2) = \\mu_1 - \\mu_2\\)\n\\(se = \\sqrt{se_1^2 + se_2^2} = \\sqrt{s_1^2 / n_1 + s_2^2 / n_2}\\)\ns1 and s2 are standard errors for y1 and y2 respectively\nConfidence Intervals\n\n\\(\\bar y_1 - \\bar y_2 \\pm t (se)\\)\nCheck if the confidence interval contains 0 or not\n\nSignificance Test\n\n\\(t= \\frac{(\\bar y_1 - \\bar y_2) - 0}{se}\\)\ndegrees of freedom for t is (n1 + n2 -2)\n\n\nDifferences in means between two groups (assuming equal variance)\n\n\\(s = \\sqrt{\\frac{(n_1 - 1)se_1^2 + (n_2 - 1)se_2^2}{n_1 + n_2 - 2}}\\)\n\\(se = s \\sqrt{{1 \\over n_1} + {1 \\over n_2}}\\)\nConfidence Interval\n\n\\((\\bar y_1 - \\bar y_2) \\pm t (se)\\)\n\nSignificance Test\n\n\\(t = \\frac{(\\bar y_1 - \\bar y_2)}{se}\\)\n\ndegrees of freedom for t is (n1 + n2 -2)\n\n\nDifference in proportions between two groups\n\n\\(\\pi_1, \\pi_2\\) are the average proportion values for the two groups\nTest for the difference in \\(\\pi_1 - \\pi_2\\)\n\\(se = \\sqrt{se_1^2 + se_2^2} = \\sqrt{(\\hat\\pi_1(1-\\hat\\pi_1)) / n_1 + (\\hat\\pi_2(1-\\hat\\pi_2)) / n_2}\\)\nConfidence Intervals\n\n\\(\\hat \\pi_1 - \\hat \\pi_2 \\pm z (se)\\)\n\nSignificance Test\n\nCalculate population average \\(\\hat \\pi_1 = \\hat \\pi_2 = \\hat \\pi\\)\n\\(se = \\sqrt{\\hat\\pi(1-\\hat\\pi)({1 \\over n_1} + {1 \\over n_2})}\\)\n\\(z=(\\hat \\pi_1 - \\hat \\pi_2) / se\\)\n\nFisher’s Exact test for smaller samples\n\nDifferneces in matched pairs\n\nSame subject’s response across different times\nControls for other sources of variations\nLongitudnal and Crossover studies\nDifference of Means == Mean of Differences\nConfidence Interval\n\n\\(\\bar y_d \\pm t {s_d \\over \\sqrt n}\\)\n\nSignificance Test\n\nPaired-difference t-test\n\n\\(t = {(y_d - 0) \\over se}; \\; se = s_d / \\sqrt n\\)\n\nEffect Size\n\n\\((\\bar y_1 - \\bar y_2) / s\\)\n\n\n\n\n\n\nOption\nYes\nNo\n\n\n\n\nYes\nN11\nN12\n\n\nNo\nN21\nN22\n\n\n\n\nComparing Dependent Proportions (McNemar Test)\n\nA 4x4 contingency table (above)\nOne subject gets multiple treatments\n\nSay disease and side effect (Cancer and Smoking)\n\n\\(z = \\frac{n_{12} - n_{21}}{\\sqrt{n_{12} + n_{21}}}\\)\nConfidence Interval\n\n\\(\\hat \\pi_1 = (n_{11} + n_{12})/ n\\)\n\\(\\hat \\pi_2 = (n_{11} + n_{21}) / n\\)\n\\(se = {1 \\over n}\\sqrt{(n_{21} + n_{12}) - (n_{21} + n_{12})^2 / n}\\)\n\n\nNon-parametric Tests\n\nWilcoxin Test\n\nCombine Samples n1 + n2\nRank each observation\nCompare the mean of the ranks for each group\n\nMann-Whitney Test\n\nForm pairs of observations from two samples\nCount the number of samples in which sample 1 is higher than sample 2"
  },
  {
    "objectID": "gen-01-basic-statistics.html#association-between-categorical-variables",
    "href": "gen-01-basic-statistics.html#association-between-categorical-variables",
    "title": "1  Basic Statistics",
    "section": "1.7 Association between Categorical Variables",
    "text": "1.7 Association between Categorical Variables\n\nVariables are statisitcally independent if population conditional distributions match the category conditional distribution\nChi-Square Test\n\nCalculate Expected Frequencies\n(row total * column total) / total observations\n\\(f_e (xy) = (n_{.y} * n_{x.}) / N\\)\nCompare to observed frequency \\(f_o\\)\n\\(\\chi^2 = \\sum\\frac{(f_e - f_o)^2}{f_e}\\)\ndegrees of freedom: (r-1)x(c-1)\nValue of chi-sq doesn’t tell the strength of association\n\nResidual Analysis\n\nThe difference of a given cell significant or not\n\\(z = (f_e - f_o) / \\sqrt{f_e (1 - row\\%)(1 - col\\%)}\\)\n\nOdds Ratio\n\nProbability of success / Probability of failure\nCross product ratio\nFrom 2x2 Contingecy Tables:\n\n\\(\\theta = (n_{11} \\times n_{22}) / (n_{12} \\times n_{21})\\)\n\n\\(\\theta = 1 \\implies\\) equal probability\n\\(\\theta > 1 \\implies\\) row 1 has higher chance\n\\(\\theta < 1 \\implies\\) row 2 has higher chance\n\nOrdinal Variables\n\nConcordance ( C )\n\nObservation higher on one variable is higher on another as well\n\nDiscordant ( D )\n\nOtherwise\n\nCalculate Gamma\n\n\\(\\gamma = (C-D) / (C+D)\\)"
  },
  {
    "objectID": "gen-02-decision_trees.html#splitting",
    "href": "gen-02-decision_trees.html#splitting",
    "title": "33  Decision Trees",
    "section": "33.2 Splitting",
    "text": "33.2 Splitting\n\nSplit Criteria for Classification Trees\n\nThe nodes are split to decrease inpurity in classification\nGini Criterion\n\n\\(1 - \\sum_C p_{i}^2\\)\nProbability that observation belongs to class i: \\(p_i\\)\nMisclassification:\nFor a given class (say i):\n\n\\(p_i \\times p_{k \\ne i} = p_i \\times (1 - p_i)\\)\n\nAcross all classes:\n\\(\\sum_C p_i \\times (1 - p_i)\\)\n\\(\\sum_C p_i - \\sum_c p_{i}^2\\)\n\\(1 - \\sum_c p_{i}^2\\)\nRanges from (0, 0.5)\n\nEntropy Criterion\n\nMeasure of uncertainly of a random variable\nGiven an event E\n\np(E) = 1 \\(\\implies\\) No Surprise\np(E) = 0 \\(\\implies\\) Huge Surprise\nInformaion Content: \\(I(E) = \\log(1 / p(E))\\)\n\nEntropy is the expectation of this information content\n\n\\(H(E) = - \\sum p(E) \\log(p(E))\\)\nMaximum when all outcomes have same probability of occurance\n\nRanges from (0, 1)\n\n\nSplit Criteria for Regression Trees\n\nSum-Squared Error\n\\(\\sum_i (Y_i - \\bar Y)^2\\)\n\nFinding the Split\n\nFor any candidate value:\n\nCalculate the weighted average reduction in impurity / error\nWeights being the number of observations flowing in the child nodes\n\nStarting Gini\n\n\\(\\text{Gini}_{\\text{Root}}\\)\n\\(N_{\\text{Root}}\\)\n\nAfter Split\n\nChild Nodes\n\n\\(\\text{Gini}_{\\text{Left}}, N_{\\text{Left}}\\)\n\\(\\text{Gini}_{\\text{Right}}, N_{\\text{Right}}\\)\n\nUpdated Gini\n\n\\(\\frac{N_{\\text{Left}}}{N_{\\text{Root}}} \\times \\text{Gini}_{\\text{Left}} + \\frac{N_{\\text{Right}}}{N_{\\text{Root}}} \\times \\text{Gini}_{\\text{Right}}\\)\n\n\nFind the split, the results in minimum updated Gini\nUpdated Gini <= Starting Gini\nGreedy algorithms to find the best splits"
  },
  {
    "objectID": "gen-02-decision_trees.html#bias-variance-trade-off",
    "href": "gen-02-decision_trees.html#bias-variance-trade-off",
    "title": "33  Decision Trees",
    "section": "33.3 Bias-Variance Trade-off",
    "text": "33.3 Bias-Variance Trade-off\n\nBias\n\nMeasures ability of an ML algorithm to model true relationship between features and target\nSimplifying assumptions made by the model to learn the relationship\n\nExample: Linear vs Parabolic relationship\n\nLow Bias: Less restrictive assupmtions\nHigh Bias: More restrictive assumptions\n\nVariance\n\nThe difference in model performance across different datasets drawn from the same distribution\nLow Variance: Small changes to model perforamance with changes in datasets\nHigh Variance: Large changes to model perforamance with changes in datasets\n\nIrreducible Error\n\nBayes error\nCannot be reduced irrespective of the model form\n\nBest model minimizes: \\(\\text{MSE} = \\text{bias}^2 + \\text{variance}\\)\nDecision trees have low bias and high variance\nDecision trees are prone to overfitting\n\nNoisy Samples\nSmall data samples in nodes down the tree\nTree Pruning solves for overfitting\n\nAdding a cost term to objetive which captures tree complexity\n\\(\\text{Tree Score} = SSR + \\alpha T\\)\nAs the tree grows in size, the reduction in SSR has to more than offset the complexity cost"
  },
  {
    "objectID": "gen-02-decision_trees.html#nature-of-decision-trees",
    "href": "gen-02-decision_trees.html#nature-of-decision-trees",
    "title": "33  Decision Trees",
    "section": "33.4 Nature of Decision Trees",
    "text": "33.4 Nature of Decision Trees\n\nDecision Trees can model non-linear relationships (complex deicison boundaries)\nSpline regressions cannot achieve the same results\n\nSpline adds indicator variables to capture interactions and create kinks\nBut the decision boundary has to be continuous\nThe same restriction doesn’t apply to decision trees\n\nDecision Trees don’t require feature sscaling\nDecision Trees are less sensitive to outliers\n\nOutliers are of various kinds:\n\nOutliers: Points with extreme values\n\nInput Features\n\nDoesn’t impact Decision Trees\nSplit finding will ignore the extreme values\n\nOutput / Target\n\nInfluential / High-Leverage Points: Undue influence on model\n\n\nDecision Trees cannot extrapolate well to ranges outside the training data\nDecision trees cannot capture linear time series based trends / seasonality"
  },
  {
    "objectID": "gen-02-decision_trees.html#bagging",
    "href": "gen-02-decision_trees.html#bagging",
    "title": "33  Decision Trees",
    "section": "33.5 Bagging",
    "text": "33.5 Bagging\n\nBootstrap Agrregation\nSampling with repetition\n\nGiven Dataset of Size N\nDraw N samples with replacement\nProbability that a point (say i) never gets selected\n\n\\((1 - \\frac{1}{N})^N \\approx \\frac{1}{e}\\)\n\nProbability that a point (say i) gets selected atleast once\n\n\\(1 - \\frac{1}{e} \\approx 63\\%\\)"
  },
  {
    "objectID": "gen-02-decision_trees.html#random-forest",
    "href": "gen-02-decision_trees.html#random-forest",
    "title": "33  Decision Trees",
    "section": "33.6 Random Forest",
    "text": "33.6 Random Forest\n\nUse bootstrap aggregation (bagging) to create multiple datasets\n\n“Random” subspace of dataset\n\nUse subset of variables for split at each node\n\nsqrt for classification\nm//3 for regression\n\nComparison to single decision tree\n\nBias remains the same\nVariance decreases\nRandomness in data and slpits reduces the correlation in prediction across trees\nLet \\(\\hat y_i\\) be the prediction from ith tree in the forest\nLet \\(\\sigma^2\\) be the variance of \\(\\hat y_i\\)\nLet \\(\\rho\\) be the correlation between two trees in the forest\n\\(V(\\sum_i \\hat y_i) = \\sum V(\\hat y_i) + 2 \\sum\\sum COV(\\hat y_i, \\hat y_j)\\)\n\\(V(\\sum_i \\hat y_i) = n \\sigma^2 + n(n-1) \\rho \\sigma^2\\)\n\\(V( \\frac{1}{n} \\sum_i \\hat y_i) = \\rho \\sigma^2 + \\frac{1-\\rho}{n} \\sigma^2\\)\nVariance goes down as more trees are added, but bias stays put\n\nOutput Combination\n\nMajority Voting for Classification\nAveraging for Regression\n\nOut-of-bag (OOB) Error\n\nUse the non-selected rows in bagging to estimate model performance\nComparable to cross-validaiton results\n\nProximity Matrix\n\nUse OOB observations\nCount the number of times each pair goes to the same terminal node\nIdentifies observations that are close/similar to each other"
  },
  {
    "objectID": "gen-02-decision_trees.html#extratrees",
    "href": "gen-02-decision_trees.html#extratrees",
    "title": "33  Decision Trees",
    "section": "33.7 ExtraTrees",
    "text": "33.7 ExtraTrees\n\nExtremely Randomized Trees\nBagging:\n\nExtraTrees: No\nExtremely Randomized Trees: Yes\n\nMutiple trees are built using:\n\nRandom variable subset for splitting\nRandom threshold subsets for a variable for splitting"
  },
  {
    "objectID": "gen-02-decision_trees.html#variable-importance",
    "href": "gen-02-decision_trees.html#variable-importance",
    "title": "33  Decision Trees",
    "section": "33.8 Variable Importance",
    "text": "33.8 Variable Importance\n\nSplit-based importance\n\nIf variable j is used for split\n\nCalculate the improvement in Gini at the split\n\nSum this improvement across all trees and splits wherever jth variable is used\nAlternate is to calculate the number of times variable is used for splitting\nBiased in favour of continuous variables which can be split multiple times\n\nPermutation-based importance / Boruta\n\nUse OOB samples to calculate variable importance\nTake bth tree:\n\nPass the OOB samples and calculate accuracy\nPermuate jth variable and calculate the decrease in accuracy\n\nAverage this decrease in accuracy across all trees to calculate variable importance for j\nEffect is simialr to setting the coefficient to 0 in regression\nTakes into account if good surrogates are present in the dataset\n\nPartial Dependence Plots\n\nMarginal effect of of a feature on target\nUnderstand the relationship between feature and target\nAssumes features are not correlated\n\\(\\hat f(x_s) =\\frac{1}{C} \\sum f(x_s,x_i)\\)\nAverage predictions over all other variables\nCan be used to identify important interactions\n\nFriedman’s H Statistic\nIf features don’t interact Joint PDP can be decomposed into marginals\n\n\nShapely Values\n\nModel agnositc feature importance\n\nLIME"
  },
  {
    "objectID": "gen-03-boosting.html#gradient-boosting",
    "href": "gen-03-boosting.html#gradient-boosting",
    "title": "34  Boosting",
    "section": "34.2 Gradient Boosting",
    "text": "34.2 Gradient Boosting\n\nBoosting paradigm extended to general loss functions\n\nBeyond squared and exponential loss\nAny loss function that’s differentiable and convex\nGradient Descent + Boosting\n\nDerivation\n\n\\(F(x_i) = \\sum_m \\alpha_m f_m(x_i)\\)\n\\(f_m(x_i) = \\arg \\min_{f \\in H} L(F(x_i) + \\alpha f_m(x_i))\\)\nThis optimization is analogous to gradient descent in functional space\nTaylor Approximation\n\n\\(\\min L(F(x_i) + \\alpha f_m(x_i))\\)\n\\(\\min L(F(x_i)) + <\\alpha f_m(x_i), \\frac{\\delta L}{\\delta F} >\\)\n\nThe first term is constant\nThe second term is inner product over two functions\n\n\\(\\min <\\alpha f_m(x_i), \\frac{\\delta L}{\\delta F} >\\)\n\nOnly interested in the behaviour of these function over training data\nEvaluate this functions at different points in training data\nTake the inner product\n\n\\(\\min \\sum_i \\frac{\\delta L}{\\delta F(x_i)} \\times \\alpha f(x_i)\\)\nPseudo-Residual\n\n\\(-\\frac{\\delta L}{\\delta F(x_i)}\\)\n\n\\(\\min - \\sum_i r_i \\times \\alpha f(x_i)\\)\nThe ensemble makes improvement as long as \\(\\sum_i r_i f(x_i) < 0\\)\n\nModifications for CART:\n\nUsing CART as weak learners\nThe minimization problem from Taylor approx can’t be directly optimized by CART\nNeed to modify this to a functional form that can be easily handled (squared loss)\n\n\\(r_i\\) is independent of \\(f_m\\), hence \\(\\sum r_i ^2\\) is a constant\n\\(\\sum \\alpha f_m (x_i) ^2\\) can also be treated as a constant\n\nScale factor to restrict the predictions to certain range\n\n\\(\\min \\sum r_i ^2 -2 \\sum_i r_i \\times \\alpha f(x_i) + \\sum \\alpha f_m (x_i) ^2\\)\n\\(\\min \\sum (r_i - \\alpha f(x_i))^2\\)\nThis squared-loss can be minimized by CART easily\n\nOptimal value of \\(\\alpha\\) via Line Search\n\n\\(L = \\sum (r_i - \\alpha f(x_i))^2\\)\n\\(\\alpha^* = \\frac{\\sum r_i f(x_i)}{\\sum f(x_i)^2} \\approx 1\\)\n\n\n\nAlgorithm\n\nGiven\n\nData \\(\\lbrace x_i, y_i \\rbrace\\)\nLoss Function \\(L(y_i, F(x_i))\\)\n\nInitialize the model with a constant value\n\n\\(\\min L(y_i, \\gamma)\\)\n\nCompute the pseudo residual\n\n\\(r_{im} = -\\frac{\\delta L(y_i, F(x_i))}{\\delta F(x_i)}\\)\n\n\nBuild the new weak learner on pseudo residuals\n\nSay a decision tree\n\\(\\gamma_{jm} = \\arg\\min \\sum_{x_\\in R_{ij}} L(y_i, F_m(x_i) + \\gamma)\\)\nOptimal \\(\\gamma_{jm}\\) value is the average of residuals in the leaf node j\n\nOnly in case of squared loss L in regression setting\n\n\nUpdate the ensemble\n\n\\(F_{m+1}(x_i) = F_m(x_i) + \\nu \\sum_j \\gamma_{jm} I(x_i \\in R_{jm})\\)\n\\(\\nu\\) is the step size or shrinkage\nIt prevents overfitting\n1st order Taylor approximation works only for small changes\n\n\nExtension to Classification\n\nBuild a weak learner to predict log-odds\nLog Odds to Probability: \\(p = \\frac{e^{\\log(odds)}}{1+ e^{\\log(odds)}}\\)\n\nObjective is to minimize Negative Log-Likelihood\n\n\\(NLL = - \\sum y_i \\log(p_i) + (1 - y_i) \\log(1-p_i)\\)\n\\(NLL = - \\sum y_i \\log(\\frac{p_i}{1-p_i}) + log(1-p_i)\\)\n\\(NLL = - \\sum y_i \\log(odds) - \\log(1 + \\exp^{\\log(odds)})\\)\n\nCompute Psuedo Residuals\n\n\\(\\frac{\\delta NLL}{\\delta \\log(odds)}\\)\n\\(r_{im} = p_i - y_i\\)\n\nAlgorithm\n\nGiven\n\nData \\(\\lbrace x_i, y_i \\rbrace\\)\nLoss Function \\(L(y_i, F(x_i))\\)\n\nInitialize the model with a constant value\n\nLog-Odds that minimizes NLL\n\\(\\min L(y_i, \\gamma)\\)\n\nCalculate Psuedo Residuals\n\n\\(r_{im} = p_i - y_i\\)\n\nBuild the new weak learner on pseudo residuals\n\n\\(\\gamma_{jm} = \\arg \\min \\sum_{x_\\in R_{ij}} L(y_i, F_m(x_i) + \\gamma)\\)\nMinimizing this function not easy\nUse 2nd order Taylor Approximation -\n\n\\(\\min L(y_i, F(x_i) + \\gamma) = C + \\gamma \\frac{dL}{dF} + {1 \\over 2}\\gamma^2 \\frac{d^2L}{dF^2}\\)\n\\(\\gamma^* = - \\frac{dL}{dF} / \\frac{d^2L}{dF^2}\\)\n\\(\\frac{dL}{dF} = p_i - y_i\\)\n\\(\\frac{d^2L}{dF^2} = p_i (1 - p_i)\\)\n\n\\(\\gamma^* = \\frac{p_i - y_i}{p_i (1 - p_i)}\\)\n\nUpdate the ensemble\n\n\\(F_{m+1}(x_i) = F_m(x_i) + \\nu \\sum_j \\gamma_{jm} I(x_i \\in R_{jm})\\)"
  },
  {
    "objectID": "gen-03-boosting.html#adaboost-for-classification",
    "href": "gen-03-boosting.html#adaboost-for-classification",
    "title": "34  Boosting",
    "section": "34.3 Adaboost for Classification",
    "text": "34.3 Adaboost for Classification\n\nAdditively combines many weak learners to make classifications\nAdaptively re-weights incorrectly classified points\nSome weak learners get more weights in the final ensemble than others\nEach subsequent learner accounts for the mistakes made by the previous one\nUses exponential loss\n\n\\(y \\in \\{-1,1\\}\\)\n\\(L(y_i, f(x_i)) = \\exp^{-y_i f(x_i)}\\)\nUpper bound on 0-1 loss, same as logistic loss\nRises more sharply than logistic loss in case of wrong predictions\nLogitBoost minimizes logistic loss\n\n\\(\\log(1 + \\exp^{-y_i f(x_i)})\\)\n\n\n\nObjective Function\n\nAdditive Ensemble: \\(F(x) = \\sum_m \\alpha_j f_j(x)\\)\nLoss: \\(L = \\sum_i \\exp^{-\\frac{1}{2} y_i \\times F(x)}\\)\nAt mth round:\n\n\\(L = \\sum_i \\exp^{- \\frac{1}{2} y_i \\times \\sum_m \\alpha_m f_m(x)}\\)\n\\(L = \\sum_i \\exp^{-\\frac{1}{2} y_i \\times \\sum_{m-1} \\alpha_j f_j(x)} \\times \\exp^{- \\frac{1}{2} y_i \\alpha_m f_m(x_i)}\\)\nAssume all the values till m-1 as constant\n\\(L = \\sum_i w^m_i \\times \\exp^{- \\frac{1}{2} y_i \\alpha_m f_m(x_i)}\\)\nMinimizie E wrt to \\(\\alpha_m\\) to find the optimal value\n\\(L = \\sum_{corr} w^m_i \\exp^{- \\frac{1}{2} \\alpha_m} + \\sum_{incorr} w^m_i \\exp^{ \\frac{1}{2} \\alpha_m}\\)\nAssuming \\(\\epsilon_m\\) as the weighted misclassification error\n\\(L = \\epsilon_m \\exp^{\\frac{1}{2} \\alpha_m} + (1-\\epsilon_m) \\exp^{- \\frac{1}{2} \\alpha_m}\\)\nOptimal value of \\(\\alpha_m^* = \\frac{1}{2}\\log\\lbrack \\frac{1-\\epsilon_m}{\\epsilon_m} \\rbrack\\)\n\n\nAlgorithm\n\nInitialization: Give equal weights to all observations\nFor next m rounds:\n\nFit a weak learner\nCalculate weighted error \\(\\epsilon_m\\)\n\n\\(\\epsilon_m = \\frac{\\sum_i w_i^m I(y_i \\ne f_m(x_i))}{\\sum_i w_i^m}\\)\n\nCalculate the weight of the new weak learner\n\n\\(\\alpha_m = \\frac{1}{2}\\log\\lbrack \\frac{1-\\epsilon_m}{\\epsilon_m} \\rbrack\\)\n\nUpdate the sample weights\n\n\\(w_i^{m+1} = w_i^{m} \\times \\exp^{\\alpha^m \\times I(y_i \\ne f_m(x_i))}\\)\n\nNormalize\n\nScale factor \\(2 \\sqrt{\\epsilon(1-\\epsilon)}\\)\n\n\n\nCan be modified to work with regression problems"
  },
  {
    "objectID": "gen-03-boosting.html#notes",
    "href": "gen-03-boosting.html#notes",
    "title": "34  Boosting",
    "section": "34.4 Notes",
    "text": "34.4 Notes\n\nGradient boosting uses weak learners which have high bias and low variance and gradually reduces the bias over the ensemble by sequentially combining these weak learners\nChronology:\n\nAdaboost\nAdaboost as gradient descent\nGeneralize adaboost to any gradient descent\n\nDifference between Gradient Descent and Gradient Boosting\n\nIn gradient descent, the gradients are used to update parameters of the model\nIn gradient boosting, the gradients are used to build new models\nGradient boosting is a meta model that combines weak learners"
  },
  {
    "objectID": "gen-04-xgboost.html#mathematical-details",
    "href": "gen-04-xgboost.html#mathematical-details",
    "title": "35  XGBoost",
    "section": "35.1 Mathematical Details",
    "text": "35.1 Mathematical Details\n\nLoss Function\n\n\\(L(y_i, p_i)\\)\nMSE\n\n\\({1 \\over 2}\\sum{(y_i - p_i)^2}\\)\n\nNLL Loss\n\n\\(- \\sum {y_i \\log p_i + (1 - y_i) \\log (1 -p_i)}\\)\n\n\n\nIn XGBoost, the objective has regularization terms\n\n\\(\\sum_i L(y_i, p_i) + \\gamma T + {1 \\over 2} \\lambda O_{value}^2\\)\n\\(O_{value}\\) is the prediction from tree (terminal value in leaf nodes)\n\\(p_i = p_i^0 + O_{value}\\)\n\\(p_i^0\\) is the initital prediction / prediction from previous round\n\nHigh values of \\(\\lambda\\) will push the optimal output values close to 0\nSecond-order Taylor approximation to simplify the objective\n\n\\(L(y_i, p_i^0 + O_{value})\\)\n\\(L(y_i, p_i^0) + \\frac{dL}{dO_{value}} O_{value} + {1 \\over 2} \\frac{d^2L}{dO_{value}^2} O_{value}^2\\)\n\\(L(y_i, p_i^0) + g O_{value} + {1 \\over 2} H O_{value}^2\\)\n\\(L(y_i, p_i^0)\\) is constant\n\\(\\sum_i L(y_i, p_i) = \\sum_i g_i O_{value} + {1 \\over 2} \\sum H_i O_{value}^2\\)\n\nObjective Function\n\n\\(\\sum_i L(y_i, p_i) + \\gamma T + {1 \\over 2} \\lambda O_{value}^2\\)\n\\(\\sum_i g_i O_{value} + \\gamma T + {1 \\over 2} (\\sum H_i + \\lambda) O_{value}^2\\)\n\nOptimal output value\n\nDifferentiate objective function wrt \\(O_{value}\\)\n\\(O_{value}^* = - \\frac{\\sum g_i}{\\sum H_i + \\lambda}\\)\nFor MSE:\n\n\\(g_i = - (y_i - p_i^0)\\)\n\\(H_i = 1\\)\n\nFor NLL\n\nOutput value is log(odds)\n\\(g_i = - (y_i - p_i)\\)\n\\(H_i = p_i (1 - p_i)\\)\n\n\nSplitting Criteria\n\nObjective value at optimal output\n\\(\\sum_i g_i O_{value} + \\gamma T + {1 \\over 2} (\\sum H_i + \\lambda) O_{value}^2\\)\n\\({1 \\over 2}{\\sum_i g_i^2 \\over \\sum H_i + \\lambda} + \\gamma T\\)"
  },
  {
    "objectID": "gen-04-xgboost.html#regression",
    "href": "gen-04-xgboost.html#regression",
    "title": "35  XGBoost",
    "section": "35.2 Regression",
    "text": "35.2 Regression\n\nCalculate similarity score\n\n\\(G^2 / (H + \\lambda)\\)\n\\(\\lambda\\) is the regularization parameter\nReduces sensitivity to a particular observation\nLarge values will result in more pruning (shrinks similarity scores)\nIn case of MSE loss function\n\n\\(\\sum_i r_i^2 / (N + \\lambda)\\)\n\\(r\\) is the residual\n\\(N\\) is the number of observations in the node\n\n\nCalculate Gain for a split\n\n\\(\\mathrm{Gain} = \\mathrm{Similarity_{left}} + \\mathrm{Similarity_{right}} - \\mathrm{Similarity_{root}}\\)\n\n\nSplit criterion\n\n\\(\\mathrm{Gain} - \\gamma > 0\\)\n\\(\\gamma\\) controls tree complexity\nHelps prevent over fitting\nSetting \\(\\gamma = 0\\) doesn’t turn-off pruning\n\nPruning\n\nMax-depth\nCover / Minimum weight of leaf node\n\nN for regression\n\nTrees are grown fully before pruning\n\nIf a child node satisfies minimum Gain but root doesn’t, the child will still exist\n\n\nOutput Value of Tree\n\n\\(\\sum_i r_i / (N + \\lambda)\\)\n\n\nOutput Value of Ensemble\n\nInitial Prediction + \\(\\eta\\) Output Value of 1st Tree ….\nInitial prediction is the simple average of target\n\\(\\eta\\) is the learning rate"
  },
  {
    "objectID": "gen-04-xgboost.html#classification",
    "href": "gen-04-xgboost.html#classification",
    "title": "35  XGBoost",
    "section": "35.3 Classification",
    "text": "35.3 Classification\n\nCalculate similarity score\n\n\\(G^2 / (H + \\lambda)\\)\nIn case of Log loss function\n\n\\(\\sum r_i^2 / (\\sum{p_i (1-p_i)} + \\lambda)\\)\n\\(r\\) is the residual\n\\(p\\) is the previous probability estimate\n\n\nCalculate Gain for a split\n\n\\(\\mathrm{Gain} = \\mathrm{Similarity_{left}} + \\mathrm{Similarity_{right}} - \\mathrm{Similarity_{root}}\\)\n\n\nSplit criterion\n\n\\(\\mathrm{Gain} - \\gamma > 0\\)\n\nPruning\n\nMax Depth\nCover / Minimum weight of leaf node\n\n\\(\\sum{p_i (1-p_i)}\\)\n\n\nOutput Value of Tree\n\n\\(\\sum r_i / (\\sum{p_i (1-p_i)} + \\lambda)\\)\n\nOutput Value of Ensemble\n\nIntial prediction\n\nSimple average of target\n\nConvert the value to log(odds)\n\nInitial Prediction + \\(\\eta\\) Output Value of 1st Tree ….\nOutput is log(odds)\nTransform the value to probability"
  },
  {
    "objectID": "gen-04-xgboost.html#optimizations",
    "href": "gen-04-xgboost.html#optimizations",
    "title": "35  XGBoost",
    "section": "35.4 Optimizations",
    "text": "35.4 Optimizations\n\nApproximate Greedy Algorithm\n\nFinding splits faster\nHistogram based splits by bucketing the variables\n\nQuantile Sketch Algorithm\n\nApproximately calculate the quantiles parallely\nQuantiles are weighted by cover / hessian\n\n\nSparsity Aware Split Finding\n\nCalculate the split based on known data values of the variable\nFor missing data:\n\nSend the observations to left node and calcluate the Gain\nSend the observations to right node and calcluate the Gain\n\nEvaluate which path gives maximum Gain\n\nCache Aware Access\n\nStores gradients and hessians in Cache\nCompress the data and store on hard-drive for faster access"
  },
  {
    "objectID": "gen-04-xgboost.html#comparisons",
    "href": "gen-04-xgboost.html#comparisons",
    "title": "35  XGBoost",
    "section": "35.5 Comparisons",
    "text": "35.5 Comparisons\n\nXGBoost\n\nStochastic Gradient Boosting\nNo Treatment for categorical variables\nDepth-wise tree growth\n\nLightGBM\n\nGradient One-Side Sampling (GOSS)\n\nMaximum Gradient Observation are oversampled\n\nEncoding for categorical variables\nExclusive Feature Bundling to reduce number of features\nHistrogram based splitting\nLeaf-wise tree growth\n\nCatBoost\n\nMinimum Variance Sampling\nSuperior encoding technniques for categorical variables\n\nTarget encoding\n\nSymmetric tree growth"
  },
  {
    "objectID": "gen-05-clustering.html#hierarchical-agglomerative-clustering",
    "href": "gen-05-clustering.html#hierarchical-agglomerative-clustering",
    "title": "36  Clustering",
    "section": "36.1 Hierarchical Agglomerative Clustering",
    "text": "36.1 Hierarchical Agglomerative Clustering\n\nAt each step, merge the two most similar groups\nKeep giong unit there is a single group left\nSimilarity between groups\n\nSingle Link: Distance between the two closest members of each group\nComplete Link: Distance between the two farthest members of each group\nAverage Link: Average Diatnace between all pairs"
  },
  {
    "objectID": "gen-05-clustering.html#k-means-clustering",
    "href": "gen-05-clustering.html#k-means-clustering",
    "title": "36  Clustering",
    "section": "36.2 K-Means Clustering",
    "text": "36.2 K-Means Clustering\n\nHierarchical Clustering is very slow\nAlgorithm\n\nAssume there are K (hyperparameter) clusters\nAssign each data point to it’s nearest cluster center\n\n\\(z_n^* = \\arg \\min ||x_n - \\mu_k ||^2\\)\n\nUpdate the cluster centers at the end of assignments\n\n\\(\\mu_k = {1 \\over N_k}\\sum_{n, z_n=k} x_n\\)\n\n\nObjective\n\nMinimize distortion\n\\(L = \\sum_{n} ||x_n - \\mu_{z_n}||^2\\)\n\nNon-Convex objective, sensitive to intialization\nMultiple Restarts to control randomness\nK-Means++ Algorithm\n\nPick centers sequentially to cover the data\nPick initial points randomly\nFor subsequent rounds, initialize with points picked with probability proportional to the distance from it’s cluster center\nPoints far away from the cluster center are morelikely to picked in subsequent iterations\n\nK-Medoids Algorithm\n\nMore robust to outliers\nDont update the cluster center with mean\nUse average dissimilarity to all other points in the cluster (i.e. medoid)\n\\(z_n^* = \\arg \\min d(x_n,\\mu_k)\\)\n\\(\\mu_k^* = \\arg \\min_n \\sum_{n'} d(x_n,x_n')\\)\nPoint has smallest sum of distances to all other points\nPartitioning around medoid\n\nSwap the current medoid center with a non-medoid to see if the cost decreases\n\n\nSelecting the number of Clusters\n\nMinimize Distortion\n\nUse a validation dataset\nSelect the parameter that minimizes distortion on validation\nBut usually it descreases monotonically with number of clusters\n\nElbow method\n\nRate at which distortion goes down with number of clusters\n\nSilhoutte Coefficient\n\nHow similar object is to it’s own cluster compared to other clusters\nMeasures Compactness\nGiven data point i\n\n\\(a_i\\) = (Mean distance to observations in own cluster)\n\\(b_i\\) = (Mean Distance ot observations in the next closest cluster)\n\n\\(S_i = (a_i - b_i) / \\max(a_i, b_i)\\)\nAverage the score for all the K clusters\nIdeal value is 1, worst value is -1\n\n\nK-Means is a variant of EM\n\nK-Means assumes that clusters are spherical\nHard assignment in K-Means vs Soft Assignment in EM"
  },
  {
    "objectID": "gen-05-clustering.html#spectral-clustering",
    "href": "gen-05-clustering.html#spectral-clustering",
    "title": "36  Clustering",
    "section": "36.3 Spectral Clustering",
    "text": "36.3 Spectral Clustering\n\nClusters in a graph\nFind a subgraph\n\nMaxmimum number of within cluster connections\nMinimum number of between cluster connections\n\nCalculate degree and adjacency matrix\nCalculate the graph Laplacian \\(L=D-A\\)\n\n0 if the nodes are not connected\n-1 if the nodes are connected\n\nSecond smallest eigenvalue and eigenvector of L gives the best cut for graph partition\n\nThe smallest value will be zero\nGroup the nodes using second smallest eigenvector\n\nApplicable to pairwise similarity matrix\n\nGraph Representation\n\nNode is the object\nDistance denotes the edge"
  },
  {
    "objectID": "gen-05-clustering.html#dbscan",
    "href": "gen-05-clustering.html#dbscan",
    "title": "36  Clustering",
    "section": "36.4 DBSCAN",
    "text": "36.4 DBSCAN\n\nDensity based spatial clustering\nClusters are dense regions in space separated by regions with low density\nRecusrvely expand the cluster based on dense connectivity\nCan find clusters of arbitrary shape\nTwo parameters:\n\n\\(\\epsilon\\) radius\nmininum # points to be contained in the neighbourhood\n\nCore Point\n\nPoint that has mininum # points in \\(\\epsilon\\) radius\n\nDirect Density Reachble\n\nPoints in \\(\\epsilon\\) radius of core point\n\nDensity Reachable\n\nChain connects the two points\nChain is formed by considering many different core points\n\nBorder Point\n\nPoint is DDR but not core\n\nExpand the clusters recursively by collapsing DR and DDR points"
  },
  {
    "objectID": "gen-07-dimensionality_reduction.html#principal-component-analysis",
    "href": "gen-07-dimensionality_reduction.html#principal-component-analysis",
    "title": "38  Dimensionality Reduction",
    "section": "38.2 Principal Component Analysis",
    "text": "38.2 Principal Component Analysis\n\nFind a linear and orthogonal projection of data from high dimension to low dimension\n\nEncode original data \\(x \\in R^D\\) using \\(W \\in R^{D \\times L}\\)\n\nEncode: \\(z = W^T x \\in R^L\\)\n\nDecode \\(z\\) by projecting it from lower dimension to higher dimension\n\nDecode: \\(\\hat x = W z\\)\n\n\nObjective is to minimize reconstruction error\n\n\\(L(w) = {1 \\over N} \\sum ||x - \\hat x||^2\\)\n\nProof: Project all the data to one dimension\n\n\\(w_1 \\in R^D\\)\n\\(\\hat x = z_{1} w_1\\)\nOptimal value of z and w that minimizes reconstruction error\n\\(L = {1 \\over N} \\sum ||x_i - z_{i1} w_1||^2\\)\n\\(L = {1 \\over N} \\sum (x_i - z_{i1} w_1)^T(x_i - z_{i1} w_1)\\)\n\\(L = {1 \\over N} \\sum x_i^T x_i -2 z_{i1} w_1^T x_i - z_{i1} w_1^Tw_1 z_{i1}\\)\nOrthonormal Assumption \\(\\implies w_1^Tw_1 = 1\\)\n\\(L = {1 \\over N} \\sum x_i^T x_i -2 z_{i1} w_1^T x_i - z_{i1}^2\\)\nTake Derivaties wrt z and w\n\\({\\delta L \\over \\delta z_{i1}} = {1 \\over N} (-2 w_1^T x_i + 2 z_{i1}) = 0\\)\nOptimal Embedding: \\(z_{i1} = w_1^T x\\)\nPlugging the value of z in L\n\\(L = {1 \\over N} \\sum x_i^T x_i - z_{i1}^2\\)\n\\(L = C - {1 \\over N} \\sum z_{i1}^2\\)\n\\(L = C - {1 \\over N} \\sum w_1^T x_i^T x_i w_1\\)\n\\(L = - {1 \\over N} w_1^T \\Sigma w_1\\)\n\\(\\Sigma\\) is the Var-Cov matrix of X\nThe loss can be minimized trivially by scaling \\(w\\)\nTo avoid this, impose a unit-norm constraint on \\(w\\)\n\\(L = {1 \\over N} w_1^T \\Sigma w_1 + \\lambda (w_1^T w_1 - 1)\\)\n\\({\\delta L \\over \\delta w_1} = -2 \\Sigma w_1 + 2 \\lambda w_1 = 0\\)\nOptimal w is given by eigen vector of \\(\\Sigma\\)\nTo minimize the loss, pick the vector corresponding to highest eigenvalue\n\nPCA finds vectors that maximize the variance of projected data\n\n\\(L = C - {1 \\over N} \\sum z_{i1}^2\\)\nThe original data is scaled\n\\(E(z_1) = E(w_1^T x) = 0\\)\n\\(L = C + \\text{Var}(z_1)\\)\n\nGeometric Explanation\n\nFind a new axis to capture the data\nDistance of the point from origin is fixed \\(R^2\\)\n\\(D^2\\) if the distance of the point from origin along the new axis (Variance)\n\\(\\epsilon\\) if the vertical distance of the point from the new axis (Distortion)\nBy Pythagoras theorem \\(R^2 = D^2 + \\epsilon\\)\nPCA maximizes the variance \\(D^2\\)\nIs equivalent to minimizing distortion \\(\\epsilon\\) as \\(R^2\\) is constant\n\nEigenvalues euqal the sum-sq(distances) on points on the principal component axis\nUse eigenvalues to understand how much variation is captured by each principal component\nUse scree plot (varation captured vs # components) to understand how many components should be included\nThe maximum number of components are equal to the number of features in the original data\n\nFull basis\nIf data is 2D, the eigen value for the 3rd PC will be 0\n\nPrincipal components are linear combinations of original features\n\nThe weights used for linear combinations are called factor loadings\nFactor loadings denote the importance of features in capturing variance\n\nPCA + linear regression is still interpretable\n\nUse estimated coefficients and factor loadings to understand how the original variables are being used\n\nPCA is calculated using SVD (singular value decomposition)\n\n\\(X = U S V^T \\in R^{N \\times D}\\)\n\n\\(U \\in R^{N \\times N}\\) is orthonormal\n\\(S \\in R^{N \\times D}\\) is diagonal\n\\(V \\in R^{D \\times D}\\) is orthonormal\n\n\\(X^T X = (U S V^{T})^T(U S V^{T}) = V(S^TS)V^T\\)\nSince S is a diagonal matrix, \\(S^TS\\) is diagonal as well\n\\(X^T X = VDV^T\\)\nOn mutiplying both Sides by V: \\((X^T X)V = VD\\)\nD matrix gives the eigen values and V matrix gives the corresponding eigenvectors\n\nNotes\n\nPCA doesn’t work well if the interrelationships are non-linear\n\nKernel PCA, Factor Analysis\n\nPCA doesn’t work well in case of outliers\nPCA can’t handle missing data\nPCA is unsupervised\n\nLDA is a supervised dimensionality reduction technique"
  },
  {
    "objectID": "gen-07-dimensionality_reduction.html#stochastic-neighbour-embedding-sne",
    "href": "gen-07-dimensionality_reduction.html#stochastic-neighbour-embedding-sne",
    "title": "38  Dimensionality Reduction",
    "section": "38.3 Stochastic Neighbour Embedding (SNE)",
    "text": "38.3 Stochastic Neighbour Embedding (SNE)\n\nUnsupervised Non-parametric Mehtod for dimensionality reduction\nManifold is a topological space which is locally Euclidean\n\nEath is a 2D surface embedded in a 3D space\nHigh-dimensional data can lie in a low dimenison manifold\n\nIdea is to preserve nearest neighbours instead of preserving distances\nConvert the distances in high-dimension to probabilities\n\nProbability the point i will select j as it’s neighbour\nGaussian Kernel\n\\(p_{j|i} \\propto \\exp({|| x_i - x_j||^2 \\over 2\\sigma_i^2})\\)\n\\(\\sigma_i^2\\) is the variance for data point i\n\nMagnify the scale of points in dense region\nDiminish the scale of points in sparse regions\nPerplexity parameter (say 30)\nVariance will be adjusted to cover approx 30 neighbours\nBalance between local and global aspects of the data\n\n\nInitialize the low-dimnesion representations and calculate the same probability\n\n\\(q_{j|i} \\propto \\exp({|| z_i - z_j||^2})\\)\nVariance is assumed to be constant here\n\nA good representation will preserve the neighbours\n\\(p\\) and \\(q\\) are probability distributions. KL Divergence will capture the distance between them\n\\(L = KL(p||q) = \\sum_i\\sum_j p_{i|j}\\log({p_{i|j} \\over q_{i|j}})\\)\n\nIf p is high and q is low, the penalty is high\nPoints were neighbours in high dimension but not in lo dimension\nIf p is low and q is high, the penalty is low\nUnrelated points are pushed closer now\n\nCalculate \\(z\\) by minimizing KL-Div using SGD\n\n\\(\\Delta_{z_i} L = 0\\)\n\\(2 \\sum (z_i - z_j) (p_{i|j} - q_{i|j} + p_{j|i} - q_{j|i})\\)\n\nSymmetric SNE\n\nIn the above formulation the distances are not symmetric\n\\(p_{i|j} \\ne p_{j|i}\\)\nTo enforce this: \\(p_{ij} = (p_{i|j} + p_{j|i}) / 2\\)\nEquivalent to using constant variance in high-dimensional space\n\\(\\Delta_{z_i} L = 4 \\sum (z_i - z_j) (p_{ij} - q_{ij})\\)\n\nSimilar to Potential energy in a spring (F = kx)\n\\((p_{ij} - q_{ij})\\) is k\n\\((z_i - z_j)\\) is x\n\n\nt-SNE\n\nSNE has a crowding problem\nGaussian Kernel pushes moderately far away points in high dimension close together in low dimension (squared errors)\nReplace it with t-distribution that has fatter tails (probability goes to 0 slowly)\n\nThe fatter tails allow dissimilar points to be far apart in lower dimension as well\nRemoves unwanted attractive forces between points that are modelrately far in high dimension\n\n\\(q_{j|i} \\propto (1+{|| z_i - z_j||^2})^{-1}\\)\n\\(\\Delta_{z_i} L = \\sum (z_i - z_j) (p_{ij} - q_{ij}) (1 + || z_i - z_j||^2)^{-1}\\)\n\\((1 + || z_i - z_j||^2)^{-1}\\) ensures well separated clusters with tightly packed points inside\nIntroduces strong repulsions between the dissimilar datapoints that are modeled by small pairwise distance in the low-dimensional map\nCoordinates after embedding have no inherent meaning\n\nUMAP\n\nUniform Manifold Approximation and Projection\n\nSimilar to t-SNE but much faster\n\nt-SNE calculates all pairwise distances\nUMAP calculates distances between close neighbours only\n\nt-SNE start with random initialization, UMAP start with spectral embeddings\nt-SNE moves every points slightly in each iteration, UMAP can move single points or subset of points in each iteration\nMathematics\n\nt-SNE uses Gaussian desnity function to calculate the distance between points in high dimension\nUMAP uses similarity scores\n\nHyperparameter: number of neighbours (similar to perplexity in t-SNE)\nCalculate log(number of neighbours)\nCalculate similarity scores\n\\(\\exp(-(\\text{raw distance} - \\text{distance to nearest neighbour}) / \\sigma\\)\nRescale the curve such that sum of distances = log(number of neighbours)\n\nUMAP makes the scores symmetrical by \\((S_1 + S_2) - S_1S_2\\)\nInitialize a low dimension graph using Spectral Embedding\n\nDecompoistion of Graph Laplacian\nGraph Laplacian = Degree Matrix - Adjacency Matrix\n\nCalculate the similarity in low dimension using t-distrbution\n\n\\((1 + \\alpha d^{2\\beta})^{-1}\\)\nThe parameters help user control the shape of the curve\n\nCost Function\n\nCross-Entropy between graphs\n\\(\\log(1 - S_{\\text{not neighbour}}) - log(S_{\\text{neighbour}})\\)\n\n\n\nUMAP can accomodate new data (predict function) without recomputation"
  },
  {
    "objectID": "gen-08-regression.html#multivariate-regression",
    "href": "gen-08-regression.html#multivariate-regression",
    "title": "39  Regression",
    "section": "39.2 Multivariate Regression",
    "text": "39.2 Multivariate Regression\n\nTypes of relationships\n\nSpurious: Both variables jointly affected by a third variable\nMediator: An intervening third variable indirectly affects the two variables\nSuppressor: Association only exists after controlling for a third variable\n\nExtending regression to multiple explanatory variables\n\n\\(E(y|x) = \\hat y = a + b_1 x_1 + b_2 x_2\\)\nb1 is the relationship between y and x1 after controlling for all other variables (x2)\nb1 is the partial regression coefficient\nIt represents first order partial correlation\n\\(r_{yx_1.x_2}= (R^2 - r^2_{yx_2}) / (1 - r^2_{yx_2})\\)\n\nPartial Regression Plots\n\nTrue association between x1 and y after controlling for x2\nRegress y on x2: \\(\\hat y = a' + b'x_2\\)\nRegress x1 on x2: \\(\\hat x_1 = a'' + b'' x_2\\)\nPlot the residuals from first regression against the second.\n\nStatistical Significance\n\nCollective Influence\n\nF Test\n\\(F = \\frac{R^2 / p-1}{(1 - R^2) / (n-p)}\\)\n\nIndividual Influence\n\nt test\n\\(t = \\beta / se\\)\n\\(s = \\sqrt{SSE / n-p}\\)\n\nComparing Two Models\n\nComplete Model: With all the variables\nReduced Model: Dropping some of the variables\n\\(F = \\frac{(SSE_r - SSE_c)/(df_c - df_r)}{SSE_c / df_c}\\)\n\n\nANOVA\n\nTotal \\(SST = \\sum (y - \\bar y)^2, \\; df = n-1\\)\nRegression \\(SSR = \\sum (\\hat y - \\bar y)^2, \\; df = p-1\\)\nError \\(SSE = \\sum (y - \\hat y)^2, \\; df = n-p\\)\n\\(F = MSR / MSE = (SSR / df) / (SSE / df)\\)\n\nBonferroni Correction\n\nMultiple comparisons\nSignificance Level // # of comparisons"
  },
  {
    "objectID": "gen-08-regression.html#logistic-regression",
    "href": "gen-08-regression.html#logistic-regression",
    "title": "39  Regression",
    "section": "39.3 Logistic Regression",
    "text": "39.3 Logistic Regression\n\nS-shaped curve to model binary response variable\nModels underlying probability as CDF of logistic distribution\n\n\\(P(y=1) = \\frac{\\exp(x \\beta)}{1 + \\exp(x \\beta)}\\)\n\nLog-odds ratio modeled as linear function of features\n\n\\(\\log{\\frac{P(y=1)}{1- P(y=1)}} = \\alpha + \\beta x\\)\nLogit link function\n\nInterpreting Logistic Regression Model\n\nThe coefficients denote odds ratio\n\\(\\exp \\beta\\) is the odds ratio wrt of x\n\nPropensity Scores\n\nAdjust for selection bias in comparing two groups\n\nSmokers and treatment impact on Death. Treatment is more prevalant in smokers\n\n\nControl for confounding variables when computing ATE (average treatment effect)\nPropensity is the probability of being in a particular group for a given setting of explanatory variable\nCan be computed using conditional probabilities but difficult to estimate in high-dimension\nUse logistic regression to estimate how propensity depends on explanatory variables\nUse propensity score to do pair matching (weighted random sampling using proensity scores)\n\nLikelihood Ratio Test\n\nCompare two models\nProbability of observed data as a function of parameters\nLikelihood Ratio: \\(-2 (\\log l_0 - \\log l_1)\\) follows chi-squared distribution\n\nWald Statistic\n\nSquare of Z-stat: \\(\\beta / se\\)\n\nOrdinal Response (ordered categories):\n\nSetup the problem as one of cumulative logits\n\\(P(y \\le 2) = P(y=1) + P(y=2)\\)\n\nNominal Response (unordered categories):\n\nOne-vs-Rest setup: One model per class\nOne-vs-One setup: One model per pair of classes"
  }
]