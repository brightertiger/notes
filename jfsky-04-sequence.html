<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>DS Notes - 4&nbsp; Sequence Architectures</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<link href="./jfsky-05-encoder.html" rel="next">
<link href="./jfsky-03-vectors.html" rel="prev">
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Sequence Architectures</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">DS Notes</a> 
    </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./jfsky-00.html" class="sidebar-item-text sidebar-link">SLP Notes</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./jfsky-01-regex.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Regex</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./jfsky-02-tokenization.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Tokenization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./jfsky-03-vectors.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Vectors</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./jfsky-04-sequence.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Sequence Architectures</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./jfsky-05-encoder.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Encoder-Decoder Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./jfsky-06-transfer.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Transfer Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./eslr-00.html" class="sidebar-item-text sidebar-link">ESLR Notes</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./eslr-01-regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Linear Methods for Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./eslr-02-classification.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Linear Methods for Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./eslr-03-kernel-methods.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Kernel Methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./eslr-04-model-assessment.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Model Selection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./eslr-07-model-assessment.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Model Selection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./eslr-08-model-selection.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Model Selection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./eslr-09-additive-models.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Additive Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./eslr-15-random-forest.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Random Forests</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Summary</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sequence-modeling" id="toc-sequence-modeling" class="nav-link active" data-scroll-target="#sequence-modeling"><span class="toc-section-number">4.1</span>  Sequence Modeling</a></li>
  <li><a href="#recurrent-neural-networks" id="toc-recurrent-neural-networks" class="nav-link" data-scroll-target="#recurrent-neural-networks"><span class="toc-section-number">4.2</span>  Recurrent Neural Networks</a></li>
  <li><a href="#lstm" id="toc-lstm" class="nav-link" data-scroll-target="#lstm"><span class="toc-section-number">4.3</span>  LSTM</a></li>
  <li><a href="#self-attention" id="toc-self-attention" class="nav-link" data-scroll-target="#self-attention"><span class="toc-section-number">4.4</span>  Self Attention</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Sequence Architectures</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<section id="sequence-modeling" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="sequence-modeling"><span class="header-section-number">4.1</span> Sequence Modeling</h2>
<ul>
<li>FFNNs cant be used because of limited context window
<ul>
<li>Languages can have longer dependencies over arbitrary context length</li>
</ul></li>
<li>Language Models assign conditional probability to the next word
<ul>
<li><span class="math inline">\(P(W_{1:n}) = \prod P(W_i | W_{1:i-1})\)</span><br>
</li>
</ul></li>
<li>Quality of a language model is assessed by perplexity
<ul>
<li><span class="math inline">\(PP = P(W_{1:n})^{-1/n}\)</span></li>
<li>Inverse probability that the model assigns to the test sequence nomarlied by the length</li>
</ul></li>
</ul>
</section>
<section id="recurrent-neural-networks" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="recurrent-neural-networks"><span class="header-section-number">4.2</span> Recurrent Neural Networks</h2>
<ul>
<li>NN architecture that contains a cycle in its network connections</li>
<li>The hidden layer output from previous step is linked to the current hidden layer output</li>
<li>Predict using current intput and previous hidden state</li>
<li>Removes the fixed context dependency arising in FFNNs</li>
<li>The temporal hidden output can be persisited for infinite steps</li>
<li>Inference
<ul>
<li><span class="math inline">\(h_t = g(U h_{t-1} + W x_t)\)</span></li>
<li><span class="math inline">\(y_t = V (h_t)\)</span></li>
</ul></li>
<li>Training
<ul>
<li>Chain rule for backpropagation</li>
<li>Output dependens on hidden state and hiddent state depends on previous time step</li>
<li>BPTT: backpropagation through time</li>
<li>In terms of computational graph, the network is “unrolled” for the entire sequence</li>
<li>For very long sequences, use truncated BPTT</li>
</ul></li>
<li>RNNs and Language Models
<ul>
<li>Predict next word using current word and previous hidden state<br>
</li>
<li>Removes the limited context problem</li>
<li>Use word embeddings to enhance the model’s generalization ability</li>
<li>$e_t = E x_t $</li>
<li><span class="math inline">\(h_t = g(U h_{t-1} + W e_t)\)</span></li>
<li><span class="math inline">\(y_t = V (h_t)\)</span></li>
<li>Output the probability distribution over the entire vocabulary</li>
<li>Loss function: Cross entropy, difference between predictied probability and true distribution</li>
<li>Minimize the error in predicting the next word</li>
<li>Teacher forcing for training
<ul>
<li>In training phase, ignore the model output for predicting the next word.</li>
<li>Use the actual word instead</li>
</ul></li>
<li>Weight tying
<ul>
<li>Input embedding lookup and output probbaility matrix have same dimensions |V|</li>
<li>Avoid using two different matrices, use the same one instead</li>
</ul></li>
</ul></li>
<li>RNN Tasks
<ul>
<li>Sequence Labeling
<ul>
<li>NER tasks, POS tagging</li>
<li>At each step predict the current tag rather than the next word</li>
<li>Use softmax over tagset with CE loss function</li>
</ul></li>
<li>Sequence Classification
<ul>
<li>Classify entire sequences rather than the tokens</li>
<li>Use hidden state from the last step and pass to FFNN</li>
<li>Backprop will be used to update the RNN cycle links</li>
<li>Use pooling to enhance performance
<ul>
<li>Element-wise Mean, Max of all intermediate hidden states</li>
</ul></li>
</ul></li>
<li>Sequence Generation
<ul>
<li>Encoder-decoder architecture<br>
</li>
<li>Autoregressive generation</li>
<li>Use <s> as the first token (BOS) and hidden state from encoder</s></li><s>
<li>Sample form RNN, using output softmax</li>
<li>Use the embedding from the generated token as next input</li>
</s><li><s>Keep sampling till </s> (EOS) token is sampled</li>
</ul></li>
</ul></li>
<li>RNN Architectures
<ul>
<li>Stacked RNNs
<ul>
<li>Multiple RNNs “stacked together”</li>
<li>Output from one layer serves as input to another layer</li>
<li>Differening levels of abstraction across layers</li>
</ul></li>
<li>Bidirectional RNNs
<ul>
<li>Many applications have full access to input sequence</li>
<li>Process the sequence from left-to-right and right-to-left</li>
<li>Concatenate the output from forward and reversed passes</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="lstm" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="lstm"><span class="header-section-number">4.3</span> LSTM</h2>
<ul>
<li>RNNs are hard to train</li>
<li>Hidden state tends to be fairly local in practice, limited long term dependencies
<ul>
<li>Vanishing gradients</li>
<li>Repeated multiplications in backpropagation step</li>
<li>Signoid derivatives between (0-0.25) and tanh derivatives between (0-1)</li>
<li>Drives the gradients to zero over long sequence lengths</li>
<li>Infinite memeory of hidden states</li>
</ul></li>
<li>LSTMs introduce context management
<ul>
<li>Enable network to learn to forget information no longer needed</li>
<li>Persist information for likely needed for deicisions yet to come</li>
<li>Use gating mechanism (through additional weights) to control the flow of information</li>
</ul></li>
<li>Architecture
<ul>
<li>Feedforward layer</li>
<li>Sigmoid activation</li>
<li>Point-wise multiplication with the layer being gated (binary mask)</li>
</ul></li>
<li>Input Gate
<ul>
<li>Actual information</li>
<li><span class="math inline">\(g_t = \sigma(U h_{t-1} + W x_t)\)</span></li>
</ul></li>
<li>Add Gate
<ul>
<li>Select the information to keep from current context</li>
<li><span class="math inline">\(i_t = \sigma(U h_{t-1} + W x_t)\)</span></li>
<li><span class="math inline">\(j_t = i_t \odot g_t\)</span></li>
</ul></li>
<li>Forget gate
<ul>
<li>Delete information from context no longer needed</li>
<li>Weighted sum of previous hidden state and current input</li>
<li><span class="math inline">\(f_t = \sigma(U h_{t-1} + W x_t)\)</span></li>
<li><span class="math inline">\(k_t = f_t \odot c_{t-1}\)</span></li>
</ul></li>
<li>Context
<ul>
<li>Sum of add and forget</li>
<li><span class="math inline">\(c_t = j_t + k_t\)</span></li>
</ul></li>
<li>Output Gate
<ul>
<li><span class="math inline">\(o_t = \sigma(U h_{t-1} + W x_t)\)</span></li>
<li><span class="math inline">\(h_t = o_t \odot \tanh(c_t)\)</span></li>
</ul></li>
<li>In addition to hidden state, LSTMs also persist the context</li>
</ul>
</section>
<section id="self-attention" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="self-attention"><span class="header-section-number">4.4</span> Self Attention</h2>
<ul>
<li>LSTMs difficult to parallelize</li>
<li>Still not effective for very long dependencies. Bahdanau attention etc. hacks needed.</li>
<li>Transformers - Replace recurrent layers with self attention layers</li>
<li>Self Attention Mechanism
<ul>
<li>Map input to output of same length</li>
<li>At step t, model has access to all inputs upto step t
<ul>
<li>Helps with auto-regressive generation</li>
</ul></li>
<li>Computation for step t is independent of all other steps
<ul>
<li>Easy parallelization</li>
</ul></li>
<li>Compare current input to the collection which reveals its relevance in the given context</li>
<li><span class="math inline">\(y_3\)</span> is generated by comparing <span class="math inline">\(x_3\)</span> to <span class="math inline">\(x_1, x_2, x_3\)</span></li>
</ul></li>
<li>Core of Attention Approach
<ul>
<li>Comparison is done using dot product operations (large value, more similar)
<ul>
<li><span class="math inline">\(\text{score}(x_i, x_j) = x_i . x_j\)</span></li>
</ul></li>
<li>Compute attention weights
<ul>
<li><span class="math inline">\(\alpha_{ij} = \text{softmax}(\text{score}(x_i, x_j))\)</span></li>
</ul></li>
<li>Compute output
<ul>
<li><span class="math inline">\(y_i = \sum \alpha_{ij} x_j\)</span></li>
</ul></li>
</ul></li>
<li>Sophistication wrt Transformers
<ul>
<li>Each input can play three different roles
<ul>
<li>Query: When it’s being compared to other inputs (Current focus)</li>
<li>Key: When it’s acting as context (previous input) fo comparison</li>
<li>Value: When it’s being used to compute the output</li>
</ul></li>
<li>For each role, there exists a separate embedding matrix
<ul>
<li><span class="math inline">\(\text{score}(x_i, x_j) = q_i . k_j / \sqrt d\)</span></li>
<li><span class="math inline">\(y_i = \sum \alpha_{ij} v_j\)</span></li>
<li>Normalization to avoid overflow in softmax layer</li>
</ul></li>
<li>Since calculations are independent, use matrix multiplications</li>
<li>Use masking to avoid peeking into the future</li>
</ul></li>
<li>Transformer Block
<ul>
<li>Attention layer followed by FFNN with residual connections and layer norm</li>
<li><span class="math inline">\(z = \text{Layer Norm}(x + \text{Self Attention}(x))\)</span></li>
<li><span class="math inline">\(y = \text{Layer Norm}(z + \text{FFNN}(z))\)</span></li>
<li>Layer Norm dies normalization across the hidden dimension</li>
</ul></li>
<li>Multi-Head Attention
<ul>
<li>Words can exhibit different interrelationships (syntactic, semantic etc.)</li>
<li>Parallel layers to capture each of the underlying relationships</li>
<li>Concatenate the output from each of the heads</li>
</ul></li>
<li>Positional Embeddings
<ul>
<li>Shuffling input order should matter</li>
<li>Self-attention logic (unlike RNNs) doesn’t respect sequence</li>
<li>Positional embeddings modify the input embedddings based on the position in the sequence</li>
<li>Size and Cosine functions</li>
</ul></li>
<li>BERT Architecture
<ul>
<li>Base Model - 12 heads, 12 layers, 64 diemnsions, 768 size (12 * 64)</li>
<li>Large Model - 16 heads, 24 layers, 64 dimensions, 1024 size (16 * 64)</li>
</ul></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./jfsky-03-vectors.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Vectors</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./jfsky-05-encoder.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Encoder-Decoder Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>