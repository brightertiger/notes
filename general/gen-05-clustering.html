<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Clustering | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="gen-05-clustering.html" class="active">Clustering</a></li>
            <li class="nav-item"><a href="gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../probml/probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="../probml/probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="../probml/probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="../probml/probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="../probml/probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="../probml/probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="../probml/probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="../probml/probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="../probml/probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="../probml/probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">General</a>
          </div>
          <h1 class="page-title">Clustering</h1>
          <div class="page-meta"><span class="tag">General</span></div>
        </header>
        <article class="content">
          <h1 id="clustering">Clustering</h1>
<p>Clustering is the task of grouping similar objects together without being told what the groups should be. It&#39;s <strong>unsupervised learning</strong>‚Äîwe have no labels, only data. The algorithm discovers structure on its own.</p>
<p><strong>Why Clustering Matters</strong>:</p>
<ul>
<li>Customer segmentation: Group customers by behavior</li>
<li>Image segmentation: Group pixels by color/texture</li>
<li>Document organization: Group articles by topic</li>
<li>Anomaly detection: Normal patterns vs. outliers</li>
<li>Data compression: Represent data by cluster centers</li>
</ul>
<h2 id="hierarchical-agglomerative-clustering">Hierarchical Agglomerative Clustering</h2>
<p><strong>The Idea</strong>: Start with each point as its own cluster, then repeatedly merge the two most similar clusters until only one remains.</p>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>Start: Each data point is its own cluster</li>
<li>Compute all pairwise distances between clusters</li>
<li>Merge the two closest clusters</li>
<li>Repeat steps 2-3 until one cluster remains</li>
</ol>
<p><strong>This produces a dendrogram</strong>‚Äîa tree showing the merge history. Cut the tree at any height to get that many clusters.</p>
<p><strong>Defining &quot;Similarity&quot; Between Clusters</strong>:</p>
<table>
<thead>
<tr>
<th>Linkage Method</th>
<th>Definition</th>
<th>Properties</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Single Link</strong></td>
<td>Distance between two <em>closest</em> members</td>
<td>Tends to create long, chain-like clusters</td>
</tr>
<tr>
<td><strong>Complete Link</strong></td>
<td>Distance between two <em>farthest</em> members</td>
<td>Creates compact, spherical clusters</td>
</tr>
<tr>
<td><strong>Average Link</strong></td>
<td>Average distance between all pairs</td>
<td>Balance between single and complete</td>
</tr>
<tr>
<td><strong>Ward&#39;s Method</strong></td>
<td>Increase in total within-cluster variance</td>
<td>Minimizes variance, popular choice</td>
</tr>
</tbody></table>
<p><strong>When to Use</strong>:</p>
<ul>
<li>When you don&#39;t know the number of clusters ahead of time</li>
<li>When you want to see hierarchical structure</li>
<li>For small to medium datasets (doesn&#39;t scale well)</li>
</ul>
<p><strong>Limitation</strong>: Very slow! $O(n^3)$ time complexity makes it impractical for large datasets.</p>
<h2 id="k-means-clustering">K-Means Clustering</h2>
<p><strong>The Idea</strong>: Partition data into exactly $K$ clusters, each represented by its centroid (center of mass).</p>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>Choose $K$ initial cluster centers (randomly or using K-Means++)</li>
<li><strong>Assign</strong> each point to nearest center: $z_n^* = \arg\min_k ||x_n - \mu_k||^2$</li>
<li><strong>Update</strong> centers as mean of assigned points: $\mu_k = \frac{1}{N_k}\sum_{n: z_n=k} x_n$</li>
<li>Repeat steps 2-3 until assignments don&#39;t change</li>
</ol>
<p><strong>Objective Function</strong> (Distortion/Inertia):
$$L = \sum_n ||x_n - \mu_{z_n}||^2$$</p>
<p>This is the total squared distance from each point to its assigned center.</p>
<p><strong>Key Properties</strong>:</p>
<ul>
<li>Each iteration decreases (or maintains) the objective</li>
<li>Guaranteed to converge, but not necessarily to global optimum</li>
<li>Converges quickly in practice</li>
</ul>
<p><strong>The Initialization Problem</strong>:</p>
<ul>
<li>K-Means is sensitive to initial centers</li>
<li>Bad initialization ‚Üí stuck in poor local minimum</li>
<li>Solution: <strong>Multiple restarts</strong>‚Äîrun many times, keep best result</li>
</ul>
<p><strong>K-Means++ Initialization</strong> (the smart way):</p>
<ol>
<li>Choose first center randomly from data points</li>
<li>For each subsequent center:<ul>
<li>Compute distance $D(x)$ from each point to nearest existing center</li>
<li>Choose new center with probability proportional to $D(x)^2$</li>
</ul>
</li>
<li>This spreads out initial centers‚Äîpoints far from existing centers more likely to be chosen</li>
</ol>
<p><strong>Result</strong>: Much better starting point, often finds better solutions.</p>
<p><strong>K-Medoids Algorithm</strong> (PAM - Partitioning Around Medoids):</p>
<ul>
<li>Centers must be actual data points (medoids), not means</li>
<li>More robust to outliers</li>
<li>Assignment: $z_n^* = \arg\min_k d(x_n, \mu_k)$ (any distance metric)</li>
<li>Update: Find point with smallest total distance to all other cluster members</li>
<li><strong>Swap step</strong>: Try swapping current medoid with non-medoid, keep if cost decreases</li>
</ul>
<p><strong>Choosing the Number of Clusters ($K$)</strong>:</p>
<p>This is one of the hardest problems in clustering!</p>
<p><strong>Elbow Method</strong>:</p>
<ol>
<li>Plot distortion vs. $K$</li>
<li>Look for &quot;elbow&quot; where distortion stops decreasing rapidly</li>
<li>Often subjective‚Äîthe elbow isn&#39;t always clear</li>
</ol>
<p><strong>Silhouette Score</strong>:
For each point $i$:</p>
<ul>
<li>$a_i$ = average distance to other points in same cluster (cohesion)</li>
<li>$b_i$ = average distance to points in nearest other cluster (separation)</li>
<li>$S_i = \frac{b_i - a_i}{\max(a_i, b_i)}$</li>
</ul>
<p>Interpretation:</p>
<ul>
<li>$S_i \approx 1$: Point is well-clustered</li>
<li>$S_i \approx 0$: Point is on boundary between clusters</li>
<li>$S_i \approx -1$: Point may be in wrong cluster</li>
</ul>
<p>Average silhouette score across all points measures overall clustering quality.</p>
<p><strong>K-Means is EM with Hard Assignments</strong>:</p>
<ul>
<li>Expectation-Maximization (EM) uses &quot;soft&quot; assignments (probabilities)</li>
<li>K-Means uses &quot;hard&quot; assignments (0 or 1)</li>
<li>Both assume spherical clusters with equal variance</li>
<li>K-Means is a special case of Gaussian Mixture Models</li>
</ul>
<p><strong>Limitations of K-Means</strong>:</p>
<ul>
<li><strong>Assumes spherical clusters</strong>: Can&#39;t find elongated or irregular shapes</li>
<li><strong>Assumes equal-sized clusters</strong>: Tends to split large clusters</li>
<li><strong>Sensitive to outliers</strong>: Means are pulled by extreme values</li>
<li><strong>Requires specifying $K$</strong>: Must know number of clusters beforehand</li>
<li><strong>Non-convex clusters</strong>: Fails on clusters with complex shapes</li>
<li><strong>Euclidean distance</strong>: May not be appropriate for all data types</li>
</ul>
<p><strong>Interpreting Results</strong>:</p>
<ul>
<li><strong>Cluster centers</strong>: &quot;Prototypical&quot; members, useful for understanding what each cluster represents</li>
<li><strong>Cluster sizes</strong>: Imbalanced sizes might indicate poor clustering or natural structure</li>
<li><strong>Within-cluster variance</strong>: Measures homogeneity</li>
<li><strong>Between-cluster variance</strong>: Measures separation</li>
</ul>
<h2 id="spectral-clustering">Spectral Clustering</h2>
<p><strong>The Idea</strong>: Use the eigenvalues (spectrum) of a similarity graph to find clusters. Works when K-Means fails on non-convex shapes.</p>
<p><strong>Graph Perspective</strong>:</p>
<ul>
<li>Data points are nodes</li>
<li>Edges connect similar points (weighted by similarity)</li>
<li>Goal: Find a partition that minimizes edges cut between groups</li>
</ul>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>Build similarity graph from data (e.g., k-nearest neighbors or Gaussian similarity)</li>
<li>Compute the Graph Laplacian: $L = D - A$<ul>
<li>$D$ = degree matrix (diagonal, $D_{ii}$ = sum of edges from node $i$)</li>
<li>$A$ = adjacency matrix (edge weights)</li>
</ul>
</li>
<li>Find eigenvectors of $L$ corresponding to smallest eigenvalues</li>
<li>Use these eigenvectors as new features, run K-Means</li>
</ol>
<p><strong>Why the Laplacian?</strong>:</p>
<ul>
<li>Smallest eigenvalue is always 0 (corresponding to all-ones vector)</li>
<li>Second smallest eigenvalue (Fiedler value) indicates best cut</li>
<li>For $K$ clusters, use the $K$ smallest eigenvectors</li>
</ul>
<p><strong>When to Use</strong>:</p>
<ul>
<li>Non-convex cluster shapes (crescents, rings)</li>
<li>When you have a natural similarity/distance matrix</li>
<li>Graph-structured data</li>
</ul>
<h2 id="dbscan">DBSCAN</h2>
<p><strong>Density-Based Spatial Clustering of Applications with Noise</strong></p>
<p><strong>The Idea</strong>: Clusters are dense regions separated by sparse regions. Points in sparse regions are &quot;noise.&quot;</p>
<p><strong>Key Concepts</strong>:</p>
<ul>
<li><strong>Core Point</strong>: Has at least <code>minPts</code> points within radius $\epsilon$</li>
<li><strong>Border Point</strong>: Within $\epsilon$ of a core point, but not itself core</li>
<li><strong>Noise Point</strong>: Neither core nor border</li>
</ul>
<p><strong>Reachability</strong>:</p>
<ul>
<li><strong>Direct Density Reachable</strong>: Point $q$ is within $\epsilon$ of core point $p$</li>
<li><strong>Density Reachable</strong>: There&#39;s a chain of core points connecting $p$ to $q$</li>
<li><strong>Density Connected</strong>: Both $p$ and $q$ are density reachable from some core point</li>
</ul>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>For each point, determine if it&#39;s a core point</li>
<li>Create clusters by connecting core points that are within $\epsilon$ of each other</li>
<li>Assign border points to nearby clusters</li>
<li>Mark remaining points as noise</li>
</ol>
<p><strong>Parameters</strong>:</p>
<ul>
<li>$\epsilon$ (epsilon): Neighborhood radius</li>
<li><code>minPts</code>: Minimum points to form dense region</li>
</ul>
<p><strong>Choosing Parameters</strong>:</p>
<ul>
<li><code>minPts</code>: Often set to $\text{dimensionality} + 1$ or higher</li>
<li>$\epsilon$: Plot k-distance graph (distance to k-th nearest neighbor), look for elbow</li>
</ul>
<p><strong>Advantages</strong>:</p>
<ul>
<li>No need to specify number of clusters</li>
<li>Finds arbitrarily shaped clusters</li>
<li>Robust to outliers (identifies them as noise)</li>
<li>Only two intuitive parameters</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Struggles with clusters of varying density</li>
<li>Parameter selection can be tricky</li>
<li>Doesn&#39;t work well in high dimensions (curse of dimensionality)</li>
<li>Can&#39;t handle clusters that are close together</li>
</ul>
<p><strong>Extensions</strong>:</p>
<ul>
<li><p><strong>OPTICS</strong> (Ordering Points To Identify Clustering Structure):</p>
<ul>
<li>Creates an ordering of points based on density</li>
<li>Can extract clusters at different density levels</li>
<li>More robust to parameter choices</li>
</ul>
</li>
<li><p><strong>HDBSCAN</strong> (Hierarchical DBSCAN):</p>
<ul>
<li>Automatically finds clusters of varying densities</li>
<li>Combines benefits of DBSCAN and hierarchical clustering</li>
<li>More robust, fewer parameters to tune</li>
<li>Often the best choice in practice</li>
</ul>
</li>
</ul>
<h2 id="choosing-a-clustering-algorithm">Choosing a Clustering Algorithm</h2>
<p><strong>Decision Guide</strong>:</p>
<table>
<thead>
<tr>
<th>Situation</th>
<th>Recommended Algorithm</th>
</tr>
</thead>
<tbody><tr>
<td>Know number of clusters, spherical shapes</td>
<td>K-Means</td>
</tr>
<tr>
<td>Don&#39;t know number, want hierarchy</td>
<td>Hierarchical Agglomerative</td>
</tr>
<tr>
<td>Arbitrary shapes, want to identify noise</td>
<td>DBSCAN or HDBSCAN</td>
</tr>
<tr>
<td>Non-convex shapes, know number</td>
<td>Spectral Clustering</td>
</tr>
<tr>
<td>Large dataset</td>
<td>K-Means or Mini-batch K-Means</td>
</tr>
<tr>
<td>Want to visualize cluster relationships</td>
<td>Hierarchical with dendrogram</td>
</tr>
</tbody></table>
<p><strong>Validation Approaches</strong>:</p>
<ul>
<li><strong>Internal metrics</strong> (no ground truth): Silhouette score, Davies-Bouldin index, Calinski-Harabasz index</li>
<li><strong>External metrics</strong> (with ground truth): Adjusted Rand Index, Normalized Mutual Information</li>
<li><strong>Stability</strong>: Do clusters persist with data perturbations?</li>
</ul>
<p><strong>Remember</strong>: Clustering is exploratory‚Äîthere&#39;s often no &quot;right&quot; answer. Different algorithms reveal different structure. The best choice depends on your data and goals.</p>

        </article>
        <nav class="page-navigation">
        <a href="gen-04-xgboost.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">XGBoost</span>
        </a>
        <a href="gen-06-support_vector_machines.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Support Vector Machines</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>