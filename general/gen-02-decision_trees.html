<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Decision Trees | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="gen-02-decision_trees.html" class="active">Decision Trees</a></li>
            <li class="nav-item"><a href="gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../probml/probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="../probml/probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="../probml/probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="../probml/probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="../probml/probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="../probml/probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="../probml/probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="../probml/probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="../probml/probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="../probml/probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">General</a>
          </div>
          <h1 class="page-title">Decision Trees</h1>
          <div class="page-meta"><span class="tag">General</span></div>
        </header>
        <article class="content">
          <h1 id="decision-trees">Decision Trees</h1>
<p>Decision trees are among the most intuitive machine learning algorithms. They make predictions by asking a series of yes/no questions about the features, essentially building a flowchart for decision-making. This mirrors how humans often make decisions‚Äîthrough a sequence of simple questions.</p>
<h2 id="decision-trees-1">Decision Trees</h2>
<p><strong>The Core Idea</strong>: Recursively split the input/feature space using simple rules (called &quot;stubs&quot;). Each split divides the data into more homogeneous groups.</p>
<p><strong>Key Characteristics</strong>:</p>
<ul>
<li>Splits are always parallel to the feature axes (like drawing vertical or horizontal lines)</li>
<li>Each path from root to leaf represents a conjunction of conditions</li>
<li>The tree structure naturally captures feature interactions</li>
</ul>
<p><strong>Mathematical Representation</strong>:</p>
<ul>
<li>Each leaf defines a region: $R_j = {x : d_1 \leq t_1, d_2 \geq t_2, ...}$</li>
<li>Prediction for a point: $\hat{Y}_i = \sum_j w_j I{x_i \in R_j}$</li>
<li>Leaf weights (for regression): $w_j = \frac{\sum_i y_i I{x_i \in R_j}}{\sum_i I{x_i \in R_j}}$ (just the average of y values in that leaf)</li>
</ul>
<p><strong>Types of Decision Trees</strong>:</p>
<p><em>Binary Splits</em> (most common):</p>
<ul>
<li><strong>CART</strong> (Classification and Regression Trees): The most widely used, always binary splits</li>
<li><strong>C4.5</strong>: Successor to ID3, handles continuous attributes, missing values</li>
</ul>
<p><em>Multi-way Splits</em>:</p>
<ul>
<li><strong>CHAID</strong> (Chi-Square Automatic Interaction Detection): Uses statistical tests for splits</li>
<li><strong>ID3</strong>: Original algorithm, only handles categorical features</li>
</ul>
<h2 id="splitting">Splitting</h2>
<p>The key question: How do we decide which feature to split on and where?</p>
<p><strong>For Classification Trees‚ÄîMeasuring Impurity</strong>:</p>
<p>We want each split to create more &quot;pure&quot; nodes (nodes dominated by one class).</p>
<p><strong>Gini Impurity</strong>:</p>
<ul>
<li>$\text{Gini} = 1 - \sum_C p_i^2$</li>
<li>Intuition: The probability of misclassifying a randomly chosen element if we labeled it randomly according to the class distribution in the node</li>
<li>For a given class $i$: Probability of picking class $i$ and misclassifying it = $p_i \times (1 - p_i)$</li>
<li>Sum across all classes: $\sum_C p_i(1 - p_i) = 1 - \sum_C p_i^2$</li>
<li>Range: 0 (pure node, all one class) to $(K-1)/K$ for K classes (max 0.5 for binary)</li>
<li>Example: If a node has 50% class A and 50% class B, Gini = $1 - (0.5^2 + 0.5^2) = 0.5$</li>
</ul>
<p><strong>Entropy Criterion</strong>:</p>
<ul>
<li>Based on information theory‚Äîmeasures uncertainty</li>
<li>If event E is very likely ($P(E) \approx 1$): No surprise when it happens</li>
<li>If event E is unlikely ($P(E) \approx 0$): Huge surprise when it happens</li>
<li>Information content: $I(E) = \log(1/P(E)) = -\log(P(E))$</li>
<li>Entropy = expected information content: $H(E) = -\sum P(E) \log P(E)$</li>
<li>Range: 0 (pure) to $\log_2(K)$ for K classes (max 1 for binary)</li>
<li>Maximum entropy when all outcomes equally likely</li>
</ul>
<p><strong>For Regression Trees‚ÄîMeasuring Error</strong>:</p>
<ul>
<li><strong>Sum of Squared Errors (SSE)</strong>: $\sum_i (Y_i - \bar{Y})^2$</li>
<li>This is just the variance times $N$ within the node</li>
<li>We want splits that minimize the total SSE across child nodes</li>
</ul>
<p><strong>Finding the Best Split</strong>:</p>
<p>For each candidate split:</p>
<ol>
<li>Calculate the weighted average reduction in impurity/error</li>
<li>Weights = number of observations flowing to each child node</li>
</ol>
<p><strong>Example of Gini Reduction</strong>:</p>
<ul>
<li>Starting Gini at root: $\text{Gini}<em>{\text{Root}}$ with $N</em>{\text{Root}}$ samples</li>
<li>After split into Left and Right:<ul>
<li>$\text{Gini}<em>{\text{New}} = \frac{N</em>{\text{Left}}}{N_{\text{Root}}} \times \text{Gini}<em>{\text{Left}} + \frac{N</em>{\text{Right}}}{N_{\text{Root}}} \times \text{Gini}_{\text{Right}}$</li>
</ul>
</li>
<li>Choose the split that minimizes $\text{Gini}_{\text{New}}$</li>
<li>Note: $\text{Gini}<em>{\text{New}} \leq \text{Gini}</em>{\text{Root}}$ always (splits never increase impurity)</li>
</ul>
<p><strong>The Algorithm</strong>: Greedy search through all features and all possible split points to find the best split at each node.</p>
<h2 id="bias-variance-trade-off">Bias-Variance Trade-off</h2>
<p>Understanding this trade-off is essential for all of machine learning.</p>
<p><strong>Bias</strong>:</p>
<ul>
<li>Measures how well the algorithm can model the true relationship</li>
<li>High bias = making strong/restrictive assumptions<ul>
<li>Example: Using a linear model for a parabolic relationship</li>
</ul>
</li>
<li>Low bias = fewer assumptions, more flexible</li>
</ul>
<p><strong>Variance</strong>:</p>
<ul>
<li>Measures how much the model changes across different training datasets</li>
<li>High variance = model is very sensitive to the specific training data</li>
<li>Low variance = model is stable across different samples</li>
</ul>
<p><strong>Irreducible Error</strong> (Bayes Error):</p>
<ul>
<li>The inherent noise in the data</li>
<li>Cannot be reduced no matter how good the model</li>
</ul>
<p><strong>The Trade-off</strong>:</p>
<ul>
<li>$\text{Expected Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}$</li>
<li>Simple models: High bias, low variance (underfit)</li>
<li>Complex models: Low bias, high variance (overfit)</li>
<li>Goal: Find the sweet spot that minimizes total error</li>
</ul>
<p><strong>Decision Trees and the Trade-off</strong>:</p>
<ul>
<li>Deep trees have <strong>low bias</strong> (can fit complex patterns)</li>
<li>But <strong>high variance</strong> (very sensitive to training data)</li>
<li>This makes them prone to <strong>overfitting</strong>, especially with:<ul>
<li>Noisy samples</li>
<li>Small data samples in deep nodes</li>
</ul>
</li>
</ul>
<p><strong>Tree Pruning</strong> addresses overfitting by adding a complexity penalty:</p>
<ul>
<li>Objective: $\text{Tree Score} = \text{SSR} + \alpha T$</li>
<li>Where $T$ = number of leaves, $\alpha$ = complexity parameter</li>
<li>As the tree grows, SSR reduction must offset the complexity cost</li>
</ul>
<h2 id="nature-of-decision-trees">Nature of Decision Trees</h2>
<p><strong>Strengths</strong>:</p>
<p><em>Non-linear Relationships</em>:</p>
<ul>
<li>Decision trees naturally model complex, non-linear decision boundaries</li>
<li>Unlike splines, which add indicator variables but require continuous boundaries</li>
<li>Trees can create completely discontinuous predictions</li>
</ul>
<p><em>No Feature Scaling Required</em>:</p>
<ul>
<li>Tree algorithms only care about the ordering of values, not their magnitude</li>
<li>No need to normalize or standardize features</li>
</ul>
<p><em>Robustness to Outliers</em>:</p>
<ul>
<li>For input feature outliers: Splits simply ignore extreme values</li>
<li>For output outliers (in regression): Some impact, but less than linear regression</li>
<li>Note: High-leverage points in regression can have extreme influence; trees are more robust</li>
</ul>
<p><strong>Weaknesses</strong>:</p>
<p><em>Extrapolation</em>:</p>
<ul>
<li>Trees cannot extrapolate beyond the range of training data</li>
<li>For values outside training range, prediction = nearest leaf&#39;s value</li>
<li>Linear models can extrapolate (for better or worse)</li>
</ul>
<p><em>Time Series</em>:</p>
<ul>
<li>Trees cannot capture linear trends or seasonality naturally</li>
<li>Each leaf is a constant prediction‚Äîno notion of &quot;trend&quot;</li>
</ul>
<h2 id="bagging">Bagging</h2>
<p><strong>Bootstrap Aggregation</strong> (Bagging) is a technique to reduce variance.</p>
<p><strong>The Bootstrap</strong>:</p>
<ul>
<li>Given a dataset of size $N$</li>
<li>Create a new dataset by sampling $N$ points <em>with replacement</em></li>
<li>Probability that point $i$ is never selected: $(1 - \frac{1}{N})^N \approx \frac{1}{e} \approx 0.37$</li>
<li>So each bootstrap sample contains ~63% of unique original points</li>
</ul>
<p><strong>Bagging for Trees</strong>:</p>
<ol>
<li>Create many bootstrap samples</li>
<li>Fit a tree to each</li>
<li>Average predictions (regression) or vote (classification)</li>
</ol>
<p><strong>Why It Works</strong>:</p>
<ul>
<li>Individual trees are unstable (high variance)</li>
<li>Averaging independent predictions reduces variance</li>
<li>If predictions were perfectly independent: Variance would decrease by factor of $n$</li>
</ul>
<h2 id="random-forest">Random Forest</h2>
<p>Random Forest = Bagging + Random Feature Selection</p>
<p><strong>The Algorithm</strong>:</p>
<ol>
<li>Create bootstrap samples (the &quot;random&quot; part of the data)</li>
<li>At each split, consider only a random subset of features:<ul>
<li>Classification: typically $\sqrt{p}$ features</li>
<li>Regression: typically $p/3$ features</li>
</ul>
</li>
<li>Combine predictions: majority vote (classification) or average (regression)</li>
</ol>
<p><strong>Why Random Feature Selection?</strong>:</p>
<ul>
<li>Without it, all trees would be very similar (correlated)</li>
<li>If one strong feature dominates, all trees split on it first</li>
<li>Random selection decorrelates the trees</li>
</ul>
<p><strong>Variance Reduction Math</strong>:</p>
<ul>
<li>Let $\hat{y}_i$ be prediction from tree $i$, with variance $\sigma^2$</li>
<li>Let $\rho$ be the correlation between trees</li>
<li>Variance of average: $V\left(\frac{1}{n}\sum_i \hat{y}_i\right) = \rho\sigma^2 + \frac{1-\rho}{n}\sigma^2$</li>
<li>As $n \to \infty$: Variance approaches $\rho\sigma^2$</li>
<li>Lower correlation $\rho$ ‚Üí lower variance ‚Üí better!</li>
</ul>
<p><strong>Bias vs Variance</strong>:</p>
<ul>
<li>Bias remains the same as a single tree (no improvement)</li>
<li>Variance decreases with more trees</li>
<li>This is why random forests are so powerful: low bias AND low variance</li>
</ul>
<p><strong>Out-of-Bag (OOB) Error</strong>:</p>
<ul>
<li>~37% of data not used in each tree (OOB samples)</li>
<li>Use these to estimate test error‚Äîlike free cross-validation!</li>
<li>For each point, average predictions from trees that didn&#39;t train on it</li>
<li>OOB error typically close to leave-one-out cross-validation error</li>
</ul>
<p><strong>Proximity Matrix</strong>:</p>
<ul>
<li>For OOB observations, count how often each pair lands in the same leaf</li>
<li>Creates a similarity measure between observations</li>
<li>Useful for clustering, visualization, missing value imputation</li>
</ul>
<h2 id="extratrees">ExtraTrees</h2>
<p><strong>Extremely Randomized Trees</strong>‚Äîtaking randomization even further.</p>
<p><strong>Key Differences from Random Forest</strong>:</p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Random Forest</th>
<th>ExtraTrees</th>
</tr>
</thead>
<tbody><tr>
<td>Data sampling</td>
<td>Bootstrap (63%)</td>
<td>Entire dataset (100%)</td>
</tr>
<tr>
<td>Split thresholds</td>
<td>Optimized search</td>
<td>Randomly selected</td>
</tr>
<tr>
<td>Feature selection</td>
<td>Random subset</td>
<td>Random subset</td>
</tr>
</tbody></table>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>Use the full training set (no bootstrapping)</li>
<li>At each node, for each candidate feature:<ul>
<li>Select a random threshold uniformly between min and max</li>
<li>Evaluate the split</li>
</ul>
</li>
<li>Choose the best feature-threshold combination</li>
</ol>
<p><strong>Trade-off</strong>:</p>
<ul>
<li>Even more randomness ‚Üí even lower variance</li>
<li>But slightly higher bias than Random Forest</li>
<li>Much faster training (no optimization of thresholds)</li>
</ul>
<h2 id="variable-importance">Variable Importance</h2>
<p>Understanding which features matter is often as important as making predictions.</p>
<p><strong>Split-Based Importance</strong> (built into tree algorithms):</p>
<ul>
<li>For each feature $j$, sum the Gini/entropy reduction across all splits using $j$</li>
<li>Alternative: Count the number of times feature is used for splitting</li>
<li><strong>Limitation</strong>: Biased toward continuous features (more possible split points)</li>
<li><strong>Limitation</strong>: Biased toward high-cardinality categorical features</li>
</ul>
<p><strong>Permutation-Based Importance</strong> (model-agnostic):</p>
<ol>
<li>Calculate baseline accuracy on OOB samples</li>
<li>For each feature $j$:<ul>
<li>Randomly shuffle feature $j$&#39;s values (breaks its relationship with target)</li>
<li>Calculate new accuracy</li>
<li>Importance = decrease in accuracy</li>
</ul>
</li>
<li>Average across all trees</li>
</ol>
<p><strong>Why Permutation Importance is Better</strong>:</p>
<ul>
<li>Measures actual predictive value, not just usage</li>
<li>Accounts for redundancy: If feature $j$ has a good surrogate, permuting $j$ won&#39;t hurt much</li>
<li>Like setting the coefficient to 0 in regression</li>
</ul>
<p><strong>Partial Dependence Plots (PDPs)</strong>:</p>
<ul>
<li>Show the marginal effect of a feature on predictions</li>
<li>Algorithm: For each value $x_s$ of feature $s$:<ul>
<li>Set all observations to have $x_s$ for that feature</li>
<li>Average the predictions: $\hat{f}(x_s) = \frac{1}{N}\sum_i f(x_s, x_{i,-s})$</li>
</ul>
</li>
<li><strong>Assumption</strong>: Features are not correlated (can be misleading otherwise)</li>
<li>Can identify interactions using Friedman&#39;s H-statistic</li>
</ul>
<p><strong>Other Importance Methods</strong>:</p>
<ul>
<li><strong>SHAP (Shapley Values)</strong>: Game-theoretic approach, model-agnostic, handles interactions</li>
<li><strong>LIME</strong>: Local interpretable model-agnostic explanations‚Äîexplains individual predictions</li>
</ul>
<h2 id="handling-categorical-variables">Handling Categorical Variables</h2>
<p><strong>Binary Categorical</strong>: Easy‚Äîjust a yes/no split</p>
<p><strong>Multi-Category Variables‚ÄîOptions</strong>:</p>
<p><em>One-Hot Encoding</em>:</p>
<ul>
<li>Create a binary feature for each category</li>
<li>Pro: Simple, works with any algorithm</li>
<li>Con: Increases dimensionality; for trees, biases toward these features</li>
</ul>
<p><em>Label Encoding</em>:</p>
<ul>
<li>Assign ordinal numbers (1, 2, 3, ...)</li>
<li>Pro: No dimensionality increase</li>
<li>Con: Imposes an artificial ordering</li>
</ul>
<p><em>Native Handling</em> (in tree algorithms):</p>
<ul>
<li>Consider all possible subsets of categories for binary splits</li>
<li>CART: Finds optimal binary grouping</li>
<li>C4.5, CHAID: Can create multi-way splits (one branch per category)</li>
<li>Pro: Optimal splits; Con: Exponential search space</li>
</ul>
<h2 id="tree-pruning">Tree Pruning</h2>
<p>Pruning prevents overfitting by limiting tree complexity.</p>
<p><strong>Pre-Pruning</strong> (Early Stopping):</p>
<ul>
<li>Stop growing before the tree is fully expanded</li>
<li>Criteria:<ul>
<li>Maximum depth</li>
<li>Minimum samples per leaf</li>
<li>Minimum impurity decrease</li>
<li>Maximum leaf nodes</li>
</ul>
</li>
<li>Pro: Fast, simple</li>
<li>Con: Might stop too early (a bad split might enable good later splits)</li>
</ul>
<p><strong>Post-Pruning</strong> (Grow then Prune):</p>
<ul>
<li>Grow a full tree, then remove unhelpful branches</li>
<li>Methods:<ul>
<li><strong>Cost-Complexity Pruning</strong> (CART): Minimize $\text{SSE} + \alpha \times (\text{number of leaves})$</li>
<li><strong>Reduced Error Pruning (REP)</strong>: Remove nodes that don&#39;t improve validation error</li>
<li><strong>Pessimistic Error Pruning (PEP)</strong>: Use statistical adjustments on training error</li>
</ul>
</li>
<li>Pro: Considers the full tree structure</li>
<li>Con: More computationally expensive</li>
</ul>
<p><strong>Selecting the Pruning Level</strong>:</p>
<ul>
<li>Use cross-validation to find optimal $\alpha$ (complexity parameter)</li>
<li>Plot training/validation error vs. $\alpha$ to visualize the trade-off</li>
<li>The &quot;right&quot; amount of pruning balances underfitting and overfitting</li>
</ul>

        </article>
        <nav class="page-navigation">
        <a href="gen-01-basic-statistics.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Basic Statistics</span>
        </a>
        <a href="gen-03-boosting.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Boosting</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>