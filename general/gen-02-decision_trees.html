<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Decision Trees | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="gen-02-decision_trees.html" class="active">Decision Trees</a></li>
            <li class="nav-item"><a href="gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">SLP Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regex</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">Tokenization</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vectors</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../probml/probml-00.html">ProbML Notes</a></li>
            <li class="nav-item"><a href="../probml/probml-01-introduction.html">Introduction</a></li>
            <li class="nav-item"><a href="../probml/probml-02-probability.html">Probability</a></li>
            <li class="nav-item"><a href="../probml/probml-03-probability.html">Probability (Advanced Topics)</a></li>
            <li class="nav-item"><a href="../probml/probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="../probml/probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="../probml/probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="../probml/probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-13-ffnn.html">Feed Forward Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-14-cnn.html">Convolution NN</a></li>
            <li class="nav-item"><a href="../probml/probml-15-rnn.html">Recurrent Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-16-exemplar.html">Exemplar Methods</a></li>
            <li class="nav-item"><a href="../probml/probml-18-trees.html">Trees</a></li>
            <li class="nav-item"><a href="../probml/probml-19-ssl.html">SSL</a></li>
            <li class="nav-item"><a href="../probml/probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">General</a>
          </div>
          <h1 class="page-title">Decision Trees</h1>
          <div class="page-meta"><span class="tag">General</span></div>
        </header>
        <article class="content">
          <h1 id="decision-trees">Decision Trees</h1>
<h2 id="decision-trees-1">Decision Trees</h2>
<ul>
<li>Recursively split the input / feature space using stubs i.e. decision rules<ul>
<li>Splits are parallel to the axis</li>
</ul>
</li>
<li>Mathematical Representation<ul>
<li>$R_j = { x : d_1 &lt;= t_1, d_2 &gt;= t_2 ... }$\</li>
<li>$\hat Y_i = \sum_j w_j I{x_i \in R_j}$</li>
<li>$w_j = \frac{\sum_i y_i I {x_i \in R_j}}{\sum_i I {x_i \in R_j}}$</li>
</ul>
</li>
<li>Types of Decision Trees<ul>
<li>Binary Splits<ul>
<li>Classification and Regression Trees (CART)</li>
<li>C4.5</li>
</ul>
</li>
<li>Multiple Splits:<ul>
<li>CHAID (Chi-Square Automatic Interaction Detection)</li>
<li>ID3</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="splitting">Splitting</h2>
<ul>
<li>Split Criteria for Classification Trees<ul>
<li>The nodes are split to decrease impurity in classification</li>
<li>Gini Criterion<ul>
<li>$1 - \sum_C p_{i}^2$</li>
<li>Probability that observation belongs to class i: $p_i$</li>
<li>Misclassification:</li>
<li>For a given class (say i):<ul>
<li>$p_i \times p_{k \ne i} = p_i \times (1 - p_i)$</li>
</ul>
</li>
<li>Across all classes:</li>
<li>$\sum_C p_i \times (1 - p_i)$</li>
<li>$\sum_C p_i - \sum_C p_{i}^2$</li>
<li>$1 - \sum_C p_{i}^2$</li>
<li>Ranges from (0, 0.5)</li>
</ul>
</li>
<li>Entropy Criterion<ul>
<li>Measure of uncertainty of a random variable</li>
<li>Given an event E<ul>
<li>p(E) = 1 $\implies$ No Surprise</li>
<li>p(E) = 0 $\implies$ Huge Surprise</li>
<li>Informaion Content: $I(E) = \log(1 / p(E))$</li>
</ul>
</li>
<li>Entropy is the expectation of this information content<ul>
<li>$H(E) = - \sum p(E) \log(p(E))$</li>
<li>Maximum when all outcomes have same probability of occurrence</li>
</ul>
</li>
<li>Ranges from (0, 1)</li>
</ul>
</li>
</ul>
</li>
<li>Split Criteria for Regression Trees<ul>
<li>Sum-Squared Error</li>
<li>$\sum_i (Y_i - \bar Y)^2$</li>
</ul>
</li>
<li>Finding the Split<ul>
<li>For any candidate value:<ul>
<li>Calculate the weighted average reduction in impurity / error</li>
<li>Weights being the number of observations flowing in the child nodes</li>
</ul>
</li>
<li>Starting Gini<ul>
<li>$\text{Gini}_{\text{Root}}$</li>
<li>$N_{\text{Root}}$</li>
</ul>
</li>
<li>After Split<ul>
<li>Child Nodes<ul>
<li>$\text{Gini}<em>{\text{Left}}, N</em>{\text{Left}}$</li>
<li>$\text{Gini}<em>{\text{Right}}, N</em>{\text{Right}}$</li>
</ul>
</li>
<li>Updated Gini<ul>
<li>$\frac{N_{\text{Left}}}{N_{\text{Root}}} \times \text{Gini}<em>{\text{Left}} + \frac{N</em>{\text{Right}}}{N_{\text{Root}}} \times \text{Gini}_{\text{Right}}$</li>
</ul>
</li>
</ul>
</li>
<li>Find the split, the results in minimum updated Gini</li>
<li>Updated Gini &lt;= Starting Gini</li>
<li>Greedy algorithms to find the best splits</li>
</ul>
</li>
</ul>
<h2 id="bias-variance-trade-off">Bias-Variance Trade-off</h2>
<ul>
<li>Bias<ul>
<li>Measures ability of an ML algorithm to model true relationship between features and target</li>
<li>Simplifying assumptions made by the model to learn the relationship<ul>
<li>Example: Linear vs Parabolic relationship</li>
</ul>
</li>
<li>Low Bias: Less restrictive assumptions</li>
<li>High Bias: More restrictive assumptions</li>
</ul>
</li>
<li>Variance<ul>
<li>The difference in model performance across different datasets drawn from the same distribution</li>
<li>Low Variance: Small changes to model performance with changes in datasets</li>
<li>High Variance: Large changes to model performance with changes in datasets</li>
</ul>
</li>
<li>Irreducible Error<ul>
<li>Bayes error</li>
<li>Cannot be reduced irrespective of the model form</li>
</ul>
</li>
<li>Best model minimizes: $\text{MSE} = \text{bias}^2 + \text{variance}$</li>
<li>Decision trees have low bias and high variance</li>
<li>Decision trees are prone to overfitting<ul>
<li>Noisy Samples</li>
<li>Small data samples in nodes down the tree</li>
<li>Tree Pruning solves for overfitting<ul>
<li>Adding a cost term to objective which captures tree complexity</li>
<li>$\text{Tree Score} = SSR + \alpha T$</li>
<li>As the tree grows in size, the reduction in SSR has to more than offset the complexity cost</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="nature-of-decision-trees">Nature of Decision Trees</h2>
<ul>
<li>Decision Trees can model non-linear relationships (complex decision boundaries)</li>
<li>Spline regressions cannot achieve the same results<ul>
<li>Spline adds indicator variables to capture interactions and create kinks</li>
<li>But the decision boundary has to be continuous</li>
<li>The same restriction doesn&#39;t apply to decision trees</li>
</ul>
</li>
<li>Decision Trees don&#39;t require feature scaling</li>
<li>Decision Trees are less sensitive to outliers<ul>
<li>Outliers are of various kinds:<ul>
<li>Outliers: Points with extreme values<ul>
<li>Input Features<ul>
<li>Doesn&#39;t impact Decision Trees</li>
<li>Split finding will ignore the extreme values</li>
</ul>
</li>
<li>Output / Target</li>
</ul>
</li>
<li>Influential / High-Leverage Points: Undue influence on model</li>
</ul>
</li>
</ul>
</li>
<li>Decision Trees cannot extrapolate well to ranges outside the training data</li>
<li>Decision trees cannot capture linear time series based trends / seasonality</li>
</ul>
<h2 id="bagging">Bagging</h2>
<ul>
<li>Bootstrap Agrregation</li>
<li>Sampling with repetition<ul>
<li>Given Dataset of Size N</li>
<li>Draw N samples with replacement</li>
<li>Probability that a point (say i) never gets selected<ul>
<li>$(1 - \frac{1}{N})^N \approx \frac{1}{e}$</li>
</ul>
</li>
<li>Probability that a point (say i) gets selected atleast once<ul>
<li>$1 - \frac{1}{e} \approx 63%$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="random-forest">Random Forest</h2>
<ul>
<li>Use bootstrap aggregation (bagging) to create multiple datasets<ul>
<li>&quot;Random&quot; subspace of dataset</li>
</ul>
</li>
<li>Use subset of variables for split at each node<ul>
<li>sqrt for classification</li>
<li>m//3 for regression</li>
</ul>
</li>
<li>Comparison to single decision tree<ul>
<li>Bias remains the same</li>
<li>Variance decreases</li>
<li>Randomness in data and splits reduces the correlation in prediction across trees</li>
<li>Let $\hat y_i$ be the prediction from ith tree in the forest</li>
<li>Let $\sigma^2$ be the variance of $\hat y_i$</li>
<li>Let $\rho$ be the correlation between two trees in the forest</li>
<li>$V(\sum_i \hat y_i) = \sum V(\hat y_i) + 2 \sum\sum COV(\hat y_i, \hat y_j)$</li>
<li>$V(\sum_i \hat y_i) = n \sigma^2 + n(n-1) \rho \sigma^2$</li>
<li>$V( \frac{1}{n} \sum_i \hat y_i) = \rho \sigma^2 + \frac{1-\rho}{n} \sigma^2$</li>
<li>Variance goes down as more trees are added, but bias stays put</li>
</ul>
</li>
<li>Output Combination<ul>
<li>Majority Voting for Classification</li>
<li>Averaging for Regression</li>
</ul>
</li>
<li>Out-of-bag (OOB) Error<ul>
<li>Use the non-selected rows in bagging to estimate model performance</li>
<li>Comparable to cross-validation results</li>
</ul>
</li>
<li>Proximity Matrix<ul>
<li>Use OOB observations</li>
<li>Count the number of times each pair goes to the same terminal node</li>
<li>Identifies observations that are close/similar to each other</li>
</ul>
</li>
</ul>
<h2 id="extratrees">ExtraTrees</h2>
<ul>
<li>Extremely Randomized Trees</li>
<li>Bagging:<ul>
<li>ExtraTrees: No</li>
<li>Extremely Randomized Trees: Yes</li>
</ul>
</li>
<li>Mutiple trees are built using:<ul>
<li>Random variable subset for splitting</li>
<li>Random threshold subsets for a variable for splitting</li>
</ul>
</li>
</ul>
<h2 id="variable-importance">Variable Importance</h2>
<ul>
<li>Split-based importance<ul>
<li>If variable j is used for split<ul>
<li>Calculate the improvement in Gini at the split</li>
</ul>
</li>
<li>Sum this improvement across all trees and splits wherever jth variable is used</li>
<li>Alternate is to calculate the number of times variable is used for splitting</li>
<li>Biased in favour of continuous variables which can be split multiple times</li>
</ul>
</li>
<li>Permutation-based importance / Boruta<ul>
<li>Use OOB samples to calculate variable importance</li>
<li>Take bth tree:<ul>
<li>Pass the OOB samples and calculate accuracy</li>
<li>Permute jth variable and calculate the decrease in accuracy</li>
</ul>
</li>
<li>Average this decrease in accuracy across all trees to calculate variable importance for j</li>
<li>Effect is similar to setting the coefficient to 0 in regression</li>
<li>Takes into account if good surrogates are present in the dataset</li>
</ul>
</li>
<li>Partial Dependence Plots<ul>
<li>Marginal effect of of a feature on target</li>
<li>Understand the relationship between feature and target</li>
<li>Assumes features are not correlated</li>
<li>$\hat f(x_s) =\frac{1}{C} \sum f(x_s,x_i)$</li>
<li>Average predictions over all other variables</li>
<li>Can be used to identify important interactions<ul>
<li>Friedman&#39;s H Statistic</li>
<li>If features don&#39;t interact Joint PDP can be decomposed into marginals</li>
</ul>
</li>
</ul>
</li>
<li>Shapley Values<ul>
<li>Model agnostic feature importance</li>
</ul>
</li>
<li>LIME</li>
</ul>
<h2 id="handling-categorical-variables">Handling Categorical Variables</h2>
<ul>
<li>Binary categorical variables are easily incorporated into decision trees</li>
<li>For multi-category variables:<ul>
<li>One-hot encoding (creates a binary feature for each category)</li>
<li>Label encoding (assigns an ordinal value to each category)</li>
</ul>
</li>
<li>Trees can directly handle categorical variables by considering all possible subsets for splitting<ul>
<li>CART typically uses binary splits (creates a binary question from categorical features)</li>
<li>C4.5 and CHAID can create multi-way splits</li>
</ul>
</li>
</ul>
<h2 id="tree-pruning">Tree Pruning</h2>
<ul>
<li>Decision trees are prone to overfitting<ul>
<li>Noisy Samples</li>
<li>Small data samples in nodes down the tree</li>
<li>Tree Pruning solves for overfitting<ul>
<li>Adding a cost term to objective which captures tree complexity</li>
<li>$\text{Tree Score} = SSR + \alpha T$</li>
<li>As the tree grows in size, the reduction in SSR has to more than offset the complexity cost</li>
</ul>
</li>
</ul>
</li>
<li>Pre-pruning vs. Post-pruning:<ul>
<li>Pre-pruning: Stops tree growth early using criteria like:<ul>
<li>Minimum samples per leaf</li>
<li>Maximum depth</li>
<li>Minimum impurity decrease</li>
</ul>
</li>
<li>Post-pruning: Grows a full tree and then removes branches that don&#39;t improve generalization<ul>
<li>Cost-complexity pruning (used in CART)</li>
<li>Reduced Error Pruning (REP)</li>
<li>Pessimistic Error Pruning (PEP)</li>
</ul>
</li>
</ul>
</li>
<li>Cross-validation can be used to determine optimal pruning level</li>
</ul>

        </article>
        <nav class="page-navigation">
        <a href="gen-01-basic-statistics.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Basic Statistics</span>
        </a>
        <a href="gen-03-boosting.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Boosting</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>