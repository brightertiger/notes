<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>XGBoost | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="gen-04-xgboost.html" class="active">XGBoost</a></li>
            <li class="nav-item"><a href="gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../probml/probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="../probml/probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="../probml/probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="../probml/probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="../probml/probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="../probml/probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="../probml/probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="../probml/probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="../probml/probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="../probml/probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">General</a>
          </div>
          <h1 class="page-title">XGBoost</h1>
          <div class="page-meta"><span class="tag">General</span></div>
        </header>
        <article class="content">
          <h1 id="xgboost">XGBoost</h1>
<p>XGBoost (eXtreme Gradient Boosting) is arguably the most successful machine learning algorithm for structured/tabular data. It&#39;s a highly optimized implementation of gradient boosting that has won countless Kaggle competitions. What makes it special? Regularization to prevent overfitting, computational tricks for speed, and smart handling of missing values.</p>
<h2 id="mathematical-details">Mathematical Details</h2>
<p><strong>The Objective Function</strong>:</p>
<p>XGBoost adds explicit regularization to the gradient boosting objective:</p>
<p>$$\text{Objective} = \sum_i L(y_i, \hat{y}<em>i) + \underbrace{\gamma T + \frac{1}{2}\lambda \sum</em>{j=1}^T w_j^2}_{\text{Regularization}}$$</p>
<p>Where:</p>
<ul>
<li>$L(y_i, \hat{y}_i)$ = Loss function (e.g., MSE, log-loss)</li>
<li>$T$ = Number of leaves in the tree</li>
<li>$w_j$ = Output value (weight) of leaf $j$</li>
<li>$\gamma$ = Penalty per leaf (controls tree complexity)</li>
<li>$\lambda$ = L2 regularization on leaf weights</li>
</ul>
<p><strong>Common Loss Functions</strong>:</p>
<ul>
<li><strong>MSE</strong> (Regression): $L = \frac{1}{2}(y_i - \hat{y}_i)^2$</li>
<li><strong>Log-loss</strong> (Classification): $L = -[y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]$</li>
</ul>
<p><strong>Prediction Update</strong>:
$$\hat{y}_i = \hat{y}<em>i^{(0)} + \sum</em>{j=1}^T w_j \cdot I(x_i \in R_j)$$</p>
<p>Where $\hat{y}_i^{(0)}$ is the prediction from previous rounds.</p>
<p><strong>Second-Order Taylor Approximation</strong>:</p>
<p>The key insight: Approximate the loss with a second-order Taylor expansion for efficient optimization.</p>
<p>For adding a new tree with output value $O$:
$$L(y_i, \hat{y}_i^{(0)} + O) \approx L(y_i, \hat{y}_i^{(0)}) + g_i \cdot O + \frac{1}{2}H_i \cdot O^2$$</p>
<p>Where:</p>
<ul>
<li>$g_i = \frac{\partial L}{\partial \hat{y}_i}$ (gradient, first derivative)</li>
<li>$H_i = \frac{\partial^2 L}{\partial \hat{y}_i^2}$ (Hessian, second derivative)</li>
</ul>
<p><strong>Why Second-Order?</strong>:</p>
<ul>
<li>First-order (like regular gradient boosting): Only tells direction</li>
<li>Second-order: Also tells curvature, enabling more accurate steps</li>
<li>Newton&#39;s method vs gradient descent‚Äîconverges faster!</li>
</ul>
<p><strong>Simplified Objective</strong>:</p>
<p>After substitution and dropping constants:
$$\text{Objective} = \sum_i g_i \cdot O + \gamma T + \frac{1}{2}\left(\sum_i H_i + \lambda\right) O^2$$</p>
<p><strong>Optimal Leaf Value</strong>:</p>
<p>Taking derivative with respect to $O$ and setting to zero:
$$O^* = -\frac{\sum g_i}{\sum H_i + \lambda}$$</p>
<p><strong>For Different Loss Functions</strong>:</p>
<table>
<thead>
<tr>
<th>Loss</th>
<th>$g_i$</th>
<th>$H_i$</th>
</tr>
</thead>
<tbody><tr>
<td>MSE</td>
<td>$\hat{y}_i - y_i$</td>
<td>1</td>
</tr>
<tr>
<td>Log-loss</td>
<td>$\hat{y}_i - y_i$</td>
<td>$\hat{y}_i(1-\hat{y}_i)$</td>
</tr>
</tbody></table>
<p><strong>The Role of $\lambda$</strong>:</p>
<ul>
<li>Appears in denominator: $O^* = -\frac{\sum g_i}{\sum H_i + \lambda}$</li>
<li>High $\lambda$ ‚Üí pushes leaf values toward 0</li>
<li>Prevents any single leaf from having extreme predictions</li>
<li>Acts like L2 regularization in ridge regression</li>
</ul>
<h2 id="regression">Regression</h2>
<p><strong>Similarity Score</strong> (for evaluating potential splits):
$$\text{Similarity} = \frac{G^2}{H + \lambda} = \frac{(\sum g_i)^2}{\sum H_i + \lambda}$$</p>
<p>For MSE loss:
$$\text{Similarity} = \frac{(\sum r_i)^2}{N + \lambda}$$</p>
<p>Where $r_i = y_i - \hat{y}_i$ is the residual and $N$ is the number of samples.</p>
<p><strong>Intuition</strong>:</p>
<ul>
<li>Numerator: Total residual squared (how much signal?)</li>
<li>Denominator: Count + regularization (how much noise?)</li>
<li>Higher similarity = more &quot;pure&quot; node (good for prediction)</li>
</ul>
<p><strong>Effect of $\lambda$</strong>:</p>
<ul>
<li>Large $\lambda$ ‚Üí smaller similarity scores</li>
<li>Reduces sensitivity to individual observations</li>
<li>More pruning, simpler trees</li>
</ul>
<p><strong>Gain from a Split</strong>:
$$\text{Gain} = \text{Similarity}<em>{\text{left}} + \text{Similarity}</em>{\text{right}} - \text{Similarity}_{\text{parent}}$$</p>
<p><strong>Split Criterion</strong>:</p>
<ul>
<li>Only split if $\text{Gain} &gt; \gamma$</li>
<li>$\gamma$ controls minimum improvement required</li>
<li>Even with $\gamma = 0$, pruning still happens (due to regularization)!</li>
</ul>
<p><strong>Pruning Mechanisms</strong>:</p>
<ul>
<li>Maximum depth</li>
<li>Minimum cover (sum of Hessians, i.e., $N$ for regression)</li>
<li>Trees are grown fully, then pruned backward</li>
<li>Key difference from pre-pruning: A &quot;bad&quot; split might enable good subsequent splits</li>
</ul>
<p><strong>Final Predictions</strong>:</p>
<ul>
<li>Leaf output: $\frac{\sum r_i}{N + \lambda}$</li>
<li>Ensemble: Initial Prediction + $\eta \times \text{(Tree 1 output)} + \eta \times \text{(Tree 2 output)} + ...$</li>
<li>Initial prediction = mean of target values</li>
<li>$\eta$ = learning rate (typically 0.01-0.3)</li>
</ul>
<h2 id="classification">Classification</h2>
<p>For classification, XGBoost works with log-odds (logits) and uses log-loss.</p>
<p><strong>Similarity Score</strong>:
$$\text{Similarity} = \frac{(\sum r_i)^2}{\sum p_i(1-p_i) + \lambda}$$</p>
<p>Where:</p>
<ul>
<li>$r_i = y_i - p_i$ (residual: actual minus predicted probability)</li>
<li>$p_i$ = current probability estimate</li>
<li>Denominator: $\sum p_i(1-p_i)$ comes from the Hessian of log-loss</li>
</ul>
<p><strong>Gain Calculation</strong>: Same as regression.</p>
<p><strong>Cover/Minimum Weight</strong>:</p>
<ul>
<li>For regression: Just $N$ (number of samples)</li>
<li>For classification: $\sum p_i(1-p_i)$</li>
<li>This is the sum of Hessians‚Äîmeasures &quot;effective sample size&quot;</li>
<li>Points with $p \approx 0.5$ contribute most (most uncertain)</li>
</ul>
<p><strong>Leaf Output</strong>:
$$O = \frac{\sum r_i}{\sum p_i(1-p_i) + \lambda}$$</p>
<p><strong>Ensemble Prediction</strong>:</p>
<ol>
<li>Initial prediction = $\log\left(\frac{\bar{y}}{1-\bar{y}}\right)$ (log-odds of class proportion)</li>
<li>Add tree contributions with learning rate</li>
<li>Final output is log-odds</li>
<li>Convert to probability: $p = \frac{1}{1 + e^{-\text{log-odds}}}$</li>
</ol>
<h2 id="optimizations">Optimizations</h2>
<p>XGBoost&#39;s speed comes from several clever tricks:</p>
<p><strong>Approximate Greedy Algorithm</strong>:</p>
<ul>
<li>Exact algorithm: Try every possible split point (slow for large data)</li>
<li>Approximate: Bucket continuous features into quantiles</li>
<li>Only consider bucket boundaries as split candidates</li>
<li>Much faster with minimal accuracy loss</li>
</ul>
<p><strong>Quantile Sketch Algorithm</strong>:</p>
<ul>
<li>Need to find quantiles in a distributed setting</li>
<li>XGBoost uses weighted quantiles (weighted by Hessian/cover)</li>
<li>Points with higher uncertainty (higher $H_i$) get more weight</li>
<li>More granular splits where they matter most</li>
</ul>
<p><strong>Sparsity-Aware Split Finding</strong>:</p>
<ul>
<li>Real data often has missing values</li>
<li>XGBoost learns optimal direction for missing values:<ol>
<li>Compute split gain sending missing values left</li>
<li>Compute split gain sending missing values right</li>
<li>Choose direction with higher gain</li>
</ol>
</li>
<li>This &quot;default direction&quot; is learned during training</li>
<li>Also works for zero values in sparse data</li>
</ul>
<p><strong>Cache-Aware Access</strong>:</p>
<ul>
<li>Gradients and Hessians stored in cache for fast access</li>
<li>Block structure for efficient memory access</li>
<li>Out-of-core computation for data that doesn&#39;t fit in memory</li>
</ul>
<h2 id="comparisons">Comparisons</h2>
<p><strong>XGBoost</strong>:</p>
<ul>
<li>Stochastic gradient boosting (row/column subsampling)</li>
<li>No native handling of categorical variables (need encoding)</li>
<li>Depth-wise tree growth (all nodes at same depth split together)</li>
<li>Level-by-level: Explores all possibilities at each depth</li>
</ul>
<p><strong>LightGBM</strong>:</p>
<ul>
<li><strong>GOSS</strong> (Gradient-based One-Side Sampling): Oversample high-gradient points</li>
<li>Native encoding for categorical variables</li>
<li><strong>EFB</strong> (Exclusive Feature Bundling): Combines mutually exclusive features</li>
<li>Histogram-based splitting (faster)</li>
<li><strong>Leaf-wise growth</strong>: Splits the leaf with highest gain<ul>
<li>Faster convergence but can overfit more easily</li>
</ul>
</li>
</ul>
<p><strong>CatBoost</strong>:</p>
<ul>
<li><strong>MVS</strong> (Minimum Variance Sampling): More statistically sound sampling</li>
<li>Superior categorical encoding (ordered target encoding to prevent leakage)</li>
<li><strong>Symmetric trees</strong>: All nodes at same depth use the same split<ul>
<li>Faster inference, natural regularization</li>
</ul>
</li>
<li>Handles missing values and categorical features natively</li>
</ul>
<h2 id="xgboost-vs-traditional-gradient-boosting">XGBoost vs. Traditional Gradient Boosting</h2>
<p><strong>System Optimizations</strong>:</p>
<ul>
<li><strong>Parallelization</strong>: Tree construction parallelized (not tree-to-tree, but within trees)</li>
<li><strong>Cache-aware access</strong>: Block structure for efficient memory usage</li>
<li><strong>Out-of-core computation</strong>: Can handle datasets larger than memory</li>
</ul>
<p><strong>Algorithmic Enhancements</strong>:</p>
<ul>
<li><strong>Regularization</strong>: Built-in L1 and L2 on weights</li>
<li><strong>Missing values</strong>: Learned default directions</li>
<li><strong>Newton boosting</strong>: Second-order optimization (faster convergence)</li>
<li><strong>Weighted quantile sketch</strong>: Approximate split finding</li>
</ul>
<p><strong>Result</strong>: XGBoost is often 10-100x faster than sklearn&#39;s GradientBoostingClassifier while achieving similar or better accuracy.</p>
<h2 id="handling-missing-values">Handling Missing Values</h2>
<p><strong>The Problem</strong>: Most ML algorithms require complete data, forcing imputation.</p>
<p><strong>XGBoost&#39;s Approach</strong>:</p>
<ul>
<li>Treat missing values as a special category</li>
<li>During training: Learn whether missing ‚Üí left or missing ‚Üí right</li>
<li>The &quot;default direction&quot; is chosen to maximize gain</li>
<li>No preprocessing needed!</li>
</ul>
<p><strong>Why This Works Better</strong>:</p>
<ul>
<li>Imputation (mean, median) assumes missing is similar to observed</li>
<li>XGBoost learns what missing values <em>actually</em> mean</li>
<li>Different features can have different optimal directions</li>
</ul>
<p><strong>Comparison to Traditional Approaches</strong>:</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Approach</th>
<th>Limitation</th>
</tr>
</thead>
<tbody><tr>
<td>Mean imputation</td>
<td>Replace with mean</td>
<td>Reduces variance</td>
</tr>
<tr>
<td>Indicator variable</td>
<td>Add &quot;is_missing&quot; feature</td>
<td>Doubles features</td>
</tr>
<tr>
<td>XGBoost native</td>
<td>Learn optimal direction</td>
<td>None‚Äîlearned from data</td>
</tr>
</tbody></table>
<h2 id="hyperparameter-tuning">Hyperparameter Tuning</h2>
<p><strong>Key Hyperparameters</strong>:</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Description</th>
<th>Typical Range</th>
</tr>
</thead>
<tbody><tr>
<td><code>n_estimators</code></td>
<td>Number of trees</td>
<td>100-10000</td>
</tr>
<tr>
<td><code>learning_rate</code> ($\eta$)</td>
<td>Step size shrinkage</td>
<td>0.01-0.3</td>
</tr>
<tr>
<td><code>max_depth</code></td>
<td>Maximum tree depth</td>
<td>3-10</td>
</tr>
<tr>
<td><code>min_child_weight</code></td>
<td>Minimum sum of Hessians in leaf</td>
<td>1-10</td>
</tr>
<tr>
<td><code>gamma</code> ($\gamma$)</td>
<td>Minimum gain for split</td>
<td>0-5</td>
</tr>
<tr>
<td><code>subsample</code></td>
<td>Fraction of rows per tree</td>
<td>0.5-1.0</td>
</tr>
<tr>
<td><code>colsample_bytree</code></td>
<td>Fraction of features per tree</td>
<td>0.5-1.0</td>
</tr>
<tr>
<td><code>lambda</code> ($\lambda$)</td>
<td>L2 regularization</td>
<td>0-10</td>
</tr>
<tr>
<td><code>alpha</code> ($\alpha$)</td>
<td>L1 regularization</td>
<td>0-10</td>
</tr>
</tbody></table>
<p><strong>Tuning Strategy</strong>:</p>
<ol>
<li><strong>Fix learning rate low</strong> (e.g., 0.1), tune other params</li>
<li><strong>Control complexity</strong>: <code>max_depth</code>, <code>min_child_weight</code>, <code>gamma</code></li>
<li><strong>Add randomness</strong>: <code>subsample</code>, <code>colsample_bytree</code></li>
<li><strong>Add regularization</strong>: <code>lambda</code>, <code>alpha</code></li>
<li><strong>Lower learning rate</strong> and increase <code>n_estimators</code></li>
</ol>
<p><strong>Common Approaches</strong>:</p>
<ul>
<li><strong>Grid Search</strong>: Exhaustive but expensive</li>
<li><strong>Random Search</strong>: Often just as good, much faster</li>
<li><strong>Bayesian Optimization</strong>: Intelligent exploration of parameter space</li>
<li><strong>Early Stopping</strong>: Use validation set, stop when performance plateaus</li>
</ul>
<p><strong>Rule of Thumb</strong>:</p>
<ul>
<li>Lower <code>learning_rate</code> + more <code>n_estimators</code> = better but slower</li>
<li>Start with defaults, then tune <code>max_depth</code> and <code>learning_rate</code></li>
<li>Use cross-validation to avoid overfitting to validation set</li>
</ul>

        </article>
        <nav class="page-navigation">
        <a href="gen-03-boosting.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Boosting</span>
        </a>
        <a href="gen-05-clustering.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Clustering</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>