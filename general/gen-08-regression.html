<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Regression | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="gen-08-regression.html" class="active">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../probml/probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="../probml/probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="../probml/probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="../probml/probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="../probml/probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="../probml/probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="../probml/probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="../probml/probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="../probml/probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="../probml/probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">General</a>
          </div>
          <h1 class="page-title">Regression</h1>
          <div class="page-meta"><span class="tag">General</span></div>
        </header>
        <article class="content">
          <h1 id="regression">Regression</h1>
<p>Regression is the task of predicting a continuous outcome from input features. It&#39;s one of the oldest and most fundamental tools in statistics and machine learning, dating back to Gauss and Legendre in the early 1800s.</p>
<h2 id="bi-variate-regression">Bi-variate Regression</h2>
<p><strong>The Goal</strong>: Fit a straight line to data‚Äîunderstand how the response variable changes with one explanatory variable.</p>
<p><strong>The Model</strong>:
$$E(y|x) = \hat{y} = a + bx$$</p>
<p>Where:</p>
<ul>
<li>$a$ = intercept (predicted $y$ when $x = 0$)</li>
<li>$b$ = slope (change in $y$ for unit change in $x$)</li>
</ul>
<p><strong>Fitting the Line</strong> (Ordinary Least Squares):</p>
<p>Minimize the Sum of Squared Errors (SSE):
$$\text{SSE} = \sum_i (y_i - \hat{y}_i)^2$$</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Slope: $b = \frac{S_{xy}}{S_{xx}} = \frac{\sum(x - \bar{x})(y - \bar{y})}{\sum(x - \bar{x})^2}$</li>
<li>Intercept: $a = \bar{y} - b\bar{x}$</li>
</ul>
<p><strong>Why Squared Errors?</strong></p>
<ul>
<li>Penalizes large errors more than small ones</li>
<li>Mathematically convenient (differentiable)</li>
<li>Leads to closed-form solutions</li>
<li>Has nice statistical properties (BLUE under certain assumptions)</li>
</ul>
<p><strong>Important Concepts</strong>:</p>
<p><strong>Outliers and Influential Points</strong>:</p>
<ul>
<li><strong>Outlier</strong>: Point far from the rest of the data</li>
<li><strong>Influential point</strong>: Point that significantly affects the slope</li>
<li>A point can be an outlier without being influential (if $x$ is near $\bar{x}$)</li>
<li>A point can be influential without being an outlier (high leverage)</li>
</ul>
<p><strong>Residual Variance</strong> (estimate of error variance):
$$s = \sqrt{\frac{\text{SSE}}{n - p}}$$</p>
<p>Where $p$ = number of parameters (2 for simple regression).</p>
<p>We divide by $(n-p)$ not $n$ because we &quot;use up&quot; degrees of freedom estimating parameters.</p>
<p><strong>Homoscedasticity</strong>: Assumption that variance is constant across all $x$ values.</p>
<p><strong>Correlation</strong>:
$$r = \frac{\sum(x - \bar{x})(y - \bar{y})}{\sqrt{\sum(x - \bar{x})^2} \cdot \sqrt{\sum(y - \bar{y})^2}}$$</p>
<p>Properties:</p>
<ul>
<li>Ranges from -1 to +1</li>
<li>Measures strength of <em>linear</em> association</li>
<li>Relationship to slope: $r = \frac{s_x}{s_y} \cdot b$</li>
<li>For standardized variables: $r = b$</li>
</ul>
<p><strong>Regression Toward the Mean</strong>:</p>
<ul>
<li>Since $|r| \leq 1$, a 1 SD increase in $x$ predicts less than 1 SD increase in $y$</li>
<li>Extreme values tend to be followed by less extreme values</li>
<li>This is why the technique is called &quot;regression&quot;!</li>
</ul>
<p><strong>R-Squared</strong> (Coefficient of Determination):
$$R^2 = \frac{\text{TSS} - \text{SSE}}{\text{TSS}} = 1 - \frac{\text{SSE}}{\text{TSS}}$$</p>
<p>Where:</p>
<ul>
<li>TSS = Total Sum of Squares = $\sum(y - \bar{y})^2$ (variance in $y$)</li>
<li>SSE = Sum of Squared Errors = $\sum(y - \hat{y})^2$ (unexplained variance)</li>
</ul>
<p><strong>Interpretation</strong>: Proportion of variance in $y$ explained by $x$.</p>
<p>For simple regression: $R^2 = r^2$ (squared correlation)</p>
<p><strong>Statistical Significance</strong> (Is the slope different from zero?):
$$t = \frac{b}{\text{SE}(b)} = \frac{b}{s / \sqrt{S_{xx}}}$$</p>
<p>This follows a t-distribution with $(n-2)$ degrees of freedom.</p>
<p>Equivalently, the F-test: $F = t^2 = \frac{R^2 / 1}{(1-R^2)/(n-2)}$</p>
<h2 id="multivariate-regression">Multivariate Regression</h2>
<p><strong>The Model</strong>:
$$E(y|x) = \hat{y} = a + b_1 x_1 + b_2 x_2 + ... + b_p x_p$$</p>
<p><strong>Interpreting Coefficients</strong>:</p>
<ul>
<li>$b_1$ is the effect of $x_1$ on $y$ <strong>holding all other variables constant</strong></li>
<li>This is called the <strong>partial regression coefficient</strong></li>
<li>Very different from simple regression coefficient!</li>
</ul>
<p><strong>Why Controlling Matters</strong> (Types of Relationships):</p>
<table>
<thead>
<tr>
<th>Relationship</th>
<th>What Happens</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Confounding</strong></td>
<td>Third variable causes both $x$ and $y$; controlling reveals true (weaker) relationship</td>
</tr>
<tr>
<td><strong>Mediation</strong></td>
<td>Third variable transmits effect from $x$ to $y$; controlling removes indirect effect</td>
</tr>
<tr>
<td><strong>Suppression</strong></td>
<td>Third variable masks relationship; controlling reveals hidden relationship</td>
</tr>
</tbody></table>
<p><strong>Partial Regression Plots</strong>:
Visualize the true relationship between $x_1$ and $y$ after removing the effect of other variables:</p>
<ol>
<li>Regress $y$ on all variables except $x_1$ ‚Üí get residuals $e_y$</li>
<li>Regress $x_1$ on all other $x$ variables ‚Üí get residuals $e_{x_1}$</li>
<li>Plot $e_y$ vs $e_{x_1}$</li>
</ol>
<p>The slope of this plot equals $b_1$ from the full model.</p>
<p><strong>Statistical Tests</strong>:</p>
<p><strong>F-test</strong> (Are any predictors significant?):
$$F = \frac{R^2 / (p-1)}{(1-R^2)/(n-p)}$$</p>
<p>Tests whether the model explains more variance than expected by chance.</p>
<p><strong>t-test</strong> (Is a specific predictor significant?):
$$t = \frac{b_j}{\text{SE}(b_j)}$$</p>
<p>Tests whether $b_j$ is significantly different from zero.</p>
<p><strong>Comparing Nested Models</strong>:</p>
<ul>
<li>Complete model: All variables</li>
<li>Reduced model: Some variables dropped
$$F = \frac{(\text{SSE}_r - \text{SSE}_c) / (df_c - df_r)}{\text{SSE}_c / df_c}$$</li>
</ul>
<p><strong>ANOVA Table</strong> (Partitioning Variance):</p>
<table>
<thead>
<tr>
<th>Source</th>
<th>Sum of Squares</th>
<th>df</th>
<th>Mean Square</th>
</tr>
</thead>
<tbody><tr>
<td>Regression</td>
<td>$\sum(\hat{y} - \bar{y})^2$</td>
<td>$p-1$</td>
<td>SSR/(p-1)</td>
</tr>
<tr>
<td>Error</td>
<td>$\sum(y - \hat{y})^2$</td>
<td>$n-p$</td>
<td>SSE/(n-p)</td>
</tr>
<tr>
<td>Total</td>
<td>$\sum(y - \bar{y})^2$</td>
<td>$n-1$</td>
<td>‚Äî</td>
</tr>
</tbody></table>
<p>$F = \text{MSR} / \text{MSE}$</p>
<p><strong>Bonferroni Correction</strong>: When testing multiple coefficients, divide significance level by number of tests to control overall Type I error.</p>
<h2 id="logistic-regression">Logistic Regression</h2>
<p><strong>The Problem</strong>: Linear regression for binary outcomes predicts values outside [0,1].</p>
<p><strong>The Solution</strong>: Model the probability using a sigmoid (S-shaped) curve.</p>
<p><strong>The Model</strong>:
$$P(y=1|x) = \frac{e^{\alpha + \beta x}}{1 + e^{\alpha + \beta x}} = \frac{1}{1 + e^{-(\alpha + \beta x)}}$$</p>
<p>Equivalently, the <strong>log-odds</strong> (logit) is linear:
$$\log\left(\frac{P(y=1)}{1 - P(y=1)}\right) = \alpha + \beta x$$</p>
<p><strong>Why Log-Odds?</strong></p>
<ul>
<li>Odds can range from 0 to ‚àû</li>
<li>Log-odds can range from -‚àû to +‚àû</li>
<li>Makes sense to model with a linear function</li>
</ul>
<p><strong>Interpreting Coefficients</strong>:</p>
<ul>
<li>$\beta$ = change in log-odds for unit increase in $x$</li>
<li>$e^\beta$ = <strong>odds ratio</strong> for unit increase in $x$</li>
<li>If $\beta = 0.5$, then $e^{0.5} \approx 1.65$: the odds multiply by 1.65 for each unit of $x$</li>
</ul>
<p><strong>Propensity Scores</strong> (Causal Inference):</p>
<p>When comparing treatment groups, selection bias can confound results.</p>
<p><strong>Propensity score</strong> = $P(\text{treatment} | \text{covariates})$</p>
<p>Use logistic regression to estimate propensity, then:</p>
<ol>
<li>Match treated/control units with similar propensity</li>
<li>Weight by inverse propensity</li>
<li>Stratify by propensity quintiles</li>
</ol>
<p>This &quot;balances&quot; groups on observed covariates.</p>
<p><strong>Model Comparison</strong>:</p>
<p><strong>Likelihood Ratio Test</strong>:
$$\chi^2 = -2(\log L_{\text{reduced}} - \log L_{\text{full}})$$</p>
<p>Follows chi-squared distribution with $df$ = difference in number of parameters.</p>
<p><strong>Wald Test</strong>: $(b_j / \text{SE}(b_j))^2$ follows chi-squared(1).</p>
<p><strong>Ordinal Logistic Regression</strong> (ordered categories):</p>
<ul>
<li>Model cumulative probabilities: $P(y \leq j)$</li>
<li>Same slopes across all cutpoints (proportional odds assumption)</li>
</ul>
<p><strong>Multinomial Logistic Regression</strong> (unordered categories):</p>
<ul>
<li>One-vs-Rest: Separate model for each class</li>
<li>One-vs-One: Model for each pair of classes</li>
</ul>
<h2 id="regression-diagnostics">Regression Diagnostics</h2>
<p>Good regression analysis requires checking assumptions.</p>
<p><strong>Residual Analysis</strong>:</p>
<ul>
<li><strong>Residuals vs. Fitted</strong>: Should show no pattern (random scatter around 0)</li>
<li><strong>Q-Q Plot</strong>: Residuals should follow the diagonal (normality check)</li>
<li><strong>Scale-Location</strong>: Spread should be constant (homoscedasticity check)</li>
<li><strong>Residuals vs. Leverage</strong>: Identifies influential outliers</li>
</ul>
<p><strong>Multicollinearity</strong> (correlated predictors):</p>
<ul>
<li>Makes coefficient estimates unstable</li>
<li>Inflates standard errors</li>
</ul>
<p><strong>Variance Inflation Factor (VIF)</strong>:
$$\text{VIF}_j = \frac{1}{1 - R_j^2}$$</p>
<p>Where $R_j^2$ is from regressing $x_j$ on all other predictors.</p>
<p>Interpretation:</p>
<ul>
<li>VIF = 1: No correlation with other predictors</li>
<li>VIF = 5: Moderate multicollinearity</li>
<li>VIF &gt; 10: Serious problem‚Äîconsider removing variables or using regularization</li>
</ul>
<p><strong>Influential Observations</strong>:</p>
<table>
<thead>
<tr>
<th>Measure</th>
<th>What It Detects</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Leverage</strong> (hat values)</td>
<td>Points far from $\bar{x}$ that <em>could</em> influence fit</td>
</tr>
<tr>
<td><strong>Residual</strong></td>
<td>Points far from fitted line</td>
</tr>
<tr>
<td><strong>Cook&#39;s Distance</strong></td>
<td>Combined influence on all predictions</td>
</tr>
<tr>
<td><strong>DFBETA</strong></td>
<td>Effect on individual coefficient estimates</td>
</tr>
</tbody></table>
<p>High leverage + large residual = influential point.</p>
<p><strong>Model Selection Criteria</strong>:</p>
<ul>
<li><strong>AIC</strong> = $2k - 2\ln(L)$: Penalizes complexity (lower is better)</li>
<li><strong>BIC</strong> = $k\ln(n) - 2\ln(L)$: Stronger penalty, favors simpler models</li>
<li><strong>Adjusted $R^2$</strong>: $R^2$ penalized for number of predictors</li>
</ul>
<p>Where $k$ = number of parameters, $L$ = likelihood, $n$ = sample size.</p>
<h2 id="advanced-regression-techniques">Advanced Regression Techniques</h2>
<p><strong>Ridge Regression</strong> (L2 regularization):
$$\min_\beta ||\mathbf{y} - \mathbf{X}\boldsymbol{\beta}||^2 + \lambda||\boldsymbol{\beta}||^2$$</p>
<p>Properties:</p>
<ul>
<li>Shrinks coefficients toward zero (but never exactly zero)</li>
<li>Reduces variance at cost of some bias</li>
<li>Excellent for multicollinearity</li>
<li>All predictors kept in model</li>
</ul>
<p><strong>Lasso Regression</strong> (L1 regularization):
$$\min_\beta ||\mathbf{y} - \mathbf{X}\boldsymbol{\beta}||^2 + \lambda||\boldsymbol{\beta}||_1$$</p>
<p>Properties:</p>
<ul>
<li>Shrinks some coefficients exactly to zero</li>
<li>Performs automatic <strong>feature selection</strong></li>
<li>Great for high-dimensional, sparse problems</li>
<li>Selects only one from a group of correlated predictors</li>
</ul>
<p><strong>Elastic Net</strong> (L1 + L2):
$$\min_\beta ||\mathbf{y} - \mathbf{X}\boldsymbol{\beta}||^2 + \lambda_1||\boldsymbol{\beta}||_1 + \lambda_2||\boldsymbol{\beta}||^2$$</p>
<p>Properties:</p>
<ul>
<li>Combines benefits of Ridge and Lasso</li>
<li>Can select groups of correlated features</li>
<li>Two hyperparameters to tune</li>
</ul>
<p><strong>Choosing $\lambda$</strong>: Use cross-validation to find the value that minimizes prediction error on held-out data.</p>
<p><strong>Quantile Regression</strong>:</p>
<ul>
<li>Standard regression models the <strong>mean</strong>: $E(y|x)$</li>
<li>Quantile regression models <strong>quantiles</strong>: e.g., median, 10th percentile</li>
<li>Robust to outliers</li>
<li>Shows how <em>distribution</em> of $y$ changes with $x$</li>
</ul>
<p><strong>When to Use</strong>:</p>
<ul>
<li>When relationship differs across the distribution (e.g., effect on high vs. low income)</li>
<li>When outliers are a concern</li>
<li>When you care about specific quantiles (e.g., 95th percentile for risk)</li>
</ul>

        </article>
        <nav class="page-navigation">
        <a href="gen-07-dimensionality_reduction.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Dimensionality Reduction</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>