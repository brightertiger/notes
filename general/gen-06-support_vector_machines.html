<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Support Vector Machines | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="gen-06-support_vector_machines.html" class="active">Support Vector Machines</a></li>
            <li class="nav-item"><a href="gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../probml/probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="../probml/probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="../probml/probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="../probml/probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="../probml/probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="../probml/probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="../probml/probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="../probml/probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="../probml/probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="../probml/probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">General</a>
          </div>
          <h1 class="page-title">Support Vector Machines</h1>
          <div class="page-meta"><span class="tag">General</span></div>
        </header>
        <article class="content">
          <h1 id="support-vector-machines">Support Vector Machines</h1>
<p>Support Vector Machines (SVMs) are powerful classifiers based on a beautifully geometric idea: find the hyperplane that separates classes with the <strong>maximum margin</strong>. This margin acts as a &quot;safety buffer&quot; that often leads to excellent generalization.</p>
<h2 id="linear-svm">Linear SVM</h2>
<p><strong>The Core Question</strong>: Given labeled data, what&#39;s the &quot;best&quot; hyperplane to separate the classes?</p>
<p><strong>The SVM Answer</strong>: The best hyperplane is the one that maximizes the distance to the nearest points from each class. These nearest points are called <strong>support vectors</strong> because they &quot;support&quot; (define) the margin.</p>
<p><strong>Why Maximize the Margin?</strong>:</p>
<ul>
<li>Think of the margin as a &quot;no man&#39;s land&quot; between classes</li>
<li>Larger margin ‚Üí more room for error on new data</li>
<li>Intuitively, a decision boundary right at the edge of your training data is fragile</li>
<li>A boundary with wide margins should generalize better</li>
</ul>
<p><strong>Hyperplane Basics</strong>:</p>
<ul>
<li>A hyperplane in $d$ dimensions: $H: \mathbf{w} \cdot \mathbf{x} + b = 0$</li>
<li>$\mathbf{w}$ = normal vector (perpendicular to the hyperplane)</li>
<li>$b$ = offset (distance from origin)</li>
<li>Points with $\mathbf{w} \cdot \mathbf{x} + b &gt; 0$ are on one side; $&lt; 0$ on the other</li>
</ul>
<p><strong>Distance from Point to Hyperplane</strong>:
$$d = \frac{|\mathbf{w} \cdot \mathbf{x} + b|}{||\mathbf{w}||}$$</p>
<p>This is just the projection of the point onto the normal vector, normalized by the length of $\mathbf{w}$.</p>
<p><strong>The Margin</strong>:
$$\gamma(\mathbf{w}, b) = \min_{x \in D} \frac{|\mathbf{w} \cdot \mathbf{x} + b|}{||\mathbf{w}||}$$</p>
<p>The margin is the distance to the <em>closest</em> point. It&#39;s scale-invariant: multiplying $\mathbf{w}$ and $b$ by any constant doesn&#39;t change the hyperplane or its margin.</p>
<p><strong>The Optimization Problem</strong>:</p>
<p>For binary classification with $y_i \in {+1, -1}$:</p>
<ul>
<li>We need: $y_i(\mathbf{w} \cdot \mathbf{x}_i + b) &gt; 0$ for all points (correct classification)</li>
<li>We want: Maximize the margin</li>
</ul>
<p><strong>Original Formulation</strong>:
$$\max_{\mathbf{w}, b} \min_{x \in D} \frac{|\mathbf{w} \cdot \mathbf{x} + b|}{||\mathbf{w}||} \quad \text{subject to} \quad y_i(\mathbf{w} \cdot \mathbf{x}_i + b) &gt; 0$$</p>
<p>This is a max-min problem‚Äîtricky to solve directly.</p>
<p><strong>Clever Simplification</strong>:
Since the hyperplane is scale-invariant, we can choose any scale. Let&#39;s fix:
$$|\mathbf{w} \cdot \mathbf{x} + b| = 1 \quad \text{for the points closest to the hyperplane}$$</p>
<p>Now the margin becomes $\frac{1}{||\mathbf{w}||}$, and maximizing margin = minimizing $||\mathbf{w}||$.</p>
<p><strong>Final SVM Objective</strong> (Hard Margin):
$$\min \frac{1}{2}||\mathbf{w}||^2 \quad \text{subject to} \quad y_i(\mathbf{w} \cdot \mathbf{x}_i + b) \geq 1 \quad \forall i$$</p>
<p><strong>Why $\frac{1}{2}||\mathbf{w}||^2$?</strong>:</p>
<ul>
<li>Equivalent to minimizing $||\mathbf{w}||$ (squared is easier mathematically)</li>
<li>The $\frac{1}{2}$ makes derivatives cleaner</li>
</ul>
<p><strong>This is a Quadratic Programming (QP) problem</strong>:</p>
<ul>
<li>Quadratic objective, linear constraints</li>
<li>Efficient solvers exist</li>
<li>Unique global solution (unlike perceptron, which finds any separating hyperplane)</li>
</ul>
<p><strong>Support Vectors</strong>: At the optimal solution, some points satisfy $y_i(\mathbf{w} \cdot \mathbf{x}_i + b) = 1$ exactly. These are the support vectors‚Äîthey lie on the margin boundary and fully determine the solution.</p>
<h2 id="soft-margin-svm">Soft Margin SVM</h2>
<p><strong>The Problem</strong>: What if data isn&#39;t perfectly separable?</p>
<p><strong>Hard margin SVM fails if</strong>:</p>
<ul>
<li>Classes overlap</li>
<li>There are outliers</li>
<li>Data is noisy</li>
</ul>
<p><strong>The Solution</strong>: Allow some misclassifications, but penalize them.</p>
<p><strong>Slack Variables</strong> ($\xi_i$):</p>
<ul>
<li>Original constraint: $y_i(\mathbf{w} \cdot \mathbf{x}_i + b) \geq 1$</li>
<li>Relaxed constraint: $y_i(\mathbf{w} \cdot \mathbf{x}_i + b) \geq 1 - \xi_i$ where $\xi_i \geq 0$</li>
</ul>
<p><strong>What $\xi_i$ Represents</strong>:</p>
<ul>
<li>$\xi_i = 0$: Point correctly classified and outside margin</li>
<li>$0 &lt; \xi_i &lt; 1$: Point correctly classified but inside margin</li>
<li>$\xi_i = 1$: Point exactly on the decision boundary</li>
<li>$\xi_i &gt; 1$: Point misclassified</li>
</ul>
<p><strong>Soft Margin Objective</strong>:
$$\min \frac{1}{2}||\mathbf{w}||^2 + C \sum_i \xi_i \quad \text{subject to} \quad y_i(\mathbf{w} \cdot \mathbf{x}_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0$$</p>
<p><strong>The Hinge Loss</strong>:
$$\xi_i = \max(1 - y_i(\mathbf{w} \cdot \mathbf{x}_i + b), 0)$$</p>
<p>This is the famous <strong>hinge loss</strong>‚Äîzero for points outside the margin, linear penalty for points inside or misclassified.</p>
<p><strong>The C Parameter</strong>:</p>
<ul>
<li>High $C$: Little tolerance for errors ‚Üí narrow margin, risk of overfitting</li>
<li>Low $C$: More tolerance for errors ‚Üí wide margin, risk of underfitting</li>
<li>$C = \infty$: Hard margin SVM</li>
</ul>
<h2 id="duality">Duality</h2>
<p><strong>The Primal Problem</strong> (what we wrote above) can be hard to solve directly. Converting to the <strong>dual problem</strong> has advantages:</p>
<ul>
<li>Often easier to solve</li>
<li>Reveals the kernel trick</li>
</ul>
<p><strong>Lagrangian Formulation</strong>:
Introduce Lagrange multipliers $\alpha_i$ for each constraint:
$$L = \frac{1}{2}||\mathbf{w}||^2 - \sum_i \alpha_i [y_i(\mathbf{w} \cdot \mathbf{x}_i + b) - 1]$$</p>
<p><strong>The Dual Problem</strong> (after taking derivatives):
$$\max_\alpha \sum_i \alpha_i - \frac{1}{2}\sum_i \sum_j \alpha_i \alpha_j y_i y_j (\mathbf{x}_i^T \mathbf{x}_j)$$
$$\text{subject to} \quad \alpha_i \geq 0, \quad \sum_i \alpha_i y_i = 0$$</p>
<p><strong>Key Insight</strong>: The data only appears as dot products $\mathbf{x}_i^T \mathbf{x}_j$!</p>
<p><strong>Support Vectors and $\alpha_i$</strong>:</p>
<ul>
<li>If $\alpha_i = 0$: Point is not a support vector (doesn&#39;t affect solution)</li>
<li>If $\alpha_i &gt; 0$: Point is a support vector</li>
</ul>
<p>Most $\alpha_i$ are zero ‚Üí the solution depends only on support vectors.</p>
<h2 id="kernelization">Kernelization</h2>
<p><strong>The Problem</strong>: What if data isn&#39;t linearly separable in the original space?</p>
<p><strong>The Solution</strong>: Map data to a higher-dimensional space where it might be linearly separable.</p>
<p><strong>Feature Mapping</strong>:</p>
<ul>
<li>Original data: $\mathbf{x}$</li>
<li>Mapped data: $\phi(\mathbf{x})$ in higher (possibly infinite) dimension</li>
<li>Find a hyperplane in this new space</li>
</ul>
<p><strong>The Kernel Trick</strong>:
Remember, the dual problem only uses dot products $\mathbf{x}_i^T \mathbf{x}_j$.</p>
<p>If we work in the mapped space, we need $\phi(\mathbf{x}_i)^T \phi(\mathbf{x}_j)$.</p>
<p><strong>Key insight</strong>: We can define a <strong>kernel function</strong> $K(\mathbf{x}_i, \mathbf{x}_j) = \phi(\mathbf{x}_i)^T \phi(\mathbf{x}_j)$ that computes this dot product <em>without ever explicitly computing $\phi$</em>.</p>
<p><strong>Common Kernels</strong>:</p>
<p><strong>Polynomial Kernel</strong>:
$$K(\mathbf{x}_i, \mathbf{x}_j) = (\mathbf{x}_i^T \mathbf{x}_j + c)^d$$</p>
<p>Example: 1D points $a$ and $b$, degree 2:</p>
<ul>
<li>$K(a, b) = (ab + 1)^2 = a^2b^2 + 2ab + 1$</li>
<li>This equals $\phi(a)^T \phi(b)$ where $\phi(x) = (x^2, \sqrt{2}x, 1)$</li>
</ul>
<p>The kernel implicitly maps to a 3D space!</p>
<p><strong>RBF (Gaussian) Kernel</strong>:
$$K(\mathbf{x}_i, \mathbf{x}_j) = \exp\left(-\gamma ||\mathbf{x}_i - \mathbf{x}_j||^2\right)$$</p>
<p>Properties:</p>
<ul>
<li>Similarity decreases exponentially with distance</li>
<li>Maps to <strong>infinite-dimensional</strong> space (via Taylor expansion of exponential)</li>
<li>Very flexible‚Äîcan fit complex boundaries</li>
<li>$\gamma$ controls the &quot;reach&quot; of each training point</li>
</ul>
<p><strong>Why Kernels are Powerful</strong>:</p>
<ol>
<li>Compute high-dimensional dot products efficiently</li>
<li>Don&#39;t need to explicitly represent the high-dimensional vectors</li>
<li>Can work in infinite-dimensional spaces (RBF)</li>
<li>Turn linear methods into non-linear methods</li>
</ol>
<h2 id="svm-for-regression-svr">SVM for Regression (SVR)</h2>
<p><strong>The Twist</strong>: Instead of maximizing margin between classes, we define a &quot;tube&quot; around the regression line and minimize points outside it.</p>
<p><strong>$\epsilon$-Insensitive Loss</strong>:</p>
<ul>
<li>No penalty if prediction is within $\epsilon$ of true value</li>
<li>Linear penalty for points outside the tube</li>
</ul>
<p><strong>Formulation</strong>:
$$\min \frac{1}{2}||\mathbf{w}||^2 + C\sum_i (\xi_i + \xi_i^*)$$</p>
<p>Subject to:</p>
<ul>
<li>$y_i - (\mathbf{w} \cdot \mathbf{x}_i + b) \leq \epsilon + \xi_i$</li>
<li>$(\mathbf{w} \cdot \mathbf{x}_i + b) - y_i \leq \epsilon + \xi_i^*$</li>
<li>$\xi_i, \xi_i^* \geq 0$</li>
</ul>
<p><strong>Intuition</strong>:</p>
<ul>
<li>The regression line lies in the middle of the tube</li>
<li>Points inside the tube (within $\epsilon$) don&#39;t contribute to the solution</li>
<li>Only points outside the tube become support vectors</li>
</ul>
<h2 id="kernel-selection">Kernel Selection</h2>
<p><strong>Linear Kernel</strong>: $K(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i^T \mathbf{x}_j$</p>
<ul>
<li>Fastest, most interpretable</li>
<li>Best when: Many features relative to samples (text classification)</li>
<li>No hyperparameters beyond $C$</li>
</ul>
<p><strong>Polynomial Kernel</strong>: $K(\mathbf{x}_i, \mathbf{x}_j) = (\mathbf{x}_i^T \mathbf{x}_j + c)^d$</p>
<ul>
<li>Captures feature interactions</li>
<li>Higher $d$ = more complex boundaries</li>
<li>Parameters: degree $d$, coefficient $c$</li>
</ul>
<p><strong>RBF Kernel</strong>: $K(\mathbf{x}_i, \mathbf{x}_j) = \exp(-\gamma ||\mathbf{x}_i - \mathbf{x}_j||^2)$</p>
<ul>
<li>Most versatile, usually a good default</li>
<li>Higher $\gamma$ = tighter fit around training points</li>
<li>Parameters: $\gamma$ (often $\gamma = 1/(2\sigma^2)$)</li>
</ul>
<p><strong>Sigmoid Kernel</strong>: $K(\mathbf{x}_i, \mathbf{x}_j) = \tanh(\alpha \mathbf{x}_i^T \mathbf{x}_j + c)$</p>
<ul>
<li>Similar to neural network activation</li>
<li>Less commonly used; can have convergence issues</li>
</ul>
<p><strong>Rule of Thumb</strong>:</p>
<ol>
<li>Start with RBF (most flexible)</li>
<li>If that works well, try linear (faster, more interpretable)</li>
<li>Use cross-validation to select kernel and hyperparameters</li>
</ol>
<h2 id="svm-hyperparameter-tuning">SVM Hyperparameter Tuning</h2>
<p><strong>The C Parameter</strong>:</p>
<ul>
<li>Trade-off: margin width vs. training accuracy</li>
<li>Low $C$: Wide margin, more misclassifications allowed, simpler model</li>
<li>High $C$: Narrow margin, few misclassifications, complex model (may overfit)</li>
</ul>
<p><strong>The $\gamma$ Parameter</strong> (RBF kernel):</p>
<ul>
<li>Controls influence radius of each support vector</li>
<li>Low $\gamma$: Large radius, smoother boundary, points influence far away</li>
<li>High $\gamma$: Small radius, complex boundary, points only influence locally</li>
</ul>
<p><strong>The Trade-off</strong>:</p>
<table>
<thead>
<tr>
<th></th>
<th>Low $C$</th>
<th>High $C$</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Low $\gamma$</strong></td>
<td>Very smooth (underfit)</td>
<td>Smooth but fits training well</td>
</tr>
<tr>
<td><strong>High $\gamma$</strong></td>
<td>Wiggly but regularized</td>
<td>Very complex (overfit)</td>
</tr>
</tbody></table>
<p><strong>Tuning Strategy</strong>:</p>
<ol>
<li>Grid search over $C$ and $\gamma$ on logarithmic scale</li>
<li>Example: $C \in {0.001, 0.01, 0.1, 1, 10, 100, 1000}$</li>
<li>Example: $\gamma \in {0.001, 0.01, 0.1, 1, 10}$</li>
<li>Use cross-validation to evaluate each combination</li>
<li>Select combination with best validation performance</li>
</ol>
<p><strong>Practical Tips</strong>:</p>
<ul>
<li>Scale your features! SVMs are sensitive to feature scales</li>
<li>RBF kernel with well-tuned $C$ and $\gamma$ is often competitive with more complex methods</li>
<li>For large datasets, consider linear SVM (much faster) or approximations</li>
</ul>

        </article>
        <nav class="page-navigation">
        <a href="gen-05-clustering.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Clustering</span>
        </a>
        <a href="gen-07-dimensionality_reduction.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Dimensionality Reduction</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>