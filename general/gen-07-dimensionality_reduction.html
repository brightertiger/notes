<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Dimensionality Reduction | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="gen-01-basic-statistics.html">Basic Statistics</a></li>
            <li class="nav-item"><a href="gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="gen-07-dimensionality_reduction.html" class="active">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../probml/probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="../probml/probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="../probml/probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="../probml/probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="../probml/probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="../probml/probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="../probml/probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="../probml/probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="../probml/probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="../probml/probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">General</a>
          </div>
          <h1 class="page-title">Dimensionality Reduction</h1>
          <div class="page-meta"><span class="tag">General</span></div>
        </header>
        <article class="content">
          <h1 id="dimensionality-reduction">Dimensionality Reduction</h1>
<p>High-dimensional data is everywhere: images (thousands of pixels), text (thousands of words), genomics (thousands of genes). Dimensionality reduction compresses this data into a smaller number of meaningful features while preserving important structure.</p>
<h2 id="background">Background</h2>
<p><strong>The Curse of Dimensionality</strong>:</p>
<p>As dimensions increase, data becomes increasingly sparse. Consider:</p>
<ul>
<li>A 1√ó1 square patch covers 1% of a 10√ó10 square</li>
<li>A 1√ó1√ó1 cubic patch covers 0.1% of a 10√ó10√ó10 cube</li>
<li>In general: Volume grows exponentially with dimension</li>
</ul>
<p><strong>Why This Matters</strong>:</p>
<ul>
<li>Machine learning needs enough data to &quot;fill&quot; the space</li>
<li>Data requirements grow exponentially with dimensions</li>
<li>Distances become less meaningful (everything is &quot;far&quot; from everything)</li>
<li>Many algorithms break down</li>
</ul>
<p><strong>Two Approaches to Reduce Dimensions</strong>:</p>
<ol>
<li><p><strong>Feature Selection</strong>: Keep a subset of original features</p>
<ul>
<li>Pros: Interpretable, simple</li>
<li>Cons: May lose important combinations</li>
</ul>
</li>
<li><p><strong>Feature Extraction/Latent Features</strong>: Create new features from combinations of originals</p>
<ul>
<li>Linear methods: PCA, LDA</li>
<li>Non-linear methods: t-SNE, UMAP, autoencoders</li>
<li>Pros: Can capture more complex structure</li>
<li>Cons: New features may be hard to interpret</li>
</ul>
</li>
</ol>
<h2 id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h2>
<p>PCA finds the directions of maximum variance in the data and projects onto them. It&#39;s the most widely used dimensionality reduction technique.</p>
<p><strong>The Idea</strong>:</p>
<ul>
<li>Find a linear projection from high dimension ($D$) to low dimension ($L$)</li>
<li>Preserve as much variance as possible</li>
<li>The directions of maximum variance are called <strong>principal components</strong></li>
</ul>
<p><strong>Mathematical Setup</strong>:</p>
<ul>
<li>Original data: $\mathbf{x} \in \mathbb{R}^D$</li>
<li>Projection matrix: $\mathbf{W} \in \mathbb{R}^{D \times L}$ (columns are orthonormal)</li>
<li><strong>Encode</strong>: $\mathbf{z} = \mathbf{W}^T \mathbf{x} \in \mathbb{R}^L$</li>
<li><strong>Decode</strong>: $\hat{\mathbf{x}} = \mathbf{W}\mathbf{z}$</li>
</ul>
<p><strong>Objective</strong>: Minimize reconstruction error
$$L(\mathbf{W}) = \frac{1}{N}\sum_i ||\mathbf{x}_i - \hat{\mathbf{x}}_i||^2$$</p>
<p><strong>Derivation</strong> (projecting to 1D):</p>
<p>We want to find $\mathbf{w}_1$ that minimizes reconstruction error:
$$L = \frac{1}{N}\sum_i ||\mathbf{x}<em>i - z</em>{i1}\mathbf{w}_1||^2$$</p>
<p>Expanding and using $\mathbf{w}_1^T\mathbf{w}_1 = 1$ (orthonormality):
$$L = \frac{1}{N}\sum_i [\mathbf{x}_i^T\mathbf{x}<em>i - 2z</em>{i1}\mathbf{w}_1^T\mathbf{x}<em>i + z</em>{i1}^2]$$</p>
<p>Taking derivative w.r.t. $z_{i1}$:
$$\frac{\partial L}{\partial z_{i1}} = -2\mathbf{w}_1^T\mathbf{x}<em>i + 2z</em>{i1} = 0$$</p>
<p><strong>Optimal encoding</strong>: $z_{i1} = \mathbf{w}_1^T\mathbf{x}_i$ (project onto $\mathbf{w}_1$)</p>
<p>Substituting back:
$$L = C - \frac{1}{N}\sum_i z_{i1}^2 = C - \frac{1}{N}\mathbf{w}_1^T\boldsymbol{\Sigma}\mathbf{w}_1$$</p>
<p>Where $\boldsymbol{\Sigma}$ is the covariance matrix of $\mathbf{X}$.</p>
<p><strong>Key Insight</strong>: Minimizing reconstruction error = Maximizing variance of projections!</p>
<p>Using Lagrange multipliers with constraint $\mathbf{w}_1^T\mathbf{w}_1 = 1$:
$$\frac{\partial}{\partial \mathbf{w}_1}\left[\mathbf{w}_1^T\boldsymbol{\Sigma}\mathbf{w}_1 + \lambda(1 - \mathbf{w}_1^T\mathbf{w}_1)\right] = 0$$
$$\boldsymbol{\Sigma}\mathbf{w}_1 = \lambda\mathbf{w}_1$$</p>
<p><strong>The optimal $\mathbf{w}_1$ is an eigenvector of the covariance matrix!</strong></p>
<p>To maximize variance, choose the eigenvector with the <strong>largest eigenvalue</strong>.</p>
<p><strong>Geometric Interpretation</strong>:</p>
<ul>
<li>Imagine the data as a cloud of points</li>
<li>The first principal component is the direction of maximum spread</li>
<li>The second PC is perpendicular and captures the next most variance</li>
<li>And so on...</li>
</ul>
<p>Think of it as finding the best &quot;viewing angle&quot; for your data.</p>
<p><strong>Eigenvalues = Variance Explained</strong>:</p>
<ul>
<li>Each eigenvalue equals the variance along that principal component</li>
<li>Sum of all eigenvalues = total variance</li>
<li>Fraction explained by first $k$ components: $\frac{\sum_{i=1}^k \lambda_i}{\sum_{j=1}^D \lambda_j}$</li>
</ul>
<p><strong>Scree Plot</strong>: Graph eigenvalues (or % variance) vs. component number</p>
<ul>
<li>Look for an &quot;elbow&quot; where variance drops off</li>
<li>Components before the elbow are usually important</li>
</ul>
<p><strong>Factor Loadings</strong>:</p>
<ul>
<li>Each PC is a linear combination of original features</li>
<li>Loadings = weights in this combination</li>
<li>High loading = feature contributes strongly to that PC</li>
</ul>
<p><strong>PCA + Regression</strong>: Still interpretable!</p>
<ul>
<li>Run regression on principal components</li>
<li>Use loadings to translate back to original features</li>
</ul>
<p><strong>Computing PCA via SVD</strong>:</p>
<ul>
<li>Singular Value Decomposition: $\mathbf{X} = \mathbf{U}\mathbf{S}\mathbf{V}^T$</li>
<li>$\mathbf{V}$ contains the principal components (eigenvectors)</li>
<li>$\mathbf{S}$ contains singular values (square roots of eigenvalues)</li>
<li>More numerically stable than eigendecomposition</li>
</ul>
<p><strong>Limitations of PCA</strong>:</p>
<ul>
<li>Only captures <strong>linear</strong> relationships</li>
<li>Sensitive to <strong>outliers</strong> (they inflate variance)</li>
<li>Can&#39;t handle <strong>missing data</strong> (need imputation first)</li>
<li><strong>Unsupervised</strong>: Doesn&#39;t use label information</li>
</ul>
<p><strong>Alternatives</strong>:</p>
<ul>
<li><strong>Kernel PCA</strong>: Non-linear version using the kernel trick</li>
<li><strong>Factor Analysis</strong>: Assumes latent factors + noise</li>
<li><strong>LDA</strong>: Supervised, maximizes between-class variance</li>
</ul>
<h2 id="stochastic-neighbor-embedding-sne">Stochastic Neighbor Embedding (SNE)</h2>
<p><strong>The Idea</strong>: Preserve local neighborhood structure rather than global distances. Points that are neighbors in high-D should be neighbors in low-D.</p>
<p><strong>Manifold Hypothesis</strong>:</p>
<ul>
<li>High-dimensional data often lies on a lower-dimensional &quot;manifold&quot;</li>
<li>Think: Earth&#39;s surface is a 2D manifold in 3D space</li>
<li>We want to &quot;unfold&quot; this manifold</li>
</ul>
<p><strong>Algorithm</strong>:</p>
<ol>
<li><strong>Convert distances to probabilities</strong> (high-D):
$$p_{j|i} \propto \exp\left(-\frac{||\mathbf{x}_i - \mathbf{x}_j||^2}{2\sigma_i^2}\right)$$</li>
</ol>
<p>This is the probability that point $i$ would pick point $j$ as its neighbor.</p>
<p><strong>Adaptive variance</strong> ($\sigma_i$):</p>
<ul>
<li>Dense regions get smaller $\sigma_i$ (be more selective)</li>
<li>Sparse regions get larger $\sigma_i$ (include more distant neighbors)</li>
<li>Controlled by <strong>perplexity</strong> parameter (roughly, the number of effective neighbors)</li>
</ul>
<ol start="2">
<li><p><strong>Initialize low-D coordinates</strong> $\mathbf{z}_i$ randomly</p>
</li>
<li><p><strong>Compute low-D probabilities</strong>:
$$q_{j|i} \propto \exp\left(-||\mathbf{z}_i - \mathbf{z}_j||^2\right)$$</p>
</li>
<li><p><strong>Minimize KL divergence</strong> between $p$ and $q$:
$$L = \sum_i \sum_j p_{j|i} \log\frac{p_{j|i}}{q_{j|i}}$$</p>
</li>
</ol>
<p><strong>Why KL Divergence?</strong>:</p>
<ul>
<li>If $p$ is high but $q$ is low: <strong>Large penalty</strong> (neighbors in high-D are far in low-D ‚Äî bad!)</li>
<li>If $p$ is low but $q$ is high: <strong>Small penalty</strong> (distant points end up close ‚Äî not as bad)</li>
<li>Prioritizes preserving local structure</li>
</ul>
<p><strong>Symmetric SNE</strong>: Make distances symmetric: $p_{ij} = \frac{p_{j|i} + p_{i|j}}{2}$</p>
<h2 id="t-sne">t-SNE</h2>
<p><strong>The Problem with SNE</strong>: Crowding. Gaussian probability decays quickly, pushing moderately distant points too close together in low-D.</p>
<p><strong>The Solution</strong>: Use the <strong>t-distribution</strong> (fat tails) for low-D probabilities:
$$q_{ij} \propto (1 + ||\mathbf{z}_i - \mathbf{z}_j||^2)^{-1}$$</p>
<p><strong>Why t-Distribution?</strong>:</p>
<ul>
<li>Heavier tails than Gaussian</li>
<li>Points that are moderately far in high-D can be <em>very</em> far in low-D</li>
<li>Creates well-separated clusters with tight internal structure</li>
</ul>
<p><strong>t-SNE Properties</strong>:</p>
<ul>
<li>Excellent for visualization (2D or 3D)</li>
<li>Creates visually appealing, well-separated clusters</li>
<li>Preserves local structure well</li>
</ul>
<p><strong>Limitations</strong>:</p>
<ul>
<li><strong>Computationally expensive</strong>: $O(N^2)$ for pairwise distances</li>
<li><strong>Random initialization</strong>: Results vary between runs</li>
<li><strong>Not invertible</strong>: Can&#39;t go from low-D back to high-D</li>
<li><strong>Coordinates are meaningless</strong>: Only relative positions matter</li>
<li><strong>Global structure distorted</strong>: Distances between clusters are not meaningful</li>
</ul>
<p><strong>Hyperparameters</strong>:</p>
<ul>
<li><strong>Perplexity</strong>: Balance between local and global (typical: 5-50)</li>
<li><strong>Learning rate</strong>: Step size for gradient descent</li>
<li><strong>Iterations</strong>: Usually 1000+</li>
</ul>
<h2 id="umap">UMAP</h2>
<p><strong>Uniform Manifold Approximation and Projection</strong>: Like t-SNE but faster and (arguably) better at preserving global structure.</p>
<p><strong>Key Differences from t-SNE</strong>:</p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>t-SNE</th>
<th>UMAP</th>
</tr>
</thead>
<tbody><tr>
<td>Pairwise distances</td>
<td>All pairs</td>
<td>Only neighbors</td>
</tr>
<tr>
<td>Initialization</td>
<td>Random</td>
<td>Spectral embedding</td>
</tr>
<tr>
<td>Updates</td>
<td>All points every iteration</td>
<td>Stochastic (subsets)</td>
</tr>
<tr>
<td>Speed</td>
<td>Slow ($O(N^2)$)</td>
<td>Much faster</td>
</tr>
<tr>
<td>Global structure</td>
<td>Often distorted</td>
<td>Better preserved</td>
</tr>
</tbody></table>
<p><strong>UMAP Algorithm</strong>:</p>
<ol>
<li><p><strong>Build neighborhood graph</strong> in high-D</p>
<ul>
<li>Compute distance to $k$ nearest neighbors</li>
<li>Convert to similarity scores (exponential decay from nearest neighbor)</li>
</ul>
</li>
<li><p><strong>Make similarities symmetric</strong>: $s_{ij} = s_{i|j} + s_{j|i} - s_{i|j} \cdot s_{j|i}$</p>
</li>
<li><p><strong>Initialize low-D with spectral embedding</strong> (decomposition of graph Laplacian)</p>
</li>
<li><p><strong>Compute low-D similarities</strong> using t-distribution variant:
$$q_{ij} \propto (1 + \alpha \cdot d^{2\beta})^{-1}$$</p>
</li>
<li><p><strong>Minimize cross-entropy</strong> between high-D and low-D graphs</p>
</li>
</ol>
<p><strong>UMAP Advantages</strong>:</p>
<ul>
<li><strong>Much faster</strong> than t-SNE (especially for large datasets)</li>
<li><strong>Better global structure</strong>: Preserves relative cluster distances better</li>
<li><strong><code>transform</code> method</strong>: Can embed new points without recomputing everything</li>
<li><strong>Flexible</strong>: Can use any distance metric</li>
</ul>
<p><strong>When to Use Which</strong>:</p>
<ul>
<li><strong>Small data, visualization only</strong>: Either works, t-SNE often prettier</li>
<li><strong>Large data</strong>: UMAP (t-SNE too slow)</li>
<li><strong>Need to embed new points</strong>: UMAP</li>
<li><strong>Need reproducibility</strong>: UMAP (more stable with same parameters)</li>
</ul>
<h2 id="applications-of-dimensionality-reduction">Applications of Dimensionality Reduction</h2>
<p><strong>Data Visualization</strong>:</p>
<ul>
<li>Reduce to 2D or 3D for plotting</li>
<li>Discover clusters, outliers, patterns visually</li>
<li>t-SNE and UMAP are standard tools</li>
</ul>
<p><strong>Noise Reduction</strong>:</p>
<ul>
<li>Signal variance &gt; noise variance</li>
<li>First few PCs capture signal, later PCs capture noise</li>
<li>Reconstructing from top PCs filters out noise</li>
</ul>
<p><strong>Preprocessing for ML</strong>:</p>
<ul>
<li>Reduces curse of dimensionality</li>
<li>Speeds up training</li>
<li>Can improve performance (by removing noise)</li>
<li>Essential for algorithms sensitive to dimensionality (e.g., k-NN)</li>
</ul>
<p><strong>Feature Extraction</strong>:</p>
<ul>
<li>Create more informative features</li>
<li>Example: Face recognition‚Äîfirst few PCs are &quot;eigenfaces&quot;</li>
</ul>
<p><strong>Multicollinearity</strong>:</p>
<ul>
<li>Highly correlated predictors cause problems in regression</li>
<li>PCA creates uncorrelated components</li>
<li>Principal Component Regression (PCR) solves this</li>
</ul>
<h2 id="comparing-techniques">Comparing Techniques</h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Linear?</th>
<th>Preserves</th>
<th>Best For</th>
<th>Interpretable?</th>
</tr>
</thead>
<tbody><tr>
<td>PCA</td>
<td>Yes</td>
<td>Global variance</td>
<td>Preprocessing, compression</td>
<td>Yes (loadings)</td>
</tr>
<tr>
<td>t-SNE</td>
<td>No</td>
<td>Local neighborhoods</td>
<td>2D/3D visualization</td>
<td>No</td>
</tr>
<tr>
<td>UMAP</td>
<td>No</td>
<td>Local + some global</td>
<td>Large-scale visualization</td>
<td>No</td>
</tr>
<tr>
<td>LDA</td>
<td>Yes</td>
<td>Class separation</td>
<td>Supervised reduction</td>
<td>Yes</td>
</tr>
</tbody></table>
<p><strong>Selection Guide</strong>:</p>
<ol>
<li><strong>Know you need visualization?</strong> ‚Üí t-SNE or UMAP</li>
<li><strong>Need to preprocess for ML?</strong> ‚Üí PCA</li>
<li><strong>Have class labels?</strong> ‚Üí Consider LDA</li>
<li><strong>Need interpretability?</strong> ‚Üí PCA</li>
<li><strong>Very large data?</strong> ‚Üí UMAP or randomized PCA</li>
<li><strong>Complex non-linear structure?</strong> ‚Üí UMAP, t-SNE, or kernel PCA</li>
</ol>

        </article>
        <nav class="page-navigation">
        <a href="gen-06-support_vector_machines.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">Support Vector Machines</span>
        </a>
        <a href="gen-08-regression.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Regression</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>