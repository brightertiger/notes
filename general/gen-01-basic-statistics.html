<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Basic Statistics | ML Notes</title>
  <link rel="stylesheet" href="../css/style.css">
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: { enableMenu: false }
    };
  </script>
</head>
<body>
  <div class="layout">
    
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <a href="../index.html" class="sidebar-logo">ML Notes</a>
      </div>
      <nav class="sidebar-nav">
        
        <div class="nav-section eslr">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìä</span>
            ESLR
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../eslr/eslr-00.html">ESLR Notes</a></li>
            <li class="nav-item"><a href="../eslr/eslr-01-regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../eslr/eslr-02-classification.html">Classification</a></li>
            <li class="nav-item"><a href="../eslr/eslr-03-kernel-methods.html">Kernel Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-04-model-assessment.html">Model Assessment and Selection</a></li>
            <li class="nav-item"><a href="../eslr/eslr-08-model-selection.html">Model Inference and Averaging</a></li>
            <li class="nav-item"><a href="../eslr/eslr-09-additive-models.html">Additive Models, Trees, and Related Methods</a></li>
            <li class="nav-item"><a href="../eslr/eslr-10-boosting.html">Boosting and Additive Trees</a></li>
            <li class="nav-item"><a href="../eslr/eslr-15-random-forest.html">Random Forests</a></li>
          </ul>
        </div>
        <div class="nav-section general">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üß†</span>
            General
          </div>
          <ul class="nav-items">
            <li class="nav-item"><a href="gen-00.html">General ML Notes</a></li>
            <li class="nav-item"><a href="gen-01-basic-statistics.html" class="active">Basic Statistics</a></li>
            <li class="nav-item"><a href="gen-02-decision_trees.html">Decision Trees</a></li>
            <li class="nav-item"><a href="gen-03-boosting.html">Boosting</a></li>
            <li class="nav-item"><a href="gen-04-xgboost.html">XGBoost</a></li>
            <li class="nav-item"><a href="gen-05-clustering.html">Clustering</a></li>
            <li class="nav-item"><a href="gen-06-support_vector_machines.html">Support Vector Machines</a></li>
            <li class="nav-item"><a href="gen-07-dimensionality_reduction.html">Dimensionality Reduction</a></li>
            <li class="nav-item"><a href="gen-08-regression.html">Regression</a></li>
          </ul>
        </div>
        <div class="nav-section jurafsky">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üí¨</span>
            Jurafsky
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../jurafsky/jfsky-00.html">Speech and Language Processing Notes</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-01-regex.html">Regular Expressions and Text Processing</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-02-tokenization.html">N-Grams and Language Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-03-vectors.html">Vector Semantics and Word Embeddings</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-04-sequence.html">Sequence Architectures: RNNs, LSTMs, and Attention</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-05-encoder.html">Encoder-Decoder Models</a></li>
            <li class="nav-item"><a href="../jurafsky/jfsky-06-transfer.html">Transfer Learning and Pre-trained Models</a></li>
          </ul>
        </div>
        <div class="nav-section probml">
          <div class="nav-section-title" onclick="this.nextElementSibling.classList.toggle('collapsed')">
            <span class="icon">üìà</span>
            ProbML
          </div>
          <ul class="nav-items collapsed">
            <li class="nav-item"><a href="../probml/probml-00.html">Probabilistic Machine Learning Notes</a></li>
            <li class="nav-item"><a href="../probml/probml-01-introduction.html">Introduction to Machine Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-02-probability.html">Probability Foundations</a></li>
            <li class="nav-item"><a href="../probml/probml-03-probability.html">Probability: Advanced Topics</a></li>
            <li class="nav-item"><a href="../probml/probml-04-statistics.html">Statistics</a></li>
            <li class="nav-item"><a href="../probml/probml-05-decision_theory.html">Decision Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-06-information_theory.html">Information Theory</a></li>
            <li class="nav-item"><a href="../probml/probml-08-optimization.html">Optimization</a></li>
            <li class="nav-item"><a href="../probml/probml-09-discriminant_analysis.html">Discriminant Analysis</a></li>
            <li class="nav-item"><a href="../probml/probml-10-logistic_regression.html">Logistic Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-11-linear_regression.html">Linear Regression</a></li>
            <li class="nav-item"><a href="../probml/probml-13-ffnn.html">Feed-Forward Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-14-cnn.html">Convolutional Neural Networks</a></li>
            <li class="nav-item"><a href="../probml/probml-15-rnn.html">Recurrent Neural Networks and Transformers</a></li>
            <li class="nav-item"><a href="../probml/probml-16-exemplar.html">Exemplar-Based Methods</a></li>
            <li class="nav-item"><a href="../probml/probml-18-trees.html">Decision Trees and Ensembles</a></li>
            <li class="nav-item"><a href="../probml/probml-19-ssl.html">Self-Supervised and Semi-Supervised Learning</a></li>
            <li class="nav-item"><a href="../probml/probml-21-recsys.html">Recommendation Systems</a></li>
          </ul>
        </div>
      </nav>
    </aside>
    
    <header class="mobile-header">
      <a href="../index.html" class="sidebar-logo">ML Notes</a>
      <button class="mobile-menu-btn" onclick="document.getElementById('sidebar').classList.toggle('open'); document.getElementById('overlay').classList.toggle('visible')">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </header>
    <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.remove('open'); this.classList.remove('visible')"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <header class="page-header">
          <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <a href="index.html">General</a>
          </div>
          <h1 class="page-title">Basic Statistics</h1>
          <div class="page-meta"><span class="tag">General</span></div>
        </header>
        <article class="content">
          <h1 id="basic-statistics">Basic Statistics</h1>
<p>Statistics is the foundation of machine learning. Before any model can learn patterns, we need to understand the data itself‚Äîhow to collect it properly, describe it meaningfully, and make valid inferences from samples to populations.</p>
<h2 id="sampling-and-measurement">Sampling and Measurement</h2>
<p>When we collect data, we&#39;re measuring characteristics (called <strong>variables</strong>) of our subjects:</p>
<p><strong>Types of Variables</strong>:</p>
<ul>
<li><strong>Quantitative</strong> (Numerical): Things we can measure with numbers (height, temperature, income)</li>
<li><strong>Categorical</strong> (Qualitative): Things we describe with categories (color, species, disease status)</li>
</ul>
<p><strong>Measurement Scales</strong>‚Äîdifferent types require different statistical treatments:</p>
<ul>
<li><strong>Interval Scale</strong> (Quantitative): Differences are meaningful, but no true zero (temperature in Celsius‚Äî0¬∞C doesn&#39;t mean &quot;no temperature&quot;)</li>
<li><strong>Nominal Scale</strong> (Qualitative): Unordered categories (eye color, blood type)‚Äîcan only check equality</li>
<li><strong>Ordinal Scale</strong> (Qualitative): Ordered categories (education level, Likert scale ratings)‚Äîcan rank but can&#39;t quantify differences</li>
</ul>
<p><strong>The Sampling Problem</strong>: Any statistic we compute from a sample will vary from sample to sample. This variation comes from two sources:</p>
<ul>
<li><strong>Sampling Error</strong>: Unavoidable random variation from using a sample instead of the whole population</li>
<li><strong>Sampling Bias</strong>: Systematic errors in how we collected the sample<ul>
<li><em>Selection Bias</em>: Some population members more likely to be included (surveying only online users)</li>
<li><em>Response Bias</em>: People don&#39;t answer truthfully (socially desirable responses)</li>
<li><em>Non-Response Bias</em>: Certain types of people don&#39;t respond (busy people skip surveys)</li>
</ul>
</li>
</ul>
<p><strong>Sampling Methods</strong>:</p>
<ul>
<li><strong>Simple Random Sampling</strong>: Each data point has equal probability of being selected‚Äîthe gold standard but sometimes impractical</li>
<li><strong>Stratified Random Sampling</strong>: Divide population into meaningful subgroups (strata), then sample from each<ul>
<li>Ensures representation from all important subgroups</li>
<li>Often produces lower variance estimates than simple random sampling</li>
<li>Example: Stratifying by age groups when studying health outcomes</li>
</ul>
</li>
<li><strong>Cluster Sampling</strong>: Divide into clusters (often geographic), randomly select clusters, sample within them<ul>
<li>More practical for large geographic areas</li>
<li>Example: Randomly selecting cities, then sampling households within those cities</li>
</ul>
</li>
<li><strong>Multi-Stage Sampling</strong>: Combination of sampling methods at different stages</li>
</ul>
<h2 id="descriptive-statistics">Descriptive Statistics</h2>
<p>Descriptive statistics summarize what the data looks like before we make inferences.</p>
<p><strong>Measures of Central Tendency</strong>:</p>
<ul>
<li><strong>Mean</strong> ($\bar{x}$): The arithmetic average‚Äîsensitive to outliers</li>
<li><strong>Median</strong>: The middle value when sorted‚Äîrobust to outliers</li>
<li><strong>Mode</strong>: The most frequent value‚Äîuseful for categorical data</li>
</ul>
<p><strong>Shape of Distribution</strong>‚Äîunderstanding where mean and median fall:</p>
<ul>
<li><strong>Symmetric</strong>: Mean coincides with median (normal distribution is symmetric)</li>
<li><strong>Left Skewed</strong> (negatively skewed): Long left tail ‚Üí Mean &lt; Median<ul>
<li>Example: Age at retirement (most retire around 65, some retire very young)</li>
</ul>
</li>
<li><strong>Right Skewed</strong> (positively skewed): Long right tail ‚Üí Mean &gt; Median<ul>
<li>Example: Income distribution (most earn moderate amounts, few earn millions)</li>
</ul>
</li>
<li><strong>Key insight</strong>: The mean is &quot;pulled&quot; toward the long tail</li>
</ul>
<p><strong>Standard Deviation</strong>‚Äîmeasures spread around the mean:</p>
<ul>
<li>Deviation = how far each observation is from the mean</li>
<li>$s = \sqrt{\frac{\sum (x_i - \bar{x})^2}{N-1}}$</li>
<li>Why $N-1$? This is &quot;Bessel&#39;s correction&quot;‚Äîusing $N$ would underestimate the true population variance</li>
</ul>
<p><strong>The 68-95-99.7 Rule</strong> (Empirical Rule) for normally distributed data:</p>
<ul>
<li>~68% of data falls within 1 standard deviation of the mean</li>
<li>~95% of data falls within 2 standard deviations</li>
<li>~99.7% of data falls within 3 standard deviations (the &quot;3-sigma rule&quot;)</li>
</ul>
<p><strong>Interquartile Range (IQR)</strong>:</p>
<ul>
<li>IQR = 75th percentile ‚àí 25th percentile (the middle 50% of data)</li>
<li>Common outlier rule: Points beyond $1.5 \times \text{IQR}$ from Q1 or Q3 are potential outliers</li>
</ul>
<h2 id="probability">Probability</h2>
<p>Probability quantifies uncertainty. Here&#39;s how we formalize it:</p>
<p><strong>Expected Value</strong> (what you&#39;d &quot;expect&quot; on average):</p>
<ul>
<li>$E(X) = \sum_i x_i \times p(X=x_i)$</li>
<li>This is the first moment about the origin‚Äîthe &quot;center of mass&quot; of the distribution</li>
</ul>
<p><strong>Variance</strong> (how spread out the values are):</p>
<ul>
<li>$V(X) = E(X^2) - (E(X))^2 = E[(X - \mu)^2]$</li>
<li>This is the second moment about the mean</li>
</ul>
<p><strong>Z-score</strong> (standardization):</p>
<ul>
<li>$z = \frac{y - \mu}{\sigma}$</li>
<li>Transforms any normal distribution to the <strong>Standard Normal Distribution</strong> $\sim N(0,1)$</li>
<li>Interpretation: &quot;How many standard deviations away from the mean?&quot;</li>
</ul>
<p><strong>Covariance and Correlation</strong>:</p>
<ul>
<li>Covariance: $Cov(X,Y) = E[(X - \mu_x)(Y - \mu_y)]$‚Äîdirection of linear relationship</li>
<li>Correlation: $\rho = \frac{Cov(X,Y)}{\sigma_x \sigma_y} = E(z_x z_y)$‚Äîstandardized covariance, ranges from -1 to +1</li>
</ul>
<p>**The Central Limit Theorem (CLT)**‚Äîone of the most important results in statistics:</p>
<ul>
<li>Sample means follow an approximately normal distribution regardless of the population distribution</li>
<li>$\bar{X} \sim N(\mu, \frac{\sigma}{\sqrt{N}})$</li>
<li><strong>Standard Error</strong> = $\frac{\sigma}{\sqrt{N}}$‚Äîthe standard deviation of the sampling distribution</li>
<li>Key insight: Standard error decreases with $\sqrt{N}$, not $N$ (need 4x samples to halve error)</li>
</ul>
<p><strong>Example: Exit Poll Survey</strong></p>
<ul>
<li>Binary outcome (vote A or B), assume $p = 0.5$, sample size $N = 1800$</li>
<li>Standard deviation: $\sqrt{p(1-p)} = 0.5$</li>
<li>Standard error: $\frac{0.5}{\sqrt{1800}} \approx 0.012$</li>
<li>99% CI: $0.5 \pm 3 \times 0.012 \approx (0.47, 0.53)$</li>
</ul>
<p><strong>Example: Income Survey</strong></p>
<ul>
<li>Population: $\sim N(380, 80^2)$ (mean $380K, SD $80K), sample size $N = 100$</li>
<li>Question: What&#39;s $P(\bar{y} \geq 400)$?</li>
<li>Standard error: $\frac{80}{\sqrt{100}} = 8$</li>
<li>$z = \frac{400 - 380}{8} = 2.5$</li>
<li>$P(Z \geq 2.5) &lt; 0.006$ (very unlikely to see sample mean ‚â• 400 by chance)</li>
</ul>
<h2 id="confidence-interval">Confidence Interval</h2>
<p>Confidence intervals quantify our uncertainty about parameter estimates.</p>
<p><strong>Point Estimate</strong>: A single number as our best guess (e.g., sample mean $\bar{x}$ for population mean $\mu$)</p>
<p><strong>Properties of Good Estimators</strong>:</p>
<ul>
<li><strong>Unbiased</strong>: $E(\bar{X}) = \mu$ (on average, the estimator equals the true value)</li>
<li><strong>Efficient/Consistent</strong>: $N \to \infty \implies V(\bar{X}) \to 0$ (variance shrinks as sample grows)</li>
</ul>
<p><strong>Interval Estimate</strong>: A range of plausible values</p>
<ul>
<li>CI = Point Estimate ¬± Margin of Error</li>
<li><strong>Confidence Level</strong>: The probability (e.g., 95%) that the procedure produces an interval containing the true parameter</li>
<li>Important: A 95% CI doesn&#39;t mean &quot;95% probability the parameter is in this interval&quot;‚Äîthe parameter is fixed, not random!</li>
</ul>
<p><strong>CI for Proportions</strong>:</p>
<ul>
<li>Point estimate: $\hat{\pi}$</li>
<li>Standard error: $\sqrt{\frac{\hat{\pi}(1-\hat{\pi})}{N}}$</li>
<li>95% CI: $\hat{\pi} \pm z_{0.025} \times se$ where $z_{0.025} \approx 1.96$</li>
</ul>
<p><strong>CI for Means</strong>:</p>
<ul>
<li>Point estimate: $\bar{X}$</li>
<li>When population variance is unknown, use sample variance and <strong>t-distribution</strong> instead of z-distribution</li>
<li>The t-distribution has heavier tails‚Äîaccounts for extra uncertainty from estimating variance</li>
<li>As $N \to \infty$, t-distribution approaches normal distribution</li>
<li>CI: $\bar{X} \pm t_{n-1,\alpha/2} \times \frac{s}{\sqrt{N}}$</li>
</ul>
<p><strong>Sample Size Determination</strong>:</p>
<ul>
<li>For proportions: $N = \frac{\pi(1-\pi) \times z^2}{M^2}$ where $M$ is desired margin of error</li>
<li>For means: $N = \frac{\sigma^2 \times z^2}{M^2}$</li>
<li>Note: Quadrupling sample size only halves the margin of error!</li>
</ul>
<p><strong>Estimation Methods</strong>:</p>
<ul>
<li><strong>Maximum Likelihood Estimation (MLE)</strong>: Find parameter values that maximize the probability of observing the data we actually observed</li>
<li><strong>Bootstrap</strong>: Resample from observed data to estimate standard errors and CIs‚Äîno distributional assumptions needed</li>
</ul>
<h2 id="significance-test">Significance Test</h2>
<p>Hypothesis testing provides a framework for making decisions based on data.</p>
<p><strong>Five Components of a Significance Test</strong>:</p>
<ol>
<li><p><strong>Assumptions</strong>: What conditions must hold?</p>
<ul>
<li>Type of data, randomization, population distribution, sample size</li>
</ul>
</li>
<li><p><strong>Hypotheses</strong>:</p>
<ul>
<li>$H_0$ (Null): The &quot;no effect&quot; or &quot;status quo&quot; hypothesis</li>
<li>$H_1$ (Alternative): What we&#39;re testing for</li>
</ul>
</li>
<li><p><strong>Test Statistic</strong>: Quantifies how far the observed data falls from what $H_0$ predicts</p>
</li>
<li><p><strong>P-value</strong>: Probability of observing data as extreme or more extreme than what we got, <em>assuming $H_0$ is true</em></p>
<ul>
<li>Small p-value ‚Üí data is unlikely under $H_0$ ‚Üí evidence against $H_0$</li>
<li>P-value is NOT the probability that $H_0$ is true!</li>
</ul>
</li>
<li><p><strong>Conclusion</strong>: &quot;Reject $H_0$&quot; or &quot;Fail to reject $H_0$&quot;</p>
<ul>
<li>Note: We never &quot;accept $H_0$&quot;‚Äîabsence of evidence is not evidence of absence</li>
</ul>
</li>
</ol>
<p><strong>Testing Proportions</strong>:</p>
<ul>
<li>$H_0: \pi = \pi_0$ vs $H_1: \pi \neq \pi_0$</li>
<li>Test statistic: $z = \frac{\hat{\pi} - \pi_0}{se}$ where $se = \sqrt{\frac{\pi_0(1-\pi_0)}{N}}$</li>
</ul>
<p><strong>Testing Means</strong>:</p>
<ul>
<li>$H_0: \mu = \mu_0$ vs $H_1: \mu \neq \mu_0$</li>
<li>Test statistic: $t = \frac{\bar{X} - \mu_0}{s/\sqrt{N}}$</li>
<li>For small samples, use exact binomial distribution</li>
</ul>
<p><strong>One-Tailed vs Two-Tailed Tests</strong>:</p>
<ul>
<li>One-tailed: Tests deviation in one direction only</li>
<li>More powerful for detecting effects in that direction</li>
<li>Risky: Can&#39;t detect effects in the opposite direction</li>
<li>Use only with strong prior justification</li>
</ul>
<p><strong>Types of Errors</strong>:</p>
<ul>
<li><strong>Type I Error</strong> (False Positive): Reject $H_0$ when it&#39;s actually true<ul>
<li>Probability = significance level ($\alpha$, often 0.05)</li>
</ul>
</li>
<li><strong>Type II Error</strong> (False Negative): Fail to reject $H_0$ when it&#39;s actually false<ul>
<li>Probability denoted $\beta$</li>
</ul>
</li>
<li><strong>Power</strong> = $1 - \beta$ = probability of correctly rejecting false $H_0$</li>
</ul>
<p><strong>Trade-offs</strong>:</p>
<ul>
<li>Decreasing Type I error increases Type II error</li>
<li>The closer the true parameter is to $H_0$, the lower the power</li>
<li>Larger samples ‚Üí more power</li>
</ul>
<p><strong>Important Warning</strong>: Statistical significance ‚â† practical significance. With large enough samples, tiny, meaningless differences become &quot;statistically significant.&quot;</p>
<h2 id="comparison-of-groups">Comparison of Groups</h2>
<p>Most research involves comparing groups‚Äîtreatments vs control, different populations, etc.</p>
<p><strong>Difference in Means (Independent Samples)</strong>:</p>
<ul>
<li>Goal: Test if $\mu_1 - \mu_2 = 0$</li>
<li>Estimate: $\bar{y}_1 - \bar{y}_2$</li>
<li>Standard error: $se = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}$</li>
<li>CI: $(\bar{y}_1 - \bar{y}_2) \pm t \times se$</li>
<li>Test statistic: $t = \frac{(\bar{y}_1 - \bar{y}_2) - 0}{se}$ with $df = n_1 + n_2 - 2$</li>
</ul>
<p><strong>Equal Variance Assumption</strong> (Pooled Variance):</p>
<ul>
<li>If we assume $\sigma_1 = \sigma_2$, we can pool the data to get a better variance estimate:</li>
<li>$s_{pooled} = \sqrt{\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}}$</li>
<li>$se = s_{pooled}\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}$</li>
</ul>
<p><strong>Difference in Proportions</strong>:</p>
<ul>
<li>Standard error: $se = \sqrt{\frac{\hat{\pi}_1(1-\hat{\pi}_1)}{n_1} + \frac{\hat{\pi}_2(1-\hat{\pi}_2)}{n_2}}$</li>
<li>For significance test, pool proportions under $H_0$:</li>
<li>$\hat{\pi}_{pooled}$ and $se = \sqrt{\hat{\pi}(1-\hat{\pi})\left(\frac{1}{n_1} + \frac{1}{n_2}\right)}$</li>
</ul>
<p><strong>Paired/Matched Samples</strong>:</p>
<ul>
<li>Same subject measured at different times (before/after)</li>
<li>Controls for between-subject variation</li>
<li>Analyze the <em>differences</em> directly</li>
<li>Test: $t = \frac{\bar{d} - 0}{s_d/\sqrt{n}}$</li>
<li>Example: Blood pressure before and after treatment for the same patients</li>
</ul>
<p><strong>Effect Size</strong>: Standardized measure of the magnitude of difference</p>
<ul>
<li>Cohen&#39;s d: $d = \frac{\bar{y}_1 - \bar{y}<em>2}{s</em>{pooled}}$</li>
<li>Interpretation: Small (<del>0.2), Medium (</del>0.5), Large (~0.8)</li>
</ul>
<p><strong>McNemar Test</strong> (for paired proportions):</p>
<table>
<thead>
<tr>
<th></th>
<th>Treatment Yes</th>
<th>Treatment No</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Control Yes</strong></td>
<td>$n_{11}$</td>
<td>$n_{12}$</td>
</tr>
<tr>
<td><strong>Control No</strong></td>
<td>$n_{21}$</td>
<td>$n_{22}$</td>
</tr>
</tbody></table>
<ul>
<li>Test statistic: $z = \frac{n_{12} - n_{21}}{\sqrt{n_{12} + n_{21}}}$</li>
<li>Only the discordant pairs ($n_{12}$ and $n_{21}$) matter!</li>
</ul>
<p><strong>Non-Parametric Alternatives</strong> (when normality is violated):</p>
<ul>
<li><strong>Wilcoxon Signed-Rank Test</strong>: For paired data‚Äîranks the absolute differences</li>
<li><strong>Mann-Whitney U Test</strong>: For independent samples‚Äîcompares ranks between groups</li>
</ul>
<h2 id="association-between-categorical-variables">Association between Categorical Variables</h2>
<p>When both variables are categorical, we analyze their relationship through contingency tables.</p>
<p><strong>Statistical Independence</strong>: Variables are independent if knowing one tells you nothing about the other. Mathematically: $P(A|B) = P(A)$</p>
<p><strong>Chi-Square Test</strong>:</p>
<ol>
<li>Calculate expected frequencies under independence:
$f_e = \frac{\text{row total} \times \text{column total}}{\text{grand total}}$</li>
<li>Compare to observed frequencies:
$\chi^2 = \sum \frac{(f_o - f_e)^2}{f_e}$</li>
<li>Degrees of freedom: $(r-1) \times (c-1)$</li>
</ol>
<p><strong>Important</strong>: $\chi^2$ tells you <em>if</em> there&#39;s an association, not <em>how strong</em> it is!</p>
<p><strong>Residual Analysis</strong>‚Äîwhich cells drive the association?</p>
<ul>
<li>Standardized residual: $z = \frac{f_o - f_e}{\sqrt{f_e(1 - \text{row%})(1 - \text{col%})}}$</li>
<li>$|z| &gt; 2$ suggests the cell is significantly different from expected</li>
</ul>
<p><strong>Odds Ratio</strong> (2√ó2 tables):</p>
<ul>
<li>$\theta = \frac{n_{11} \times n_{22}}{n_{12} \times n_{21}}$</li>
<li>Interpretation:<ul>
<li>$\theta = 1$: No association (equal odds)</li>
<li>$\theta &gt; 1$: Higher odds for row 1</li>
<li>$\theta &lt; 1$: Lower odds for row 1</li>
</ul>
</li>
<li>Example: If odds ratio for smoking and lung cancer is 10, smokers have 10√ó the odds of lung cancer</li>
</ul>
<p><strong>Ordinal Variables</strong>‚Äîwhen categories have a natural order:</p>
<ul>
<li><strong>Concordant pairs</strong>: Higher on X corresponds to higher on Y</li>
<li><strong>Discordant pairs</strong>: Higher on X corresponds to lower on Y</li>
<li><strong>Gamma coefficient</strong>: $\gamma = \frac{C - D}{C + D}$<ul>
<li>Ranges from -1 to +1, like correlation for ordinal data</li>
</ul>
</li>
</ul>
<h2 id="p-value-interpretation-and-common-misconceptions">P-value Interpretation and Common Misconceptions</h2>
<p>Understanding what the p-value actually means is crucial:</p>
<p><strong>What p-value IS</strong>:</p>
<ul>
<li>The probability of obtaining a test statistic at least as extreme as observed, <em>given that the null hypothesis is true</em></li>
<li>A measure of how compatible the data is with the null hypothesis</li>
</ul>
<p><strong>What p-value is NOT</strong>:</p>
<ul>
<li>‚ùå The probability that the null hypothesis is true</li>
<li>‚ùå The probability that results occurred &quot;by chance&quot;</li>
<li>‚ùå The probability of making a Type I error (that&#39;s $\alpha$, which you set beforehand)</li>
</ul>
<p><strong>Guidelines for Interpretation</strong>:</p>
<ul>
<li>Small p-value (e.g., &lt; 0.05): Data is unlikely under $H_0$‚Äîevidence against $H_0$</li>
<li>Large p-value: Data is compatible with $H_0$‚Äîbut doesn&#39;t prove $H_0$ is true</li>
<li>Always report effect sizes alongside p-values</li>
<li>Consider practical significance, not just statistical significance</li>
</ul>

        </article>
        <nav class="page-navigation">
        <a href="gen-00.html" class="nav-link prev">
          <span class="nav-link-label">‚Üê Previous</span>
          <span class="nav-link-title">General ML Notes</span>
        </a>
        <a href="gen-02-decision_trees.html" class="nav-link next">
          <span class="nav-link-label">Next ‚Üí</span>
          <span class="nav-link-title">Decision Trees</span>
        </a></nav>
      </div>
    </main>
  </div>
  <script>
    document.addEventListener('scroll', function() {
      const btn = document.querySelector('.back-to-top');
      if (btn) btn.classList.toggle('visible', window.scrollY > 300);
    });
  </script>
</body>
</html>