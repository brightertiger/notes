
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Probml 15 Rnn | My Notes</title>
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap">
  <!-- MathJax for LaTeX support -->
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: {
        enableMenu: false
      }
    };
  </script>
</head>
<body>
  <div class="container">
    <header>
      <h1>Probml 15 Rnn</h1>
      
      <a href="/" class="home-link">← Back to Home</a>
    </header>
    <main class="content">
      <h1 id="recurrent-neural-networks">Recurrent Neural Networks</h1>
<ul>
<li><p>RNNs are designed to process sequential data by maintaining internal state</p>
</li>
<li><p>Unlike feedforward networks, RNNs share parameters across different time steps</p>
</li>
<li><p>The hidden state carries information across the sequence, acting as memory</p>
</li>
<li><p>Core Recurrent Cell</p>
<ul>
<li>Basic RNN update: $h_t = \phi(W_{xh}x_t + W_{hh}h_{t-1} + b_h)$<ul>
<li>$W_{xh}$: Input-to-hidden weights</li>
<li>$W_{hh}$: Hidden-to-hidden weights (recurrent connections)</li>
<li>$h_{t-1}$: Previous hidden state</li>
<li>$\phi$: Activation function (typically tanh)</li>
</ul>
</li>
</ul>
</li>
<li><p>Types of Sequence Processing Tasks</p>
<ul>
<li><p>Seq2Seq (sequence generation)</p>
<ul>
<li>Maps fixed input to variable-length output sequence</li>
<li>Examples: Image captioning, machine translation</li>
<li>Autoregressive generation: Each output depends on previous outputs</li>
</ul>
</li>
<li><p>Seq2Vec (sequence classification)</p>
<ul>
<li>Maps variable-length input to fixed output vector</li>
<li>Examples: Sentiment analysis, document classification</li>
<li>Often uses final hidden state or aggregation of all states</li>
</ul>
</li>
<li><p>Vec2Seq (conditioned generation)</p>
<ul>
<li>Maps fixed input to variable-length output sequence</li>
<li>Example: Generate text conditioned on a topic vector</li>
</ul>
</li>
<li><p>Seq2Seq (sequence-to-sequence)</p>
<ul>
<li>Maps variable-length input to variable-length output</li>
<li>Examples: Machine translation, summarization</li>
<li>Typically employs encoder-decoder architecture</li>
</ul>
</li>
</ul>
</li>
<li><p>Bidirectional RNNs</p>
<ul>
<li>Process sequence in both forward and backward directions</li>
<li>Captures both past and future context for each position</li>
<li>Forward hidden states: $\vec{h}<em>t = \phi(W</em>{xh}^{\rightarrow}x_t + W_{hh}^{\rightarrow}\vec{h}_{t-1})$</li>
<li>Backward hidden states: $\overleftarrow{h}<em>t = \phi(W</em>{xh}^{\leftarrow}x_t + W_{hh}^{\leftarrow}\overleftarrow{h}_{t+1})$</li>
<li>Final representation combines both directions: $h_t = [\vec{h}_t; \overleftarrow{h}_t]$</li>
</ul>
</li>
<li><p>Challenges with Basic RNNs</p>
<ul>
<li>Vanishing Gradients: Signal from distant time steps diminishes exponentially</li>
<li>Exploding Gradients: Gradients grow uncontrollably (solved with gradient clipping)</li>
<li>Limited context window: Difficulty capturing long-range dependencies</li>
</ul>
</li>
<li><p>Advanced RNN Architectures</p>
<ul>
<li><p>LSTM (Long Short-Term Memory)</p>
<ul>
<li>Explicitly designed to capture long-term dependencies</li>
<li>Cell state ($C_t$) acts as conveyor belt of information through time</li>
<li>Three gates control information flow:<ul>
<li>Input gate ($I_t$): Controls what new information enters the cell</li>
<li>Forget gate ($F_t$): Controls what information is discarded</li>
<li>Output gate ($O_t$): Controls what information is exposed as output</li>
</ul>
</li>
<li>LSTM equations:<ul>
<li>$I_t = \sigma(W_{ix}X_t + W_{ih}H_{t-1})$</li>
<li>$F_t = \sigma(W_{fx}X_t + W_{fh}H_{t-1})$</li>
<li>$O_t = \sigma(W_{ox}X_t + W_{oh}H_{t-1})$</li>
<li>$\tilde{C}<em>t = \tanh(W</em>{cx}X_t + W_{ch}H_{t-1})$ (candidate cell state)</li>
<li>$C_t = F_t \odot C_{t-1} + I_t \odot \tilde{C}_t$ (cell state update)</li>
<li>$H_t = O_t \odot \tanh(C_t)$ (hidden state)</li>
</ul>
</li>
<li>Solves vanishing gradient through additive updates and gating</li>
</ul>
</li>
<li><p>GRU (Gated Recurrent Unit)</p>
<ul>
<li>Simplified version of LSTM with fewer parameters</li>
<li>Has two gates: update gate and reset gate</li>
<li>Update gate controls how much previous state is retained</li>
<li>Reset gate controls how much previous state influences candidate state</li>
<li>Competitive performance with LSTM but more efficient</li>
</ul>
</li>
</ul>
</li>
<li><p>Backpropagation through Time (BPTT)</p>
<ul>
<li>Unrolling the computation graph along time axis</li>
<li>$h_t = W_{hx}x_t + W_{hh}h_{t-1} = f(x_t, h_{t-1}, w_h)$</li>
<li>$o_t = W_{ho}h_t = g(h_t, w_{oh})$</li>
<li>$L = {1 \over T}\sum l(y_t, o_t)$</li>
<li>${\delta L \over \delta w_h} = {1 \over T} \sum {\delta l \over \delta w_h}$</li>
<li>${\delta L \over \delta w_h} = {1 \over T} \sum {\delta l \over \delta o_t} {\delta o_t \over \delta h_t} {\delta h_t \over \delta w_h}$</li>
<li>${\delta h_t \over \delta w_h} = {\delta h_t \over \delta w_h} + {\delta h_t \over \delta h_{t-1}} {\delta h_{t-1} \over \delta w_h}$</li>
<li>Common to truncate the update to length of the longest subsequence in the batch</li>
<li>As the sequence goes forward, the hidden state keeps getting multiplied by W(hh)</li>
<li>Gradients can decay or explode as we go backwards in time</li>
<li>Solution is to use additive rather than multiplicative updates</li>
</ul>
</li>
<li><p>Decoding</p>
<ul>
<li>Output is generated one token at a time</li>
<li>Simple Solution: Greedy Decoding<ul>
<li>Argmax over vocab at each step</li>
<li>Keep sampling unless <EOS> token output</li>
</ul>
</li>
<li>May not be globally optimal path</li>
<li>Alternative: Beam Search<ul>
<li>Compute top-K candidate outputs at each step</li>
<li>Expand each one in V possible ways</li>
<li>Total VK candidates generated</li>
</ul>
</li>
<li>GPT used top-k and top-p sampling<ul>
<li>Top-K sampling: Redistribute the probability mass</li>
<li>Top-P sampling: Sample till the cumulative probability exceeds p</li>
</ul>
</li>
</ul>
</li>
<li><p>Attention</p>
<ul>
<li>In RNNs, hidden state linearly combines the inputs and then sends them to an activation function</li>
<li>Attention mechanism allows for more flexibility.<ul>
<li>Suppose there are m feature vectors or values</li>
<li>Model decides which to use based on the input query vector q and its similarity to a set of m keys</li>
<li>If query is most similar to key i, then we use value i.</li>
</ul>
</li>
<li>Attention acts as a soft dictionary lookup<ul>
<li>Compare query q to each key k(i)</li>
<li>Retrieve the corresponding value v(i)</li>
<li>To make the operation differentiable:<ul>
<li>Compute a convex combination</li>
</ul>
</li>
<li>$Attn(q,(k_1,v_1),(k_2, v_2)...,(k_m,v_m)) = \sum_{i=1}^m \alpha_i (q, {k_i}) v_i$<ul>
<li>$\alpha_i (q, {k_i})$ are the attention weights</li>
</ul>
</li>
<li>Attention weights are computed from an attention score function $a(q,k_i)$<ul>
<li>Computes the similarity between query and key</li>
</ul>
</li>
<li>Once the scores are computed, use soft max to impose distribution</li>
<li>Masking helps in ignoring the index which are invalid while computing soft max</li>
<li>For computational efficiency, set the dim of query and key to be same (say d)<ul>
<li>The similarity is given by dot product</li>
<li>The weights are randomly initialized</li>
<li>The expected variance of dot product will be d.</li>
<li>Scale the dot product by $\sqrt d$</li>
<li>Scaled Dot-Product Attention<ul>
<li>Attention Weight: $a(q,k) = {q^Tk \over \sqrt d}$</li>
<li>Scaled Dot Product Attention: $Attn(Q,K,V) =  S({QK^T \over \sqrt d})V$</li>
</ul>
</li>
</ul>
</li>
<li>Example: Seq2Seq with Attention<ul>
<li>Consider encoder-decoder architecture</li>
<li>In the decoder:<ul>
<li>$h_t = f(h_{t-1}, c)$</li>
<li>c is the context vector from encoder</li>
<li>Usually the last hidden state of the encoder</li>
</ul>
</li>
<li>Attention allows the decoder to look at all the input words<ul>
<li>Better alignment between source and target</li>
</ul>
</li>
<li>Make the context dynamic<ul>
<li>Query: previous hidden state of the decoder</li>
<li>Key: all the hidden states from the encoder</li>
<li>Value: all the hidden states from the encoder</li>
<li>$c_t = \sum_{i=1}^T \alpha_i(h_{t-1}^d, {h_i^e})h_i^e$</li>
</ul>
</li>
<li>If RNN has multiple hidden layers, usually take the top most layer</li>
<li>Can be extended to Seq2Vec models</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Transformers</p>
<ul>
<li>Transformers are seq2seq models using attention in both encoder and decoder steps</li>
<li>Eliminate the need for RNNs</li>
<li>Self Attention:<ul>
<li>Modify the encoder such that it attends to itself</li>
<li>Given a sequence of input tokens $[x_1, x_2, x_3...,x_n]$</li>
<li>Sequence of output tokens: $y_i = Attn(x_i, (x_1,x_1), (x_2, x_2)...,(x_n, x_n))$<ul>
<li>Query is xi</li>
<li>Keys and Values are are x1,x2…xn (all valid inputs)</li>
</ul>
</li>
<li>In the decoder step:<ul>
<li>$y_i = Attn(y_{i-1}, (y_1,y_1), (y_2, y_2)...(y_{i-1}, y_{i-1}))$</li>
<li>Each new token generated has access to all the previous output</li>
</ul>
</li>
</ul>
</li>
<li>Multi-Head Attention<ul>
<li>Use multiple attention matrices to capture different nuances and similarities</li>
<li>$h_i = Attn(W_i^q q_i, (W_i^k k_i, W_i^v v_i))$</li>
<li>Stack all the heads together and use a projection matrix to get he output</li>
<li>Set $p_q h = p_k h = p_v h = p_o$ for parallel computation **How?</li>
</ul>
</li>
<li>Positional Encoding<ul>
<li>Attention is permutation invariant</li>
<li>Positional encodings help overcome this</li>
<li>Sinusoidal Basis</li>
<li>Positional Embeddings are combined with original input X → X + P</li>
</ul>
</li>
<li>Combining All the Blocks<ul>
<li>Encoder<ul>
<li>Input: $ Z = LN(MHA(X,X,X) + X$</li>
<li>Encoder: $E = LN(FF(Z) + Z)$<ul>
<li>For the first layer:<ul>
<li>$ Z = \text{POS}(\text{Embed}(X))$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>In general, model has N copies of the encoder</li>
<li>Decoder <ul>
<li>Has access to both: encoder and previous tokens</li>
<li>Input: $ Z = LN(MHA(X,X,X) + X$</li>
<li>Input $ Z = LN(MHA(Z,E,E) + Z$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Representation Learning</p>
<ul>
<li>Contextual Word Embeddings<ul>
<li>Hidden state depends on all previous tokens</li>
<li>Use the latent representation for classification / other downstream tasks</li>
<li>Pre-train on a large corpus </li>
<li>Fine-tune on small task specific dataset</li>
<li>Transfer Learning</li>
</ul>
</li>
<li>ELMo<ul>
<li>Embeddings from Language Model</li>
<li>Fit two RNN models<ul>
<li>Left to Right</li>
<li>Right to Left</li>
</ul>
</li>
<li>Combine the hidden state representations to fetch embedding for each word</li>
</ul>
</li>
<li>BERT<ul>
<li>Bi-Directional Encoder Representations from Transformers</li>
<li>Pre-trained using Cloze task (MLM i.e. Masked Language Modeling)</li>
<li>Additional Objective: Next sentence Prediction</li>
</ul>
</li>
<li>GPT <ul>
<li>Generative Pre-training Transformer</li>
<li>Causal model using Masked Decoder</li>
<li>Train it as a language model on web text</li>
</ul>
</li>
<li>T5<ul>
<li>Text-to-Text Transfer Transformer</li>
<li>Single model to perform multiple tasks</li>
<li>Tell the task to perform as part of input sequence</li>
</ul>
</li>
</ul>
</li>
</ul>

    </main>
    <footer>
      <p>Generated with Markdown Notes Static Site Generator</p>
    </footer>
  </div>
</body>
</html>
  