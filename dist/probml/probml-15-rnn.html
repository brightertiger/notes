<!DOCTYPE html><html lang="en"><head><link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&amp;display=swap" rel="stylesheet">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Recurrent Neural Networks</title>
    <link rel="stylesheet" href="../styles.css">

    <!-- MathJax for equation rendering -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
    // MathJax configuration
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$']],
            displayMath: [['$$', '$$']]
        },
        svg: {
            fontCache: 'global'
        }
    };
    </script>
    </head>

<body>
    <div class="container">
        <aside class="sidebar">
    <a href="../index.html" class="home-link">
        <svg class="home-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path>
            <polyline points="9 22 9 12 15 12 15 22"></polyline>
        </svg>
    </a>
    <h2>Navigation</h2>
    <div class="nav-section">
        <h3>Eslr Notes</h3>
        <ul>
            <li><a href="eslr-00.html">Introduction</a></li>
<li><a href="eslr-01-regression.html">01-Regression</a></li>
<li><a href="eslr-02-classification.html">02-Classification</a></li>
<li><a href="eslr-03-kernel-methods.html">03-Kernel-Methods</a></li>
<li><a href="eslr-04-model-assessment.html">04-Model-Assessment</a></li>
<li><a href="eslr-08-model-selection.html">08-Model-Selection</a></li>
<li><a href="eslr-09-additive-models.html">09-Additive-Models</a></li>
<li><a href="eslr-10-boosting.html">10-Boosting</a></li>
<li><a href="eslr-15-random-forest.html">15-Random-Forest</a></li>
        </ul>
    </div><div class="nav-section">
        <h3>General Notes</h3>
        <ul>
            <li><a href="gen-00.html">00</a></li>
<li><a href="gen-01-basic-statistics.html">01-Basic-Statistics</a></li>
<li><a href="gen-02-decision_trees.html">02-Decision Trees</a></li>
<li><a href="gen-03-boosting.html">03-Boosting</a></li>
<li><a href="gen-04-xgboost.html">04-Xgboost</a></li>
<li><a href="gen-05-clustering.html">05-Clustering</a></li>
<li><a href="gen-06-support_vector_machines.html">06-Support Vector Machines</a></li>
<li><a href="gen-07-dimensionality_reduction.html">07-Dimensionality Reduction</a></li>
<li><a href="gen-08-regression.html">08-Regression</a></li>
        </ul>
    </div><div class="nav-section">
        <h3>Jurafsky Notes</h3>
        <ul>
            <li><a href="jfsky-00.html">00</a></li>
<li><a href="jfsky-01-regex.html">01-Regex</a></li>
<li><a href="jfsky-02-tokenization.html">02-Tokenization</a></li>
<li><a href="jfsky-03-vectors.html">03-Vectors</a></li>
<li><a href="jfsky-04-sequence.html">04-Sequence</a></li>
<li><a href="jfsky-05-encoder.html">05-Encoder</a></li>
<li><a href="jfsky-06-transfer.html">06-Transfer</a></li>
        </ul>
    </div><div class="nav-section">
        <h3>Probml Notes</h3>
        <ul>
            <li><a href="probml-00.html">Introduction</a></li>
<li><a href="probml-01-introduction.html">Introduction (Detailed)</a></li>
<li><a href="probml-02-probability.html">02-probability</a></li>
<li><a href="probml-03-probability.html">Probability (Advanced)</a></li>
<li><a href="probml-04-statistics.html">04-statistics</a></li>
<li><a href="probml-05-decision_theory.html">05-decision Theory</a></li>
<li><a href="probml-06-information_theory.html">06-information Theory</a></li>
<li><a href="probml-08-optimization.html">08-optimization</a></li>
<li><a href="probml-09-discriminant_analysis.html">09-discriminant Analysis</a></li>
<li><a href="probml-10-logistic_regression.html">10-logistic Regression</a></li>
<li><a href="probml-11-linear_regression.html">11-linear Regression</a></li>
<li><a href="probml-13-ffnn.html">13-ffnn</a></li>
<li><a href="probml-14-cnn.html">14-cnn</a></li>
        </ul>
    </div></aside>

        <main class="content"><h1>Recurrent Neural Networks</h1>
<ul>
<li><p>RNNs are designed to process sequential data by maintaining internal state</p>
</li>
<li><p>Unlike feedforward networks, RNNs share parameters across different time steps</p>
</li>
<li><p>The hidden state carries information across the sequence, acting as memory</p>
</li>
<li><p>Core Recurrent Cell</p>
<ul>
<li>Basic RNN update: $h_t = \phi(W_{xh}x_t + W_{hh}h_{t-1} + b_h)$<ul>
<li>$W_{xh}$: Input-to-hidden weights</li>
<li>$W_{hh}$: Hidden-to-hidden weights (recurrent connections)</li>
<li>$h_{t-1}$: Previous hidden state</li>
<li>$\phi$: Activation function (typically tanh)</li>
</ul>
</li>
</ul>
</li>
<li><p>Types of Sequence Processing Tasks</p>
<ul>
<li><p>Seq2Seq (sequence generation)</p>
<ul>
<li>Maps fixed input to variable-length output sequence</li>
<li>Examples: Image captioning, machine translation</li>
<li>Autoregressive generation: Each output depends on previous outputs</li>
</ul>
</li>
<li><p>Seq2Vec (sequence classification)</p>
<ul>
<li>Maps variable-length input to fixed output vector</li>
<li>Examples: Sentiment analysis, document classification</li>
<li>Often uses final hidden state or aggregation of all states</li>
</ul>
</li>
<li><p>Vec2Seq (conditioned generation)</p>
<ul>
<li>Maps fixed input to variable-length output sequence</li>
<li>Example: Generate text conditioned on a topic vector</li>
</ul>
</li>
<li><p>Seq2Seq (sequence-to-sequence)</p>
<ul>
<li>Maps variable-length input to variable-length output</li>
<li>Examples: Machine translation, summarization</li>
<li>Typically employs encoder-decoder architecture</li>
</ul>
</li>
</ul>
</li>
<li><p>Bidirectional RNNs</p>
<ul>
<li>Process sequence in both forward and backward directions</li>
<li>Captures both past and future context for each position</li>
<li>Forward hidden states: $\vec{h}_t = \phi(W_{xh}^{\rightarrow}x_t + W_{hh}^{\rightarrow}\vec{h}_{t-1})$</li>
<li>Backward hidden states: $\overleftarrow{h}_t = \phi(W_{xh}^{\leftarrow}x_t + W_{hh}^{\leftarrow}\overleftarrow{h}_{t+1})$</li>
<li>Final representation combines both directions: $h_t = [\vec{h}_t; \overleftarrow{h}_t]$</li>
</ul>
</li>
<li><p>Challenges with Basic RNNs</p>
<ul>
<li>Vanishing Gradients: Signal from distant time steps diminishes exponentially</li>
<li>Exploding Gradients: Gradients grow uncontrollably (solved with gradient clipping)</li>
<li>Limited context window: Difficulty capturing long-range dependencies</li>
</ul>
</li>
<li><p>Advanced RNN Architectures</p>
<ul>
<li><p>LSTM (Long Short-Term Memory)</p>
<ul>
<li>Explicitly designed to capture long-term dependencies</li>
<li>Cell state ($C_t$) acts as conveyor belt of information through time</li>
<li>Three gates control information flow:<ul>
<li>Input gate ($I_t$): Controls what new information enters the cell</li>
<li>Forget gate ($F_t$): Controls what information is discarded</li>
<li>Output gate ($O_t$): Controls what information is exposed as output</li>
</ul>
</li>
<li>LSTM equations:<ul>
<li>$I_t = \sigma(W_{ix}X_t + W_{ih}H_{t-1})$</li>
<li>$F_t = \sigma(W_{fx}X_t + W_{fh}H_{t-1})$</li>
<li>$O_t = \sigma(W_{ox}X_t + W_{oh}H_{t-1})$</li>
<li>$\tilde{C}_t = \tanh(W_{cx}X_t + W_{ch}H_{t-1})$ (candidate cell state)</li>
<li>$C_t = F_t \odot C_{t-1} + I_t \odot \tilde{C}_t$ (cell state update)</li>
<li>$H_t = O_t \odot \tanh(C_t)$ (hidden state)</li>
</ul>
</li>
<li>Solves vanishing gradient through additive updates and gating</li>
</ul>
</li>
<li><p>GRU (Gated Recurrent Unit)</p>
<ul>
<li>Simplified version of LSTM with fewer parameters</li>
<li>Has two gates: update gate and reset gate</li>
<li>Update gate controls how much previous state is retained</li>
<li>Reset gate controls how much previous state influences candidate state</li>
<li>Competitive performance with LSTM but more efficient</li>
</ul>
</li>
</ul>
</li>
<li><p>Backpropagation through Time (BPTT)</p>
<ul>
<li>Unrolling the computation graph along time axis</li>
<li>$h_t = W_{hx}x_t + W_{hh}h_{t-1} = f(x_t, h_{t-1}, w_h)$</li>
<li>$o_t = W_{ho}h_t = g(h_t, w_{oh})$</li>
<li>$L = {1 \over T}\sum l(y_t, o_t)$</li>
<li>${\delta L \over \delta w_h} = {1 \over T} \sum {\delta l \over \delta w_h}$</li>
<li>${\delta L \over \delta w_h} = {1 \over T} \sum {\delta l \over \delta o_t} {\delta o_t \over \delta h_t} {\delta h_t \over \delta w_h}$</li>
<li>${\delta h_t \over \delta w_h} = {\delta h_t \over \delta w_h} + {\delta h_t \over \delta h_{t-1}} {\delta h_{t-1} \over \delta w_h}$</li>
<li>Common to truncate the update to length of the longest subsequence in the batch</li>
<li>As the sequence goes forward, the hidden state keeps getting multiplied by W(hh)</li>
<li>Gradients can decay or explode as we go backwards in time</li>
<li>Solution is to use additive rather than multiplicative updates</li>
</ul>
</li>
<li><p>Decoding</p>
<ul>
<li>Output is generated one token at a time</li>
<li>Simple Solution: Greedy Decoding<ul>
<li>Argmax over vocab at each step</li>
<li>Keep sampling unless <eos> token output</eos></li>
</ul>
</li>
<li>May not be globally optimal path</li>
<li>Alternative: Beam Search<ul>
<li>Compute top-K candidate outputs at each step</li>
<li>Expand each one in V possible ways</li>
<li>Total VK candidates generated</li>
</ul>
</li>
<li>GPT used top-k and top-p sampling<ul>
<li>Top-K sampling: Redistribute the probability mass</li>
<li>Top-P sampling: Sample till the cumulative probability exceeds p</li>
</ul>
</li>
</ul>
</li>
<li><p>Attention</p>
<ul>
<li>In RNNs, hidden state linearly combines the inputs and then sends them to an activation function</li>
<li>Attention mechanism allows for more flexibility.<ul>
<li>Suppose there are m feature vectors or values</li>
<li>Model decides which to use based on the input query vector q and its similarity to a set of m keys</li>
<li>If query is most similar to key i, then we use value i.</li>
</ul>
</li>
<li>Attention acts as a soft dictionary lookup<ul>
<li>Compare query q to each key k(i)</li>
<li>Retrieve the corresponding value v(i)</li>
<li>To make the operation differentiable:<ul>
<li>Compute a convex combination</li>
</ul>
</li>
<li>$Attn(q,(k_1,v_1),(k_2, v_2)...,(k_m,v_m)) = \sum_{i=1}^m \alpha_i (q, \{k_i\}) v_i$<ul>
<li>$\alpha_i (q, \{k_i\})$ are the attention weights</li>
</ul>
</li>
<li>Attention weights are computed from an attention score function $a(q,k_i)$<ul>
<li>Computes the similarity between query and key</li>
</ul>
</li>
<li>Once the scores are computed, use soft max to impose distribution</li>
<li>Masking helps in ignoring the index which are invalid while computing soft max</li>
<li>For computational efficiency, set the dim of query and key to be same (say d)<ul>
<li>The similarity is given by dot product</li>
<li>The weights are randomly initialized</li>
<li>The expected variance of dot product will be d.</li>
<li>Scale the dot product by $\sqrt d$</li>
<li>Scaled Dot-Product Attention<ul>
<li>Attention Weight: $a(q,k) = {q^Tk \over \sqrt d}$</li>
<li>Scaled Dot Product Attention: $Attn(Q,K,V) =  S({QK^T \over \sqrt d})V$</li>
</ul>
</li>
</ul>
</li>
<li>Example: Seq2Seq with Attention<ul>
<li>Consider encoder-decoder architecture</li>
<li>In the decoder:<ul>
<li>$h_t = f(h_{t-1}, c)$</li>
<li>c is the context vector from encoder</li>
<li>Usually the last hidden state of the encoder</li>
</ul>
</li>
<li>Attention allows the decoder to look at all the input words<ul>
<li>Better alignment between source and target</li>
</ul>
</li>
<li>Make the context dynamic<ul>
<li>Query: previous hidden state of the decoder</li>
<li>Key: all the hidden states from the encoder</li>
<li>Value: all the hidden states from the encoder</li>
<li>$c_t = \sum_{i=1}^T \alpha_i(h_{t-1}^d, \{h_i^e\})h_i^e$</li>
</ul>
</li>
<li>If RNN has multiple hidden layers, usually take the top most layer</li>
<li>Can be extended to Seq2Vec models</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Transformers</p>
<ul>
<li>Transformers are seq2seq models using attention in both encoder and decoder steps</li>
<li>Eliminate the need for RNNs</li>
<li>Self Attention:<ul>
<li>Modify the encoder such that it attends to itself</li>
<li>Given a sequence of input tokens $[x_1, x_2, x_3...,x_n]$</li>
<li>Sequence of output tokens: $y_i = Attn(x_i, (x_1,x_1), (x_2, x_2)...,(x_n, x_n))$<ul>
<li>Query is xi</li>
<li>Keys and Values are are x1,x2…xn (all valid inputs)</li>
</ul>
</li>
<li>In the decoder step:<ul>
<li>$y_i = Attn(y_{i-1}, (y_1,y_1), (y_2, y_2)...(y_{i-1}, y_{i-1}))$</li>
<li>Each new token generated has access to all the previous output</li>
</ul>
</li>
</ul>
</li>
<li>Multi-Head Attention<ul>
<li>Use multiple attention matrices to capture different nuances and similarities</li>
<li>$h_i = Attn(W_i^q q_i, (W_i^k k_i, W_i^v v_i))$</li>
<li>Stack all the heads together and use a projection matrix to get he output</li>
<li>Set $p_q h = p_k h = p_v h = p_o$ for parallel computation **How?</li>
</ul>
</li>
<li>Positional Encoding<ul>
<li>Attention is permutation invariant</li>
<li>Positional encodings help overcome this</li>
<li>Sinusoidal Basis</li>
<li>Positional Embeddings are combined with original input X → X + P</li>
</ul>
</li>
<li>Combining All the Blocks<ul>
<li>Encoder<ul>
<li>Input: $ Z = LN(MHA(X,X,X) + X$</li>
<li>Encoder: $E = LN(FF(Z) + Z)$<ul>
<li>For the first layer:<ul>
<li>$ Z = \text{POS}(\text{Embed}(X))$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>In general, model has N copies of the encoder</li>
<li>Decoder <ul>
<li>Has access to both: encoder and previous tokens</li>
<li>Input: $ Z = LN(MHA(X,X,X) + X$</li>
<li>Input $ Z = LN(MHA(Z,E,E) + Z$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Representation Learning</p>
<ul>
<li>Contextual Word Embeddings<ul>
<li>Hidden state depends on all previous tokens</li>
<li>Use the latent representation for classification / other downstream tasks</li>
<li>Pre-train on a large corpus </li>
<li>Fine-tune on small task specific dataset</li>
<li>Transfer Learning</li>
</ul>
</li>
<li>ELMo<ul>
<li>Embeddings from Language Model</li>
<li>Fit two RNN models<ul>
<li>Left to Right</li>
<li>Right to Left</li>
</ul>
</li>
<li>Combine the hidden state representations to fetch embedding for each word</li>
</ul>
</li>
<li>BERT<ul>
<li>Bi-Directional Encoder Representations from Transformers</li>
<li>Pre-trained using Cloze task (MLM i.e. Masked Language Modeling)</li>
<li>Additional Objective: Next sentence Prediction</li>
</ul>
</li>
<li>GPT <ul>
<li>Generative Pre-training Transformer</li>
<li>Causal model using Masked Decoder</li>
<li>Train it as a language model on web text</li>
</ul>
</li>
<li>T5<ul>
<li>Text-to-Text Transfer Transformer</li>
<li>Single model to perform multiple tasks</li>
<li>Tell the task to perform as part of input sequence</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="page-navigation"><span class="nav-arrow prev disabled">
                <svg class="arrow" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <line x1="19" y1="12" x2="5" y2="12"></line>
                    <polyline points="12 19 5 12 12 5"></polyline>
                </svg>
                Previous
            </span><span class="nav-arrow next disabled">
                <svg class="arrow" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <line x1="5" y1="12" x2="19" y2="12"></line>
                    <polyline points="12 5 19 12 12 19"></polyline>
                </svg>
            </span></div></main>
    </div>

    <script src="../script.js"></script>






</body></html>