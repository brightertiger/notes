
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Probml 19 Ssl | My Notes</title>
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap">
  <!-- MathJax for LaTeX support -->
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: {
        enableMenu: false
      }
    };
  </script>
</head>
<body>
  <div class="container">
    <header>
      <h1>Probml 19 Ssl</h1>
      
      <a href="/" class="home-link">‚Üê Back to Home</a>
    </header>
    <main class="content">
      <h1 id="ssl">SSL</h1>
<ul>
<li><p>Data Augmentation</p>
<ul>
<li>Artificially modified versions of input vectors that may appear in real world data</li>
<li>Improves accuracy, makes model robust</li>
<li>Empirical risk minimization to vicinical risk minimization</li>
<li>Minimizing risk in the vicinity of input data point</li>
</ul>
</li>
<li><p>Transfer Learning </p>
<ul>
<li>Some data poor tasks may have structural similarity to other data rich tasks </li>
<li>Transferring information from one dataset to another via shared parameters of a model </li>
<li>Pretrain the model on a large source dataset </li>
<li>Fine tune the model on a small target dataset </li>
<li>Chop-off the head of the pretrained model and add a new one </li>
<li>The parameters may be frozen during fine-tuning </li>
<li>In case the parameters aren&#39;t frozen, use small learning rates.</li>
</ul>
</li>
<li><p>Adapters </p>
<ul>
<li>Modify the model structure to customize feature extraction </li>
<li>For example: Add MLPs after transformer blocks and initialize them for identity mappings </li>
<li>Much less parameters to be learned during fine-tuning</li>
</ul>
</li>
<li><p>Pre-training</p>
<ul>
<li>Can be supervised or unsupervised.</li>
<li>Supervised<ul>
<li>Imagenet is supervised pretraining.</li>
<li>For unrelated domains, less helpful.</li>
<li>More like speedup trick with a good initialization.</li>
</ul>
</li>
<li>Unsupervised<ul>
<li>Use unlabeled dataset</li>
<li>Minimize reconstruction error</li>
</ul>
</li>
<li>Self-supervised<ul>
<li>Labels are created from ulabeled dataset algorithmically</li>
<li>Cloze Task<ul>
<li>Fill in the blanks</li>
</ul>
</li>
<li>Proxy Tasks<ul>
<li>Create representations</li>
<li>Siamese Neural Networks</li>
<li>Capture relationship between inputs</li>
</ul>
</li>
<li>Contrastive Tasks<ul>
<li>Use data augmentation</li>
<li>Ensure that similar inputs have closer representations</li>
</ul>
</li>
</ul>
</li>
<li>SimCLR<ul>
<li>Simple Contrastive Learning for Visual Representations</li>
<li>Pretraining<ul>
<li>Take an unlabeled image X</li>
<li>Apply data augmentation A and A&#39; and get two views X and X&#39;</li>
<li>Apply encoder to both views</li>
<li>Apply projection head to both encodings</li>
<li>Contrastive Loss function with mini-batch to identify the positive pairs</li>
</ul>
</li>
<li>Pretraining<ul>
<li>Finetune the encoder and a task specific head</li>
<li>The original projection head is discarded</li>
</ul>
</li>
<li>NLP Encoders<ul>
<li>Contrastively trained language models</li>
<li>SimCSE: Simple Contrastive Learning for Sentence Embeddings<ul>
<li>Dropout acts as data augmentation</li>
<li>Cosine Similarity between representations</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Non-Contrastive Learning<ul>
<li>BYOL<ul>
<li>Bootstrap your own latent (BYOL)</li>
<li>Two copies of encoder</li>
<li>Teacher: Online, Student: Target</li>
<li>Teacher is EMA of updates made to student</li>
<li>MSE Loss</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Semi-Supervised Learning</p>
<ul>
<li>Learn from labeled + unlabeled data in tandem</li>
<li>Self-Training<ul>
<li>Train the model on labeled data</li>
<li>Run inference on unlabeled data to get predictions (pseudo-labels, machine generated)</li>
<li>Combine machine generated and human generated labels</li>
<li>Self-training is similar to EM algorithm where pseudo-labels are the E-step</li>
</ul>
</li>
<li>Noise Student Training<ul>
<li>Adds noise to student model to improve generalization</li>
<li>Add noise via dropout, stochastic depth, data augmentation</li>
</ul>
</li>
<li>Consistency Regularization<ul>
<li>Model&#39;s prediction shouldn&#39;t change much for small changes to the input</li>
<li>Can be implemented by passing augmented versions of same image as loss</li>
</ul>
</li>
<li>Label Propagation<ul>
<li>Graph: Nodes are i/p datapoints and edges denote similarity</li>
<li>Use graph clustering to group related nodes</li>
<li>Class labels are assigned to unlabeled data, based on the cluster distribution</li>
<li>Model Labels: Labels of the data points</li>
<li>Propagate the labels in such a way that there is minimal label disagreement between node and it&#39;s neighbours</li>
<li>Label guesses for unlabeled data that can be used for superised learning</li>
<li>Details:<ul>
<li>M labeled points, N unlabeled points</li>
<li>T: (M+N) x (M+N) transition matrix of normalized edge weights</li>
<li>Y: Label matrix for class distribution of (M+N) x C dimension</li>
<li>Use transition matrix to propagate labels Y = TY until convergence</li>
</ul>
</li>
<li>Success depends on calculating similarity between data points</li>
</ul>
</li>
<li>Consistency Regularization<ul>
<li>Small perturbation to input data point should not change the model predicitons</li>
</ul>
</li>
</ul>
</li>
<li><p>Generative Models</p>
<ul>
<li>Natural way of using unlabeled data by learning a model of data generative process.</li>
<li>Variational Autoencoders<ul>
<li>Models joint distribution of data (x) and latent variables (z)</li>
<li>First sample: $z \sim p(z)$ and then sample $x\sim p(x|z)$</li>
<li>Encoder: Approximate the posterior</li>
<li>Decoder: Approximate the likelihood</li>
<li>Maximize evidence lower bound of the data (ELBO) (derived from Jensen&#39;s ineuqlity)</li>
<li>Use VAEs to learn representations for downstream tasks</li>
</ul>
</li>
<li>Generative Adversarial Netwworks<ul>
<li>Generator: Maps latent distribution to data space</li>
<li>Discriminator: Distinguish between outputs of generator and true distribution</li>
<li>Modify discriminator to predict class labels and fake rather than just fake</li>
</ul>
</li>
</ul>
</li>
<li><p>Active Learning</p>
<ul>
<li>Identify true predictive mapping by quering as few data points as possible</li>
<li>Query Synthesis: Model asks output for any input</li>
<li>Pool Based: Model selects the data point from a pool of ulabeled data points</li>
<li>Maximum Entropy Sampling<ul>
<li>Uncertainty in predicted label</li>
<li>Fails when examples are ambiguous of mislabeled</li>
</ul>
</li>
<li>Bayesian Active Learning by Disagreement (BALD)<ul>
<li>Select examples where model makes predictions tht are highly diverese</li>
</ul>
</li>
</ul>
</li>
<li><p>Few-Shot Learning</p>
<ul>
<li>Learn to predict from very few labeled example</li>
<li>One-Shot Learning: Learn to predict from single example</li>
<li>Zero-Shot Lerning: Learn to predict without labeled examples</li>
<li>Model has to generalize for unseen labels during traning time</li>
<li>Works by learning a distance metric</li>
</ul>
</li>
<li><p>Weak Supervision</p>
<ul>
<li>Exact label not aviabale for data points</li>
<li>Distribution of labels for each case</li>
<li>Soft labels / label smoothing</li>
</ul>
</li>
</ul>
<h1 id="semi-supervised-and-self-supervised-learning">Semi-Supervised and Self-Supervised Learning</h1>
<ul>
<li><p>Semi-Supervised Learning (SSL): Leveraging both labeled and unlabeled data</p>
<ul>
<li>Motivation: Labels are expensive, unlabeled data is abundant</li>
<li>Assumption: Underlying data distribution contains useful structure</li>
</ul>
</li>
<li><p>Data Augmentation</p>
<ul>
<li>Creates artificial training examples through transformations</li>
<li>Preserves semantic content while changing surface features</li>
<li>Common augmentations:<ul>
<li>Image domain: rotations, flips, color jitter, cropping</li>
<li>Text domain: synonym replacement, back-translation</li>
<li>Audio domain: pitch shifting, time stretching</li>
</ul>
</li>
<li>Theoretical framework: Vicinical risk minimization<ul>
<li>Minimize risk in local neighborhoods around training examples</li>
<li>Improves robustness and generalization</li>
</ul>
</li>
</ul>
</li>
<li><p>Transfer Learning</p>
<ul>
<li>Leverages knowledge from data-rich domains to improve performance in data-poor domains</li>
<li>Process:<ol>
<li>Pretrain model on large source dataset (e.g., ImageNet, Common Crawl)</li>
<li>Adapt model to target task with smaller dataset</li>
<li>Options for adaptation:<ul>
<li>Feature extraction: Freeze pretrained layers, train only new head</li>
<li>Fine-tuning: Update all or subset of pretrained parameters</li>
</ul>
</li>
</ol>
</li>
<li>Parameter-efficient fine-tuning:<ul>
<li>Adapters: Small bottleneck layers added between frozen transformer blocks</li>
<li>LoRA: Low-rank adaptation of weight matrices</li>
<li>Prompt tuning: Learn soft prompts while keeping model parameters frozen</li>
</ul>
</li>
</ul>
</li>
<li><p>Self-Supervised Learning</p>
<ul>
<li><p>Creates supervisory signals from unlabeled data</p>
</li>
<li><p>Pretext tasks:</p>
<ul>
<li>Reconstruction tasks: Autoencoders, masked language modeling</li>
<li>Context prediction: Predict arrangement of shuffled patches</li>
<li>Contrastive tasks: Learn similar representations for related inputs</li>
</ul>
</li>
<li><p>Contrastive Learning</p>
<ul>
<li><p>Learn representations by comparing similar and dissimilar examples</p>
</li>
<li><p>SimCLR framework:</p>
<ol>
<li>Generate two views of each image via augmentation</li>
<li>Encode both views with shared encoder</li>
<li>Apply projection head to map encodings to space for contrastive loss</li>
<li>Contrastive loss: Maximize similarity between positive pairs (same image)
and minimize similarity between negative pairs (different images)</li>
<li>For downstream tasks, discard projection head and fine-tune encoder</li>
</ol>
</li>
<li><p>Key challenges:</p>
<ul>
<li>Hard negative mining: Finding informative negative examples</li>
<li>Batch size dependence: Performance scales with number of negatives</li>
<li>Feature collapse: Trivial solutions that ignore semantic content</li>
</ul>
</li>
</ul>
</li>
<li><p>Non-Contrastive Methods</p>
<ul>
<li><p>BYOL (Bootstrap Your Own Latent):</p>
<ul>
<li>Teacher-student architecture with no negative examples</li>
<li>Student network predicts teacher network outputs</li>
<li>Teacher parameters updated via exponential moving average of student</li>
<li>Avoids collapse through asymmetric architecture and predictor networks</li>
</ul>
</li>
<li><p>Masked Autoencoders:</p>
<ul>
<li>Inspired by BERT&#39;s success in NLP</li>
<li>Mask significant portions of input (e.g., 75% of image patches)</li>
<li>Train encoder-decoder to reconstruct original input</li>
<li>For downstream tasks, use only encoder</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Practical Considerations</p>
<ul>
<li>Pretraining often provides:<ul>
<li>Better initialization for optimization</li>
<li>More generalizable features</li>
<li>Sample efficiency: Fewer labeled examples needed</li>
</ul>
</li>
<li>Domain gap between pretraining and target task affects transfer effectiveness</li>
<li>Large pretrained models may contain useful knowledge but require careful adaptation</li>
</ul>
</li>
</ul>

    </main>
    <footer>
      <p>Generated with Markdown Notes Static Site Generator</p>
    </footer>
  </div>
</body>
</html>
  