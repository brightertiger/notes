<!DOCTYPE html><html lang="en"><head><link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&amp;display=swap" rel="stylesheet">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Trees</title>
    <link rel="stylesheet" href="../styles.css">

    <!-- MathJax for equation rendering -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
    // MathJax configuration
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$']],
            displayMath: [['$$', '$$']]
        },
        svg: {
            fontCache: 'global'
        }
    };
    </script>
    </head>

<body>
    <div class="container">
        <aside class="sidebar">
    <a href="../index.html" class="home-link">
        <svg class="home-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path>
            <polyline points="9 22 9 12 15 12 15 22"></polyline>
        </svg>
    </a>
    <h2>Navigation</h2>
    <div class="nav-section">
        <h3>Eslr Notes</h3>
        <ul>
            <li><a href="eslr-00.html">Introduction</a></li>
<li><a href="eslr-01-regression.html">01-Regression</a></li>
<li><a href="eslr-02-classification.html">02-Classification</a></li>
<li><a href="eslr-03-kernel-methods.html">03-Kernel-Methods</a></li>
<li><a href="eslr-04-model-assessment.html">04-Model-Assessment</a></li>
<li><a href="eslr-08-model-selection.html">08-Model-Selection</a></li>
<li><a href="eslr-09-additive-models.html">09-Additive-Models</a></li>
<li><a href="eslr-10-boosting.html">10-Boosting</a></li>
<li><a href="eslr-15-random-forest.html">15-Random-Forest</a></li>
        </ul>
    </div><div class="nav-section">
        <h3>General Notes</h3>
        <ul>
            <li><a href="gen-00.html">00</a></li>
<li><a href="gen-01-basic-statistics.html">01-Basic-Statistics</a></li>
<li><a href="gen-02-decision_trees.html">02-Decision Trees</a></li>
<li><a href="gen-03-boosting.html">03-Boosting</a></li>
<li><a href="gen-04-xgboost.html">04-Xgboost</a></li>
<li><a href="gen-05-clustering.html">05-Clustering</a></li>
<li><a href="gen-06-support_vector_machines.html">06-Support Vector Machines</a></li>
<li><a href="gen-07-dimensionality_reduction.html">07-Dimensionality Reduction</a></li>
<li><a href="gen-08-regression.html">08-Regression</a></li>
        </ul>
    </div><div class="nav-section">
        <h3>Jurafsky Notes</h3>
        <ul>
            <li><a href="jfsky-00.html">00</a></li>
<li><a href="jfsky-01-regex.html">01-Regex</a></li>
<li><a href="jfsky-02-tokenization.html">02-Tokenization</a></li>
<li><a href="jfsky-03-vectors.html">03-Vectors</a></li>
<li><a href="jfsky-04-sequence.html">04-Sequence</a></li>
<li><a href="jfsky-05-encoder.html">05-Encoder</a></li>
<li><a href="jfsky-06-transfer.html">06-Transfer</a></li>
        </ul>
    </div><div class="nav-section">
        <h3>Probml Notes</h3>
        <ul>
            <li><a href="probml-00.html">Introduction</a></li>
<li><a href="probml-01-introduction.html">Introduction (Detailed)</a></li>
<li><a href="probml-02-probability.html">02-probability</a></li>
<li><a href="probml-03-probability.html">Probability (Advanced)</a></li>
<li><a href="probml-04-statistics.html">04-statistics</a></li>
<li><a href="probml-05-decision_theory.html">05-decision Theory</a></li>
<li><a href="probml-06-information_theory.html">06-information Theory</a></li>
<li><a href="probml-08-optimization.html">08-optimization</a></li>
<li><a href="probml-09-discriminant_analysis.html">09-discriminant Analysis</a></li>
<li><a href="probml-10-logistic_regression.html">10-logistic Regression</a></li>
<li><a href="probml-11-linear_regression.html">11-linear Regression</a></li>
<li><a href="probml-13-ffnn.html">13-ffnn</a></li>
<li><a href="probml-14-cnn.html">14-cnn</a></li>
<li><a href="probml-15-rnn.html">15-rnn</a></li>
<li><a href="probml-16-exemplar.html">16-exemplar</a></li>
        </ul>
    </div></aside>

        <main class="content"><h1>Trees</h1>
<ul>
<li><p>Recursively partition the input space and define a local model in the resulting region of the input space</p>
<ul>
<li>Node i</li>
<li>Feature dimension d_i is compared to threshold t_i<ul>
<li>$R_i = \{x : x_{d_1} \leq t_1, x_{d_2} \leq t_2, \ldots\}$</li>
<li>Axis parallel splits</li>
</ul>
</li>
<li>At leaf node, model specifies the predicted output for any input that falls in the region<ul>
<li>$w_1 = \frac{\sum_{n} y_n I\{x_n \in R_1\}}{\sum_{n} I\{x_n \in R_1\}}$</li>
</ul>
</li>
<li>Tree structure can be represented as<ul>
<li>$f(x, \theta) = \sum_j w_j I\{x \in R_j\}$ </li>
<li>where j denotes a leaf node</li>
</ul>
</li>
</ul>
</li>
<li><p>Model Fitting</p>
<ul>
<li>$L(\theta) = \sum_J \sum_{i \in R_j} (y_i, w_j)$</li>
<li>The tree structure is non-differentiable</li>
<li>Greedy approach to grow the tree</li>
<li>C4.5, ID3 etc.</li>
<li>Finding the split<ul>
<li>$L(\theta) = {|D_l \over |D|} c_l + {|D_r \over |D|} c_r$</li>
<li>Find the split such that the new weighted overall cost after splitting is minimized</li>
<li>Looks for binary splits because of data fragmentation</li>
</ul>
</li>
<li>Determining the cost<ul>
<li>Regression: Mean Squared Error</li>
<li>Classification:<ul>
<li>Gini Index: $\sum \pi_ic (1 - \pi_ic)$</li>
<li>$\pi_ic$ probability that the observation i belongs to class c </li>
<li>$1 - \pi_ic$ probability of misclassification</li>
<li>Entropy: $\sum \pi_{ic} \log \pi_{ic}$</li>
</ul>
</li>
</ul>
</li>
<li>Regularization<ul>
<li>Approach 1: Stop growing the tree according to some heuristic<ul>
<li>Example: Tree reaches some maximum depth</li>
</ul>
</li>
<li>Approach 2: Grow the tree to its maximum possible depth and prune it back</li>
</ul>
</li>
<li>Handling missing features<ul>
<li>Categorical: Consider missing value as a new category</li>
<li>Continuous: Surrogate splits<ul>
<li>Look for variables that are most correlated to the feature used for split</li>
</ul>
</li>
</ul>
</li>
<li>Advantages of Trees<ul>
<li>Easy to interpret</li>
<li>Minimal data preprocessing is required</li>
<li>Robust to outliers</li>
</ul>
</li>
<li>Disadvantages of Trees<ul>
<li>Easily overfit</li>
<li>Perform poorly on distributional shifts</li>
</ul>
</li>
</ul>
</li>
<li><p>Ensemble Learning</p>
<ul>
<li>Decision Trees are high variance estimators</li>
<li>Average multiple models to reduce variance</li>
<li>$f(y| x) = {1 \over M} \sum f_m (y | x)$</li>
<li>In case of classification, take majority voting<ul>
<li>$p = Pr(S &gt; M/2) = 1 - \text{Bin}(M, M/2, \theta)$</li>
<li>Bin(.) if the CDF of the binomial distribution</li>
<li>If the errors of the models are uncorrelated, the averaging of classifiers can boost the performance</li>
</ul>
</li>
<li>Stacking<ul>
<li>Stacked Generalization</li>
<li>Weighted Average of the models</li>
<li>$f(y| x) = {1 \over M} \sum w_m f_m (y | x)$</li>
<li>Weights have to be learned on unseen data</li>
<li>Stacking is different from Bayes averaging<ul>
<li>Weights need not add up to 1</li>
<li>Only a subset of hypothesis space considered in stacking</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Bagging</p>
<ul>
<li>Bootstrap aggregation</li>
<li>Sampling with replacement<ul>
<li>Start with N data points</li>
<li>Sample with replacement till N points are sampled</li>
<li>Probability that a point is never selected<ul>
<li>$(1 - {1 \over N})^N$</li>
<li>As N â†’ $\infty$, the value is roughly 1/e (37% approx)</li>
</ul>
</li>
</ul>
</li>
<li>Build different estimators of these sampled datasets</li>
<li>Model doesn't overly rely on any single data point</li>
<li>Evaluate the performance on the 37% excluded data points<ul>
<li>OOB (out of bag error)</li>
</ul>
</li>
<li>Performance boost relies on de-correlation between various models<ul>
<li>Reduce the variance is predictions</li>
<li>The bias remains put</li>
<li>$V = \rho \sigma ^ 2 + {(1 - \rho) \over B} \sigma ^2$</li>
<li>If the trees are IID, correlation is 0, and variance is 1/B</li>
</ul>
</li>
<li>Random Forests<ul>
<li>De-correlate the trees further by randomizing the splits</li>
<li>A random subset of features chosen for split at each node</li>
<li>Extra Trees: Further randomization by selecting subset of thresholds</li>
</ul>
</li>
</ul>
</li>
<li><p>Boosting</p>
<ul>
<li>Sequentially fitting additive models<ul>
<li>In the first round, use original data</li>
<li>In the subsequent rounds, weight data samples based on the errors<ul>
<li>Misclassified examples get more weight</li>
</ul>
</li>
</ul>
</li>
<li>Even if each single classifier is a weak learner, the above procedure makes the ensemble a strong classifier</li>
<li>Boosting reduces the bias of the individual weak learners to result in an overall strong classifier</li>
<li>Forward Stage-wise Additive Modeling<ul>
<li>$F_m(x) = F_{m-1}(x) + \beta_m f_m(x; \theta_m)$</li>
<li>$\beta_m, \theta_m$ are chosen to minimize the loss.</li>
<li>Add new models that address the residual error $r_i = (y_i - F_{m-1}(x_i))$</li>
<li>AdaBoodst<ul>
<li>Classification with exponential loss: $L(y, F(x)) = exp(-yF(x))$</li>
<li>$y \in \{-1, +1\}$</li>
<li>Output of tree has to be {-1, +1} in each region</li>
<li>Each round m, the optimization is simplified to finding the best classifier fit to the weighted training data.</li>
<li>$\theta_m = \arg\min \sum w_i^{(m)} I \{y_i \ne f_m(x_i; \theta)\}$</li>
</ul>
</li>
</ul>
</li>
<li>Gradient Boosting<ul>
<li>Fit the entire model in forward stagewise manner</li>
<li>$F_m(x) = F_{m-1}(x) + \beta_m f_m(x; \theta_m)$</li>
<li>Expand this as a Taylor series around $F_{m-1}(x)$</li>
<li>$L(y, F_m(x)) = L(y, F_{m-1}(x)) + g_m(x) + \beta_m f_m(x) + O(\beta_m^2)$</li>
<li>Neglect higher order terms</li>
<li>Minimize the loss</li>
<li>$f_m(x) = - g_m(x)$</li>
<li>$g_m(x) = {\delta L(y, F(x)) \over \delta F(x)}|_{F(x) = F_{m-1}(x)}$</li>
<li>The new predictor is trained to approximate the negative gradient of the loss</li>
</ul>
</li>
<li>In the current form, the optimization is limited to the set of training points</li>
<li>Need a function that can generalize</li>
<li>Train a weak learner that can approximate the negative gradient signal<ul>
<li>$F_m = \arg\min \sum (-g_m -F(x_i))^2$</li>
<li>Use a shrinkage factor for regularization</li>
</ul>
</li>
<li>Stochastic Gradient Boosting<ul>
<li>Data Subsampling for faster computation and better generalization</li>
</ul>
</li>
</ul>
</li>
<li><p>XGBoost</p>
<ul>
<li>Extreme Gradient Boosting</li>
<li>Add regularization to the objective</li>
<li>$L(f) = \sum l(y_i, f(x_i)) + \Omega(f)$</li>
<li>$\Omega(f) = \gamma J + {1 \over 2} \lambda \sum w_j^2$</li>
<li>Consider the forward stage wise additive modeling</li>
<li>$L_m(f) = \sum l(y_i, f_{m-1}(x_i) + F(x_i)) + \Omega(f)$</li>
<li>Use Taylor's approximation on F(x)</li>
<li>$L_m(f) = \sum l(y_i, f_{m-1}(x_i)) + g_{im} F_m(x_i) + {1 \over 2} h_{im} F_m(x_i)^2) + \Omega(f)$<ul>
<li>g is the gradient and h is the hessian</li>
</ul>
</li>
<li>Dropping the constant terms and using a decision tree form of F</li>
<li>$F(x_{ij}) = w_{j}$</li>
<li>$L_m = \sum_j (\sum_{i \in I_j} g_{im}w_j) + (\sum_{i \in I_j} h_{im} w_j^2) + \gamma J + {1 \over 2} \lambda \sum w_j^2$ </li>
<li>Solution to the Quadratic Equation:<ul>
<li>$G_{jm} = \sum_{i \in I_j} g_{im}$</li>
<li>$H_{jm} = \sum_{i \in I_j} h_{im}$</li>
<li>$w^* = {- G \over H + \lambda}$</li>
<li>$L(w^*) = - {1 \over 2} \sum_J {G^2_{jm} \over H_{jm} + \lambda} + \gamma J$</li>
</ul>
</li>
<li>Condition for Splitting the node:<ul>
<li>$\text{gain} = [{G^2_L \over H_L + \lambda} + {G^2_R \over H_R + \lambda} - {G^2_L + G^2_R \over H_R + H_L + \lambda}] - \gamma$</li>
<li>Gamma acts as regularization</li>
<li>Tree wont split if the gain from split is less than gamma</li>
</ul>
</li>
</ul>
</li>
<li><p>Feature Importance</p>
<ul>
<li>$R_k(T) = \sum_J G_j  I(v_j = k)$</li>
<li>G is the gain in accuracy / reduction in cost</li>
<li>I(.) returns 1 if node uses the feature</li>
<li>Average the value of R over the ensemble of trees</li>
<li>Normalize the values </li>
<li>Biased towards features with large number of levels</li>
</ul>
</li>
<li><p>Partial Dependency Plot</p>
<ul>
<li>Assess the impact of a feature on output</li>
<li>Marginalize all other features except k</li>
</ul>
</li>
</ul>
<div class="page-navigation"><span class="nav-arrow prev disabled">
                <svg class="arrow" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <line x1="19" y1="12" x2="5" y2="12"></line>
                    <polyline points="12 19 5 12 12 5"></polyline>
                </svg>
                Previous
            </span><span class="nav-arrow next disabled">
                <svg class="arrow" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <line x1="5" y1="12" x2="19" y2="12"></line>
                    <polyline points="12 5 19 12 12 19"></polyline>
                </svg>
            </span></div></main>
    </div>

    <script src="../script.js"></script>






</body></html>