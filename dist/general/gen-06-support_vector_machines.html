<!DOCTYPE html><html lang="en"><head><link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&amp;display=swap" rel="stylesheet">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Support Vector Machines</title>
    <link rel="stylesheet" href="../styles.css">

    <!-- MathJax for equation rendering -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
    // MathJax configuration
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$']],
            displayMath: [['$$', '$$']]
        },
        svg: {
            fontCache: 'global'
        }
    };
    </script>
    </head>

<body>
    <div class="container">
        <aside class="sidebar">
    <a href="../index.html" class="home-link">
        <svg class="home-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path>
            <polyline points="9 22 9 12 15 12 15 22"></polyline>
        </svg>
    </a>
    <h2>Navigation</h2>
    <div class="nav-section">
        <h3>Eslr Notes</h3>
        <ul>
            <li><a href="eslr-00.html">Introduction</a></li>
<li><a href="eslr-01-regression.html">01-Regression</a></li>
<li><a href="eslr-02-classification.html">02-Classification</a></li>
<li><a href="eslr-03-kernel-methods.html">03-Kernel-Methods</a></li>
<li><a href="eslr-04-model-assessment.html">04-Model-Assessment</a></li>
<li><a href="eslr-08-model-selection.html">08-Model-Selection</a></li>
<li><a href="eslr-09-additive-models.html">09-Additive-Models</a></li>
<li><a href="eslr-10-boosting.html">10-Boosting</a></li>
<li><a href="eslr-15-random-forest.html">15-Random-Forest</a></li>
        </ul>
    </div><div class="nav-section">
        <h3>General Notes</h3>
        <ul>
            <li><a href="gen-00.html">00</a></li>
<li><a href="gen-01-basic-statistics.html">01-Basic-Statistics</a></li>
<li><a href="gen-02-decision_trees.html">02-Decision Trees</a></li>
<li><a href="gen-03-boosting.html">03-Boosting</a></li>
<li><a href="gen-04-xgboost.html">04-Xgboost</a></li>
<li><a href="gen-05-clustering.html">05-Clustering</a></li>
<li><a href="#" class="active">06-Support Vector Machines</a></li>
<li><a href="gen-07-dimensionality_reduction.html">07-Dimensionality Reduction</a></li>
<li><a href="gen-08-regression.html">08-Regression</a></li>
        </ul>
    </div><div class="nav-section">
        <h3>Jurafsky Notes</h3>
        <ul>
            <li><a href="jfsky-00.html">00</a></li>
<li><a href="jfsky-01-regex.html">01-Regex</a></li>
<li><a href="jfsky-02-tokenization.html">02-Tokenization</a></li>
<li><a href="jfsky-03-vectors.html">03-Vectors</a></li>
<li><a href="jfsky-04-sequence.html">04-Sequence</a></li>
<li><a href="jfsky-05-encoder.html">05-Encoder</a></li>
<li><a href="jfsky-06-transfer.html">06-Transfer</a></li>
        </ul>
    </div></aside>

        <main class="content"><h1>Support Vector Machines</h1>
<h2>Linear SVM</h2>
<ul>
<li>Classification setting</li>
<li>Find the maximum-margin hyperplane that can separate the data</li>
<li>Best hyperplane is the one that maximizes the margin<ul>
<li>Margin the distance of the hyperplane to closest data points from both classes</li>
<li>Hyperplane: $H : wx +b = 0$</li>
</ul>
</li>
<li>Distance of a point (x) to a hyperplane (h):<ul>
<li>$d = \frac{|Wx + b|}{||W||}$</li>
</ul>
</li>
<li>Margin is defined by the point closest to the hyperplane<ul>
<li>$\gamma(W,b) = \min_{x \in D} \frac{|Wx + b|}{||W||^2}$</li>
<li>Margin is scale invariant</li>
</ul>
</li>
<li>SVM wants to maximize this margin<ul>
<li>For margin to be maximized, hyperplane must lie right in the middle of the two classes</li>
<li>Otherwise it can be moved towards data points of the class that is further away and be further increased</li>
</ul>
</li>
<li>Mathematics<ul>
<li>Binary Classification<ul>
<li>$y_i \in \{+1,-1\}$</li>
</ul>
</li>
<li>Need to find a separating hyperplane such that<ul>
<li>$(Wx_i + b) &gt; 0 \; \forall \; y_i = +1$</li>
<li>$(Wx_i + b) &lt; 0 \; \forall \; y_i = -1$</li>
<li>$y_i(Wx_i + b) &gt; 0$</li>
</ul>
</li>
<li>SVM posits that the best hyperplane is the one that maximizes the margin<ul>
<li>Margin acts as buffer which can lead to better generalization</li>
</ul>
</li>
<li>Objective<ul>
<li>$\max_{W,b} \gamma(W,b) \; \text{subject to} \; y_i(Wx_i + b) &gt; 0$</li>
<li>$\max_{W,b} \min_{x \in D} \frac{|Wx + b|}{||W||^2} \; \text{subject to} \; y_i(Wx_i + b) &gt; 0$</li>
<li>A max-min optimization problem</li>
</ul>
</li>
<li>Simplification<ul>
<li>The best possible hyperplace is scale invariant</li>
<li>Add a constraint such that $|Wx +b| = 1$</li>
</ul>
</li>
<li>Updated objective<ul>
<li>$\max \frac{1}{||W||^2} \; \text{subject to} \; y_i(Wx_i + b) \ge 0 \; ; |Wx +b| = 1$</li>
<li>$\min ||W||^2 \; \text{subject to} \; y_i(Wx_i + b) \ge 0 \; ; |Wx +b| = 1$</li>
</ul>
</li>
<li>Combining the contraints<ul>
<li>$y_i(Wx_i + b) \ge 0\; ; |Wx +b| = 1 \implies y_i(Wx_i + b) \ge 1$</li>
<li>Holds true because the objective is trying to minimize W</li>
</ul>
</li>
<li>Final objective<ul>
<li>$\min ||W||^2 \; \text{subject to} \; y_i(Wx_i + b) \ge 1$\</li>
</ul>
</li>
<li>Quadratic optimization problem<ul>
<li>Can be solved quickly unlike regression which involves inverting a large matrix</li>
<li>Gives a unique solution unlike perceptron</li>
</ul>
</li>
<li>At the optimal solution, some training points will lie of the margin<ul>
<li>$y_i(Wx_i + b) = 1$</li>
<li>These points are called support vectors</li>
</ul>
</li>
</ul>
</li>
<li>Soft Constraints<ul>
<li>What if the optimization problem is infeasible?<ul>
<li>No solution exists</li>
</ul>
</li>
<li>Add relaxations i.e. allow for some misclassification<ul>
<li>Original: $y_i(Wx_i + b) \ge 1$</li>
<li>Relaxed: $y_i(Wx_i + b) \ge 1 - \xi_i \; ; \xi_i &gt; 0$</li>
<li>$\xi_i = \begin{cases} 1 - y_i(Wx_i + b), &amp; \text{if } y_i(Wx_i + b) &lt; 1\\0, &amp; \text{otherwise} \end{cases}$</li>
<li>Hinge Loss $\xi_i = \max (1 - y_i(Wx_i + b), 0)$</li>
</ul>
</li>
<li>Objective: $\min ||W||^2 + C \sum_i \max (1 - y_i(Wx_i + b), 0)$<ul>
<li>C is the regularization parameter that calculates trade-off</li>
<li>High value of C allows for less torelance on errors</li>
</ul>
</li>
</ul>
</li>
<li>Duality<ul>
<li>Primal problem is hard to solve</li>
<li>Convert the problem to a Dual, which is easier to solve and also provides near-optimal solution to primal</li>
<li>The gap is the optimality that arises in this process is the duality gap</li>
<li>Lagrangian multipliers determine if strong suality exists</li>
<li>Convert the above soft-margin SVM to dual via Lagrangian multipliers</li>
<li>$\sum \alpha_i + \sum\sum \alpha_i \alpha_j y_i y_j x_i^T x_j$</li>
<li>$\alpha$ is the Lagrangian multiplier</li>
</ul>
</li>
<li>Kernelization<ul>
<li>Say the points are not separable in lower dimension<ul>
<li>Transform them via kernels to project them to a higher dimension</li>
<li>The points may be separable the higher dimension</li>
<li>Non-linear feature transformation</li>
<li>Solve non-linear problems via Linear SVM</li>
</ul>
</li>
<li>Polynomial Kernel<ul>
<li>$K(x_i, x_j) = (x_i^T x_j + c)^d$</li>
<li>The d regers to the degree of the polynomial</li>
<li>Example: 2 points in 1-D (a and b) transformerd via second order polynomial kernel<ul>
<li>$K(a,b) = (ab + 1)^2 = 2ab+ a^2b^2 + 1 = (\sqrt{2a}, a, 1)(\sqrt{2b}, b, 1)$</li>
</ul>
</li>
<li>Calculates similarity between points in higher dimension</li>
</ul>
</li>
<li>RBF Kernel<ul>
<li>$K(x_i, x_j) = \exp \{\gamma |x_i - x_j|^2\}$</li>
<li>The larger the distance between two observations, the less is the similarity</li>
<li>Radial Kernel determines how much influence each observation has on classifying new data points\</li>
<li>Transforms points to an infinite dimension space<ul>
<li>Tayloy Expansion of exponential term shows how RBF is a polynomial function with inifnite dimensions</li>
</ul>
</li>
<li>2 points in 1-D (a and b) transformerd via RBF<ul>
<li>$K(a,b) = (1, \sqrt{\frac{1}{1!}}a, \sqrt{\frac{1}{2!}}a^2...)(1, \sqrt{\frac{1}{1!}}b, \sqrt{\frac{1}{2!}}b^2...)$</li>
</ul>
</li>
</ul>
</li>
<li>Kernel Trick<ul>
<li>Transforming the original dataset via Kernels and training SVM is expensive</li>
<li>Convert Dot-products of support vectors to dot-products of mapping functions</li>
<li>$x_i^T x_j \implies \phi(x_i)^T \phi(x_j)$</li>
<li>Kernels are chosen in a way that this is feasible</li>
</ul>
</li>
</ul>
</li>
<li>SVM For Regression<ul>
<li>Margins should cover all data points (Hard) or most data points (Soft)</li>
<li>The boundary now lies in the middle of the margins<ul>
<li>The regression model to estimate the target values</li>
</ul>
</li>
<li>The objective is to minimize the the distance of the points to the boundary</li>
<li>Hard SVM is sensitive to outliers</li>
</ul>
</li>
</ul>
<h2>Kernel Selection</h2>
<ul>
<li><p>Choosing the right kernel:</p>
<ul>
<li>Linear kernel: $K(x_i, x_j) = x_i^T x_j$<ul>
<li>Efficient for high-dimensional data</li>
<li>Works well when number of features exceeds number of samples</li>
<li>Simplest kernel with fewest hyperparameters</li>
</ul>
</li>
<li>Polynomial kernel: $K(x_i, x_j) = (x_i^T x_j + c)^d$<ul>
<li>Good for normalized training data</li>
<li>Degree d controls flexibility (higher d = more complex decision boundary)</li>
<li>Can capture feature interactions</li>
</ul>
</li>
<li>RBF/Gaussian kernel: $K(x_i, x_j) = \exp(-\gamma ||x_i - x_j||^2)$<ul>
<li>Most commonly used non-linear kernel</li>
<li>Works well for most datasets</li>
<li>Gamma parameter controls influence radius (higher gamma = more complex boundary)</li>
</ul>
</li>
<li>Sigmoid kernel: $K(x_i, x_j) = \tanh(\alpha x_i^T x_j + c)$<ul>
<li>Similar to neural networks (hyperbolic tangent activation)</li>
<li>Less commonly used in practice</li>
</ul>
</li>
</ul>
</li>
<li><p>Cross-validation should be used to select the optimal kernel and hyperparameters</p>
</li>
</ul>
<h2>SVM Hyperparameter Tuning</h2>
<ul>
<li>C parameter (regularization strength):<ul>
<li>Controls trade-off between maximizing margin and minimizing training error</li>
<li>Smaller C: Wider margin, more regularization, potential underfitting</li>
<li>Larger C: Narrower margin, less regularization, potential overfitting</li>
</ul>
</li>
<li>Gamma parameter (for RBF kernel):<ul>
<li>Controls influence radius of support vectors</li>
<li>Smaller gamma: Larger radius, smoother decision boundary</li>
<li>Larger gamma: Smaller radius, more complex decision boundary</li>
</ul>
</li>
<li>Practical suggestions:<ul>
<li>Start with RBF kernel, grid search over C and gamma</li>
<li>Try logarithmic scale for both C and gamma (e.g., 0.001, 0.01, 0.1, 1, 10, 100)</li>
<li>Use cross-validation to evaluate performance</li>
</ul>
</li>
</ul>
<div class="page-navigation"><span class="nav-arrow prev disabled">
                <svg class="arrow" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <line x1="19" y1="12" x2="5" y2="12"></line>
                    <polyline points="12 19 5 12 12 5"></polyline>
                </svg>
                Previous
            </span><span class="nav-arrow next disabled">
                <svg class="arrow" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <line x1="5" y1="12" x2="19" y2="12"></line>
                    <polyline points="12 5 19 12 12 19"></polyline>
                </svg>
            </span></div></main>
    </div>

    <script src="../script.js"></script>






</body></html>