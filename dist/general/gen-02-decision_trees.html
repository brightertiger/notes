
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Gen 02 Decision Trees | My Notes</title>
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap">
  <!-- MathJax for LaTeX support -->
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: {
        enableMenu: false
      }
    };
  </script>
</head>
<body>
  <div class="container">
    <header>
      <h1>Gen 02 Decision Trees</h1>
      
      <a href="/" class="home-link">‚Üê Back to Home</a>
    </header>
    <main class="content">
      <h1 id="decision-trees">Decision Trees</h1>
<h2 id="decision-trees-1">Decision Trees</h2>
<ul>
<li>Recursively split the input / feature space using stubs i.e. decision rules<ul>
<li>Splits are parallel to the axis</li>
</ul>
</li>
<li>Mathematical Represenation<ul>
<li>$R_j = { x : d_1 &lt;= t_1, d_2 &gt;= t_2 ... }$\</li>
<li>$\hat Y_i = \sum_j w_j I{x_i \in R_j}$</li>
<li>$w_j = \frac{\sum_i y_i I {x_i \in R_j}}{\sum_i I {x_i \in R_j}}$</li>
</ul>
</li>
<li>Types of Decision Trees<ul>
<li>Binary Splits<ul>
<li>Classification and Regression Trees (CART)</li>
<li>C4.5</li>
</ul>
</li>
<li>Multiple Splits:<ul>
<li>CHAID (Chi-Square Automatic Interaction Detection)</li>
<li>ID3</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="splitting">Splitting</h2>
<ul>
<li>Split Criteria for Classification Trees<ul>
<li>The nodes are split to decrease impurity in classification</li>
<li>Gini Criterion<ul>
<li>$1 - \sum_C p_{i}^2$</li>
<li>Probability that observation belongs to class i: $p_i$</li>
<li>Misclassification:</li>
<li>For a given class (say i):<ul>
<li>$p_i \times p_{k \ne i} = p_i \times (1 - p_i)$</li>
</ul>
</li>
<li>Across all classes:</li>
<li>$\sum_C p_i \times (1 - p_i)$</li>
<li>$\sum_C p_i - \sum_C p_{i}^2$</li>
<li>$1 - \sum_C p_{i}^2$</li>
<li>Ranges from (0, 0.5)</li>
</ul>
</li>
<li>Entropy Criterion<ul>
<li>Measure of uncertainly of a random variable</li>
<li>Given an event E<ul>
<li>p(E) = 1 $\implies$ No Surprise</li>
<li>p(E) = 0 $\implies$ Huge Surprise</li>
<li>Informaion Content: $I(E) = \log(1 / p(E))$</li>
</ul>
</li>
<li>Entropy is the expectation of this information content<ul>
<li>$H(E) = - \sum p(E) \log(p(E))$</li>
<li>Maximum when all outcomes have same probability of occurance</li>
</ul>
</li>
<li>Ranges from (0, 1)</li>
</ul>
</li>
</ul>
</li>
<li>Split Criteria for Regression Trees<ul>
<li>Sum-Squared Error</li>
<li>$\sum_i (Y_i - \bar Y)^2$</li>
</ul>
</li>
<li>Finding the Split<ul>
<li>For any candidate value:<ul>
<li>Calculate the weighted average reduction in impurity / error</li>
<li>Weights being the number of observations flowing in the child nodes</li>
</ul>
</li>
<li>Starting Gini<ul>
<li>$\text{Gini}_{\text{Root}}$</li>
<li>$N_{\text{Root}}$</li>
</ul>
</li>
<li>After Split<ul>
<li>Child Nodes<ul>
<li>$\text{Gini}<em>{\text{Left}}, N</em>{\text{Left}}$</li>
<li>$\text{Gini}<em>{\text{Right}}, N</em>{\text{Right}}$</li>
</ul>
</li>
<li>Updated Gini<ul>
<li>$\frac{N_{\text{Left}}}{N_{\text{Root}}} \times \text{Gini}<em>{\text{Left}} + \frac{N</em>{\text{Right}}}{N_{\text{Root}}} \times \text{Gini}_{\text{Right}}$</li>
</ul>
</li>
</ul>
</li>
<li>Find the split, the results in minimum updated Gini</li>
<li>Updated Gini &lt;= Starting Gini</li>
<li>Greedy algorithms to find the best splits</li>
</ul>
</li>
</ul>
<h2 id="bias-variance-trade-off">Bias-Variance Trade-off</h2>
<ul>
<li>Bias<ul>
<li>Measures ability of an ML algorithm to model true relationship between features and target</li>
<li>Simplifying assumptions made by the model to learn the relationship<ul>
<li>Example: Linear vs Parabolic relationship</li>
</ul>
</li>
<li>Low Bias: Less restrictive assupmtions</li>
<li>High Bias: More restrictive assumptions</li>
</ul>
</li>
<li>Variance<ul>
<li>The difference in model performance across different datasets drawn from the same distribution</li>
<li>Low Variance: Small changes to model perforamance with changes in datasets</li>
<li>High Variance: Large changes to model perforamance with changes in datasets</li>
</ul>
</li>
<li>Irreducible Error<ul>
<li>Bayes error</li>
<li>Cannot be reduced irrespective of the model form</li>
</ul>
</li>
<li>Best model minimizes: $\text{MSE} = \text{bias}^2 + \text{variance}$</li>
<li>Decision trees have low bias and high variance</li>
<li>Decision trees are prone to overfitting<ul>
<li>Noisy Samples</li>
<li>Small data samples in nodes down the tree</li>
<li>Tree Pruning solves for overfitting<ul>
<li>Adding a cost term to objetive which captures tree complexity</li>
<li>$\text{Tree Score} = SSR + \alpha T$</li>
<li>As the tree grows in size, the reduction in SSR has to more than offset the complexity cost</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="nature-of-decision-trees">Nature of Decision Trees</h2>
<ul>
<li>Decision Trees can model non-linear relationships (complex deicison boundaries)</li>
<li>Spline regressions cannot achieve the same results<ul>
<li>Spline adds indicator variables to capture interactions and create kinks</li>
<li>But the decision boundary has to be continuous</li>
<li>The same restriction doesn&#39;t apply to decision trees</li>
</ul>
</li>
<li>Decision Trees don&#39;t require feature sscaling</li>
<li>Decision Trees are less sensitive to outliers<ul>
<li>Outliers are of various kinds:<ul>
<li>Outliers: Points with extreme values<ul>
<li>Input Features<ul>
<li>Doesn&#39;t impact Decision Trees</li>
<li>Split finding will ignore the extreme values</li>
</ul>
</li>
<li>Output / Target</li>
</ul>
</li>
<li>Influential / High-Leverage Points: Undue influence on model</li>
</ul>
</li>
</ul>
</li>
<li>Decision Trees cannot extrapolate well to ranges outside the training data</li>
<li>Decision trees cannot capture linear time series based trends / seasonality</li>
</ul>
<h2 id="bagging">Bagging</h2>
<ul>
<li>Bootstrap Agrregation</li>
<li>Sampling with repetition<ul>
<li>Given Dataset of Size N</li>
<li>Draw N samples with replacement</li>
<li>Probability that a point (say i) never gets selected<ul>
<li>$(1 - \frac{1}{N})^N \approx \frac{1}{e}$</li>
</ul>
</li>
<li>Probability that a point (say i) gets selected atleast once<ul>
<li>$1 - \frac{1}{e} \approx 63%$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="random-forest">Random Forest</h2>
<ul>
<li>Use bootstrap aggregation (bagging) to create multiple datasets<ul>
<li>&quot;Random&quot; subspace of dataset</li>
</ul>
</li>
<li>Use subset of variables for split at each node<ul>
<li>sqrt for classification</li>
<li>m//3 for regression</li>
</ul>
</li>
<li>Comparison to single decision tree<ul>
<li>Bias remains the same</li>
<li>Variance decreases</li>
<li>Randomness in data and slpits reduces the correlation in prediction across trees</li>
<li>Let $\hat y_i$ be the prediction from ith tree in the forest</li>
<li>Let $\sigma^2$ be the variance of $\hat y_i$</li>
<li>Let $\rho$ be the correlation between two trees in the forest</li>
<li>$V(\sum_i \hat y_i) = \sum V(\hat y_i) + 2 \sum\sum COV(\hat y_i, \hat y_j)$</li>
<li>$V(\sum_i \hat y_i) = n \sigma^2 + n(n-1) \rho \sigma^2$</li>
<li>$V( \frac{1}{n} \sum_i \hat y_i) = \rho \sigma^2 + \frac{1-\rho}{n} \sigma^2$</li>
<li>Variance goes down as more trees are added, but bias stays put</li>
</ul>
</li>
<li>Output Combination<ul>
<li>Majority Voting for Classification</li>
<li>Averaging for Regression</li>
</ul>
</li>
<li>Out-of-bag (OOB) Error<ul>
<li>Use the non-selected rows in bagging to estimate model performance</li>
<li>Comparable to cross-validaiton results</li>
</ul>
</li>
<li>Proximity Matrix<ul>
<li>Use OOB observations</li>
<li>Count the number of times each pair goes to the same terminal node</li>
<li>Identifies observations that are close/similar to each other</li>
</ul>
</li>
</ul>
<h2 id="extratrees">ExtraTrees</h2>
<ul>
<li>Extremely Randomized Trees</li>
<li>Bagging:<ul>
<li>ExtraTrees: No</li>
<li>Extremely Randomized Trees: Yes</li>
</ul>
</li>
<li>Mutiple trees are built using:<ul>
<li>Random variable subset for splitting</li>
<li>Random threshold subsets for a variable for splitting</li>
</ul>
</li>
</ul>
<h2 id="variable-importance">Variable Importance</h2>
<ul>
<li>Split-based importance<ul>
<li>If variable j is used for split<ul>
<li>Calculate the improvement in Gini at the split</li>
</ul>
</li>
<li>Sum this improvement across all trees and splits wherever jth variable is used</li>
<li>Alternate is to calculate the number of times variable is used for splitting</li>
<li>Biased in favour of continuous variables which can be split multiple times</li>
</ul>
</li>
<li>Permutation-based importance / Boruta<ul>
<li>Use OOB samples to calculate variable importance</li>
<li>Take bth tree:<ul>
<li>Pass the OOB samples and calculate accuracy</li>
<li>Permuate jth variable and calculate the decrease in accuracy</li>
</ul>
</li>
<li>Average this decrease in accuracy across all trees to calculate variable importance for j</li>
<li>Effect is simialr to setting the coefficient to 0 in regression</li>
<li>Takes into account if good surrogates are present in the dataset</li>
</ul>
</li>
<li>Partial Dependence Plots<ul>
<li>Marginal effect of of a feature on target</li>
<li>Understand the relationship between feature and target</li>
<li>Assumes features are not correlated</li>
<li>$\hat f(x_s) =\frac{1}{C} \sum f(x_s,x_i)$</li>
<li>Average predictions over all other variables</li>
<li>Can be used to identify important interactions<ul>
<li>Friedman&#39;s H Statistic</li>
<li>If features don&#39;t interact Joint PDP can be decomposed into marginals</li>
</ul>
</li>
</ul>
</li>
<li>Shapely Values<ul>
<li>Model agnositc feature importance</li>
</ul>
</li>
<li>LIME</li>
</ul>
<h2 id="handling-categorical-variables">Handling Categorical Variables</h2>
<ul>
<li>Binary categorical variables are easily incorporated into decision trees</li>
<li>For multi-category variables:<ul>
<li>One-hot encoding (creates a binary feature for each category)</li>
<li>Label encoding (assigns an ordinal value to each category)</li>
</ul>
</li>
<li>Trees can directly handle categorical variables by considering all possible subsets for splitting<ul>
<li>CART typically uses binary splits (creates a binary question from categorical features)</li>
<li>C4.5 and CHAID can create multi-way splits</li>
</ul>
</li>
</ul>
<h2 id="tree-pruning">Tree Pruning</h2>
<ul>
<li>Decision trees are prone to overfitting<ul>
<li>Noisy Samples</li>
<li>Small data samples in nodes down the tree</li>
<li>Tree Pruning solves for overfitting<ul>
<li>Adding a cost term to objetive which captures tree complexity</li>
<li>$\text{Tree Score} = SSR + \alpha T$</li>
<li>As the tree grows in size, the reduction in SSR has to more than offset the complexity cost</li>
</ul>
</li>
</ul>
</li>
<li>Pre-pruning vs. Post-pruning:<ul>
<li>Pre-pruning: Stops tree growth early using criteria like:<ul>
<li>Minimum samples per leaf</li>
<li>Maximum depth</li>
<li>Minimum impurity decrease</li>
</ul>
</li>
<li>Post-pruning: Grows a full tree and then removes branches that don&#39;t improve generalization<ul>
<li>Cost-complexity pruning (used in CART)</li>
<li>Reduced Error Pruning (REP)</li>
<li>Pessimistic Error Pruning (PEP)</li>
</ul>
</li>
</ul>
</li>
<li>Cross-validation can be used to determine optimal pruning level</li>
</ul>

    </main>
    <footer>
      <p>Generated with Markdown Notes Static Site Generator</p>
    </footer>
  </div>
</body>
</html>
  